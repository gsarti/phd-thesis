# Quantifying Context Usage in Neural Machine Translation {#sec-chap-4-pecore}

::: {.callout-note icon="false"}

## Chapter Summary

This chapter investigates how contextual information is leveraged by context-aware machine translation models. For this purpose, we introduce **P**lausibility **E**valuation of **Co**ntext **Re**liance ([PECoRe]{.smallcaps}), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to contrastively identify context-sensitive target tokens in generated texts and link them to contextual cues justifying their prediction. We demonstrate the framework's effectiveness by assessing the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. We integrate [PECoRe]{.smallcaps} in the Inseq toolkit API and apply it to unannotated model outputs to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.

\vspace{7pt}\noindent

This chapter is adapted from the paper *Quantifying the Plausibility of Context Reliance in Neural Machine Translation* [@sarti-etal-2024-quantifying]. @sec-chap4-inseq-integration is adapted from the case study in *Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit* [@sarti-etal-2024-democratizing].

:::

> *An interpretation will be meaningful to the extent that it accurately reflects some isomorphism to the real world.*
>
> *-- Douglas R. Hofstadter, G√∂del, Escher, Bach: An Eternal Golden Braid (1979)*

## Introduction {#sec-chap4-intro}

Research in NLP interpretability defines various desiderata for rationales of model behaviors, i.e. the contributions of input tokens toward model predictions computed using input attribution [@madsen-etal-2022-posthoc]. One such property is *plausibility*, corresponding to the alignment between model rationales and salient input words identified by human annotators [@jacovi-goldberg-2020-towards]. Low-plausibility rationales usually occur alongside generalization failures or biased predictions and can be useful to identify cases of models being "right for the wrong reasons" [@mccoy-etal-2019-right].

However, while plausibility has an intuitive interpretation for classification tasks involving a single prediction, extending this methodology to generative language models presents several challenges. First, LMs have a large output space in which semantically equivalent tokens (e.g. "PC" and "computer") are competing candidates for next-word prediction [@holtzman-etal-2021-surface]. Moreover, LMs' generations are the product of optimization pressures to ensure independent properties such as semantic relatedness, topical coherence and grammatical correctness, which can hardly be captured by a single attribution score [@yin-neubig-2022-interpreting]. Finally, since autoregressive generation involves an iterative prediction process, model rationales could be extracted for every generated token. This raises the issue of *which generated tokens* can have plausible contextual explanations.

Recent attribution techniques for explaining language models incorporate contrastive alternatives to disentangle different aspects of model predictions (e.g. the choice of "meowing" over "screaming" for "*The cat is \_\_\_*" is motivated by semantic appropriateness, but not by grammaticality) [@ferrando-etal-2023-explaining; @sarti-etal-2023-inseq-fixed]. However, these studies avoid the issues above by narrowing the evaluation to a single generation step matching a phenomenon of interest. For example, given the sentence "*The pictures of the cat \_\_\_*", a plausible rationale for the prediction of the word "are" should reflect the role of "pictures" in subject-verb agreement. While this approach can be useful to validate model rationales, it confines plausibility assessment to a small set of handcrafted benchmarks where tokens with plausible explanations are known in advance. Moreover, it risks overlooking important patterns of context usage, including those that do not immediately match linguistic intuitions. In light of this, we suggest that identifying *which* generated tokens were most affected by contextual input information should be an integral part of plausibility evaluation for language generation tasks.

To achieve this goal, we propose a novel interpretability framework, which we dub **P**lausibility **E**valuation of **Co**ntext **Re**liance ([PECoRe]{.smallcaps}). [PECoRe]{.smallcaps} enables the end-to-end extraction of *cue-target token pairs* consisting of context-sensitive generated tokens and their respective influential contextual cues from language model generations, as shown in @fig-pecore-example. These pairs can uncover context dependence in naturally occurring generations and, for cases where human annotations are available, help quantify context usage plausibility in language models. Importantly, our approach is compatible with modern attribution methods using contrastive targets [@yin-neubig-2022-interpreting], avoids using reference translations to stay clear of problematic distributional shifts [@vamvas-sennrich-2021-limits], and can be applied on unannotated inputs to identify context usage in model generations.

![Examples of sentence-level English$\rightarrow$Italian translation with [lack-of-context errors]{bg-color="brand-color.lightredclear"} and their correct contextual counterpart. In the contextual case [context-sensitive source tokens]{bg-color="brand-color.lightorange"} are disambiguated using source (‚ì¢) or target-based (‚ì£) [contextual cues]{color="brand-color.lightbluedim"} to produce correct [context-sensitive target tokens]{bg-color="brand-color.lightgreen"}. [PECoRe]{.smallcaps} enables the end-to-end extraction of [cue]{color="brand-color.lightbluedim"}-[target]{bg-color="brand-color.lightgreen"} pairs (e.g. [she]{color="brand-color.lightbluedim"}-[alla pastorella]{bg-color="brand-color.lightgreen"}, [le pecore]{color="brand-color.lightbluedim"}-[le]{bg-color="brand-color.lightgreen"}).](../figures/chap-4-pecore/example.pdf){#fig-pecore-example width=60% fig-pos="t"}

After formalizing our proposed approach in @sec-chap4-pecore-framework, we apply [PECoRe]{.smallcaps} to contextual machine translation to study the plausibility of context reliance in bilingual and multilingual MT models. While [PECoRe]{.smallcaps} can easily be used alongside encoder-decoder and decoder-only language models for interpreting context usage in any text generation task, we focus our evaluation on MT because of its constrained output space facilitating automatic assessment and the availability of MT datasets annotated with human rationales of context usage. We thoroughly test [PECoRe]{.smallcaps} on well-known discourse phenomena, benchmarking several context sensitivity metrics and attribution methods to identify cue-target pairs. We conclude by applying [PECoRe]{.smallcaps} to unannotated examples and showcasing some reasonable and questionable cases of context reliance in MT model translations.^[Code: <https://github.com/gsarti/pecore>]

In sum, we make the following contributions:

- We introduce [PECoRe]{.smallcaps}, an interpretability framework to detect and attribute context reliance in language models. [PECoRe]{.smallcaps} enables a quantitative evaluation of plausibility for language generation beyond the limited artificial settings explored in previous literature.
- We compare the effectiveness of context sensitivity metrics and input attribution methods for context-aware MT, showing the limitations of metrics currently in use.
- We apply [PECoRe]{.smallcaps} to naturally-occurring translations to identify interesting discourse-level phenomena and discuss issues in the context usage abilities of context-aware MT models.

## Related Work {#sec-chap4-related-work}

[Context Usage in Language Generation]{.paragraph} An appropriate^[We avoid using the term *faithfulness* due to its ambiguous usage in interpretability research.] usage of input information is fundamental in tasks such as summarization [@maynez-etal-2020-faithfulness] to ensure the soundness of generated texts. While appropriateness is traditionally verified post-hoc using trained models [@durmus-etal-2020-feqa; @kryscinski-etal-2020-evaluating; @goyal-durrett-2021-annotating], recent interpretability works aim to gauge input influence on model predictions using internal properties of language models, such as the mixing of contextual information across model layers [@kobayashi-etal-2020-attention; @ferrando-etal-2022-measuring; @mohebbi-etal-2023-quantifying] or the layer-by-layer refinement of next token predictions [@geva-etal-2022-transformer; @belrose-etal-2023-eliciting]. Recent attribution methods can disentangle factors influencing generation in language models [@yin-neubig-2022-interpreting] and were successfully used to detect and mitigate hallucinatory behaviors [@tang-etal-2022-reducing; @dale-etal-2023-detecting; @dale-etal-2023-halomi]. Our proposed method adopts this intrinsic perspective to identify context reliance without ad-hoc trained components.

[Context Usage in Neural Machine Translation]{.paragraph} Despite advances in context-aware MT [@voita-etal-2018-context; @voita-etal-2019-context; @lopes-etal-2020-document; @majumder-etal-2022-baseline; @jin-etal-2023-challenges; *inter alia*, surveyed by @maruf-etal-2021-survey], only a few works explored whether context usage in MT models aligns with human intuition. Notably, some studies focused on *which parts of context* inform model predictions, finding that supposedly context-aware MT models are often incapable of using contextual information [@kim-etal-2019-document; @fernandes-etal-2021-measuring] and tend to pay attention to irrelevant words [@voita-etal-2018-context], with an overall poor agreement between human annotations and model rationales [@yin-etal-2021-context]. Other works instead investigated *which parts of generated texts* are influenced by context, proposing various contrastive methods to detect gender biases, over/under-translations [@vamvas-sennrich-2021-contrastive; @vamvas-sennrich-2022-little], and to identify various discourse-level phenomena in MT corpora [@fernandes-etal-2023-translation]. While these two directions have generally been investigated separately, our work proposes a unified framework to enable an end-to-end evaluation of context-reliance plausibility in language models.

[Plausibility evaluation in NLP]{.paragraph} Plausibility evaluation for NLP models has largely focused on classification models [@deyoung-etal-2020-eraser; @atanasova-etal-2020-diagnostic; @attanasio-etal-2023-ferret]. While few works investigate plausibility in language generation [@vafa-etal-2021-rationales; @ferrando-etal-2023-explaining], such evaluations typically involve a single generation step to complete a target sentence with a token connected to preceding information (e.g. subject/verb agreement, as in "*The pictures of the cat [is/are]*"), effectively biasing the evaluation by using a pre-selected token of interest. On the contrary, our framework proposes a more comprehensive evaluation of generation plausibility that includes the identification of context-sensitive generated tokens as an important prerequisite.

## The [PECoRe]{.smallcaps} Framework {#sec-chap4-pecore-framework}

[PECoRe]{.smallcaps} is a two-step framework for identifying context dependence in generative language models. First, *context-sensitive tokens identification* (CTI) selects which tokens among those generated by the model were influenced by the presence of the preceding context (e.g. the feminine options "alla pastorella, le" in @fig-pecore-example). Then, *contextual cues imputation* (CCI) attributes the prediction of context-sensitive tokens to specific cues in the provided context (e.g. the feminine cues "she, Le pecore" in @fig-pecore-example). **Cue-target pairs** formed by influenced target tokens and their respective influential context cues can then be compared to human rationales to assess the models' plausibility of context reliance for contextual phenomena of interest. @fig-pecore provides an overview of the two steps applied to the context-aware MT setting discussed by this work. A more general formalization of the framework for language generation is proposed in the following sections.

![The [PECoRe]{.smallcaps} framework applied to an encoder-decoder MT model. **Left:** Context-sensitive token identification (CTI). ‚ìµ: A context-aware MT model translates source context ($C_x$) and current ($x$) sentences into target context ($C_{\hat y}$) and current ($\hat y$) outputs. ‚ì∂: $\hat y$ is force-decoded in the non-contextual setting instead of natural output $\tilde y$. ‚ì∑: Contrastive metrics are collected throughout the model for every $\hat y$ token to compare the two settings. ‚ì∏: Selector $s_\text{cti}$ maps metrics to binary context-sensitive labels for every $\hat y_i$. **Right:** Contextual cues imputation (CCI). ‚ìµ: Non-contextual target $\tilde y^*$ is generated from contextual prefix $\hat y_{<t}$. ‚ì∂: Function $f_\text{tgt}$ is selected to contrast model predictions with ($\hat y_t$) and without ($\tilde y_t^*$) input context. ‚ì∑: Attribution method $f_\text{att}$ using $f_\text{tgt}$ as target scores contextual cues driving $\hat y_t$ prediction. ‚ì∏: Selector $s_\text{cci}$ selects relevant cues, and cue-target pairs are assembled.](../figures/chap-4-pecore/pecore.pdf){#fig-pecore fig-pos="t"}

### Notation

Let $X_\text{ctx}^{i}$ be the sequence of contextual inputs containing $N$ tokens from vocabulary $\mathcal{V}$, composed by current input $x$, generation prefix $y_{<i}$ and context $C$. Let $X_\text{no-ctx}^{i}$ be the non-contextual input in which $C$ tokens are excluded.[^3] $P_\text{ctx}^{i} = P\left(x,\, y_{<i},\,C,\,\theta\right)$ is the discrete probability distribution over $\mathcal{V}$ at generation step $i$ of a language model with $\theta$ parameters receiving contextual inputs $X_\text{ctx}^{i}$. Similarly, $P_\text{no-ctx}^{i} = P\left(x,\, y_{<i},\,\theta\right)$ is the distribution obtained from the same model for non-contextual input $X_\text{no-ctx}^{i}$. Both distributions are equivalent to vectors in the probability simplex in $\mathbb{R}^{|\mathcal{V}|}$, and we use $P_\text{ctx}(y_i)$ to denote the probability of next token $y_i$ in $P_\text{ctx}^{i}$, i.e. $P(y_i\,|\,x,\,y_{<i},\,C)$.

[^3]: In the contextual MT example of @fig-pecore, $C$ includes source context $C_x$ and target context $C_y$.

### Context-sensitive Token Identification (CTI) {#sec-chap4-cti}

CTI adapts the contrastive conditioning paradigm proposed by @vamvas-sennrich-2021-contrastive to detect input context influence on model predictions using the contrastive pair $P_\text{ctx}^{i}, P_\text{no-ctx}^{i}$. Both distributions are relative to the **contextual target sentence** $\hat y = \{\hat y_1 \dots \hat y_n\}$, corresponding to the sequence produced by a decoding strategy of choice in the presence of input context. In @fig-pecore, the contextual target sentence $\hat y=$ "Sont-elles √† l'h√¥tel?" is generated when $x$ and contexts $C_x, C_{\hat y}$ are provided as inputs, while **non-contextual target sentence** $\tilde y =$ "Ils sont √† l'h√¥tel?" would be produced when only $x$ is provided. In the latter case, $\hat y$ is instead force-decoded from the non-contextual setting to enable a direct comparison of matching outputs. We define a set of **contrastive metrics** $\mathcal{M} = \{m_1, \dots, m_M\}$, where each $m: \displaystyle \Delta_{|\mathcal{V}|} \times \Delta_{|\mathcal{V}|} \mapsto \mathbb{R}$ maps a contrastive pair of probability vectors to a continuous score. For example, the difference in next token probabilities for contextual and non-contextual settings, i.e. $P_\text{diff}(\hat y_i) = P_\text{ctx}(\hat y_i) - P_\text{no-ctx}(\hat y_i)$, might be used for this purpose.[^4] Target tokens with high contrastive metric scores can be identified as *context-sensitive*, provided $C$ is the only added parameter in the contextual setting. Finally, a **selector** function $s_\text{cti}: \displaystyle \mathbb{R}^{| \mathcal{M} |} \mapsto \{0,1\}$ (e.g. a statistical threshold selecting salient scores) is used to classify every $\hat y_i$ as context-sensitive or not.

[^4]: We use $m^i$ to denote the result of $m(P_\text{ctx}^{i}, P_\text{no-ctx}^{i})$. Several metrics are presented in @sec-chap4-cti-metrics.

### Contextual Cues Imputation (CCI) {#sec-chap4-cci}

CCI applies the contrastive attribution paradigm [@yin-neubig-2022-interpreting] to trace the generation of every context-sensitive token in $\hat y$ back to context $C$, identifying cues driving model predictions.

::: {#def-target-dependent-attribution}
Let $s, s'$ be the resulting scores of two attribution target functions $f_\text{tgt}, f'_\text{tgt}$. An attribution method $f_\text{att}$ is \textbf{target-dependent} if importance scores $A$ are computed in relation to the outcome of its attribution target function, i.e. whenever the following condition is verified.

$$f_\text{att}(x, y_{<t}, C, \theta, s) \neq f_\text{att}(x, y_{<t}, C, \theta, s') \;\; \forall s \neq s'$$
:::

In practice, common gradient-based attribution approaches [@simonyan-etal-2014-saliency; @sundararajan-etal-2017-ig] are target-dependent as they rely on the outcome predicted by the model (typically the logit or the probability of the predicted class) as differentiation target to backpropagate importance to model input features. Similarly, perturbation-based approaches [@zeiler-fergus-2014-visualizing] use the variation in prediction probability for the predicted class when noise is added to some of the model inputs to quantify the importance of the noised features.

On the contrary, recent approaches relying solely on model internals to define input importance are generally *target-insensitive*. For example, attention weights used as model rationales, either in their raw form or after a rollout procedure to obtain a unified score [@abnar-zuidema-2020-quantifying], are independent of the predicted outcome. Similarly, value zeroing scores [@mohebbi-etal-2023-quantifying] reflect only the representational dissimilarity across model layers before and after zeroing value vectors, and as such do not explicitly account for model predictions.

::: {#def-contrastive-attribution-method}
Let $\mathcal{T}$ be the set of indices corresponding to context-sensitive tokens identified by the CTI step, such that $t \in \hat y$ and $\forall t \in \mathcal{T}, s_\text{cti}(m_1^{t}, \dots, m_M^{t}) = 1$. Let also $f_\text{tgt}: \Delta_{|\mathcal{V}|} \times \dots \mapsto \mathbb{R}$ be a **contrastive attribution target** function representing an attribution target of interest, for example the difference in next token probabilities between the contextual option $\hat y_t$ and the non-contextual option $\tilde y^*_t$ from the same contextual distribution $P_\text{ctx}^{t}$, plus any additional required parameter. The **contrastive attribution method** $f_\text{att}$ is a composite function quantifying the importance of contextual inputs to determine the output of $f_\text{tgt}$ for a given model with $\theta$ parameters.
:::

$$f_\text{att}(\hat y_{t}) = f_\text{att}(x, \hat y_{<t}, C, \theta, f_\text{tgt}) = f_\text{att}\big(x, \hat y_{<t}, C, \theta, f_\text{tgt}(P_\text{ctx}^t, \dots)\big)$$


::: {#rem-contrastive-attribution-method}
The non-contextual next token $\tilde y^*_t$ can be computed using the contextual prefix $\hat y_{<t} = \{ \hat y_1, \dots, \hat y_{t - 1}\}$ (e.g. $\hat y_{<t} =$"Sont-" in @fig-pecore) and non-contextual inputs $X_\text{no-ctx}^{t}$. This is conceptually equivalent to predicting the next token of a new non-contextual sequence $\tilde y^*$ which, contrary to the original $\tilde y$, starts from a forced contextual prefix $\hat y_{<t}$ (e.g. "ils" in $\tilde y^* =$ "\underline{Sont-}ils √† l'h√¥tel?" in @fig-pecore).
:::

::: {#rem-different-prob-for-same-target}
A $f_\text{tgt}$ making use of both $P_\text{ctx}^{t}$ and $P_\text{no-ctx}^{t}$, e.g. the KL divergence between the contextual and non-contextual probability distributions [@kullback-leibler-1951-information], can ultimately result in non-zero $f_\text{att}(\hat y_t)$ scores, even when $\hat y_t = \tilde y^*_t$, i.e. even when the next predicted token is the same, since probabilities $P_\text{ctx}(\hat y_t), P_{no-ctx}(\tilde y^*_t)$ are likely to differ beyond top-1 predictions. This is a desirable property of $f_\text{att}$, as it allows the attribution method to capture the influence of context on the model's decision-making process, even in the case where the predicted token remains unchanged.
:::

::: {#rem-generalized-contrastive-attribution}
Our formalization of $f_\text{att}$ generalizes the method proposed by @yin-neubig-2022-interpreting to support any target-dependent attribution method, such as popular gradient-based approaches [@simonyan-etal-2014-saliency; @sundararajan-etal-2017-ig], and any contrastive attribution target $f_\text{tgt}$.
:::

$f_\text{att}$ produces a sequence of attribution scores $A_t = \{a_1, \dots, a_N\}$ matching contextual input length $N$. From those, only the subset $A_{t\,\text{ctx}}$ of scores corresponding to context input sequence $C$ are passed to **selector** function $s_\text{cci}: \displaystyle \mathbb{R} \mapsto \{0,1\}$, which predicts a set $\mathcal{C}_{t}$ of indices corresponding to contextual cues identified by CCI, such that $\forall c \in \mathcal{C}_t, \forall a \in A_{t\,\text{ctx}}, s_\text{cci}(a_{c}) = 1$.

Having collected all context-sensitive generated token indices $\mathcal{T}$ using CTI and their contextual cues through CCI ($C_t$), [PECoRe]{.smallcaps} ultimately returns a sequence $S_\text{ct}$ of all identified cue-target pairs:

$$
\begin{aligned}
\mathcal{T} &= \text{CTI}(C, x, \hat y, \theta, \mathcal{M}, s_\text{cti}) = \{t \;|\; s_\text{cti}(m_1^t, \dots, m_M^t) = 1 \} \\
\mathcal{C} &= \text{CCI}(\mathcal{T}, C, x, \hat y, \theta,  f_\text{att}, f_\text{tgt}, s_\text{cci}) = \{ c \;|\; s_\text{cci}(a_c) = 1 \,\forall a_c \in A_{t\,\text{ctx}}, \forall t \in \mathcal{T}\} \\
S &= \texttt{PECoRe}(C, x, \theta, s_\text{cti}, s_\text{cci}, \mathcal{M}, f_\text{att}, f_\text{tgt}) = \{ (C_c, \hat y_t) \;|\; \forall t \in \mathcal{T}, \forall c \in \mathcal{C}_t, \forall \mathcal{C}_t \in \mathcal{C} \}
\end{aligned}
$$

A pseudocode implementation for the [PECoRe]{.smallcaps} algorithm is provided in @alg-pecore.

```pseudocode
#| label: alg-pecore
#| pdf-placement: "ht!"

\begin{algorithm}
\caption{PECoRe cue-target extraction process}
\begin{algorithmic}
\Require $C, x$ (Input context and current sequences), $\theta$ (Model parameters), $s_{\text{cti}}, s_{\text{cci}}$ (Selector functions), $\mathcal{M}$ (Contrastive metrics), $f_\text{att}$ (Contrastive attribution method), $f_\text{tgt}$ (Contrastive attribution target function)
\Procedure{PECoRe}{$C, x, \theta, s_\text{cti}, s_\text{cci}, \mathcal{M}, f_\text{att}, f_\text{tgt}$}
  \State $\hat y = \textnormal{generate(}C, x, \theta$) using any decoding strategy and parameters
  \State $\mathcal{T} = \textnormal{CTI(}C, x, \hat y, \theta, \mathcal{M}, s_\text{cti}\textnormal{)}$
  \ForAll{$t \in \mathcal{T}$}
    \State $\mathcal{C}_t = \textnormal{CCI(}t, C, x, \hat y, \theta, f_\text{att}, f_\text{tgt}, s_\text{cci}\textnormal{)}$
    \ForAll{$c \in \mathcal{C}_t$}
      \State Store $(C_t^c, \hat y_t)$ in $S_\text{ct}$
    \EndFor
  \EndFor
  \State \textbf{return} $S_\text{ct}$ // Set of cue-target pairs
\EndProcedure
\Procedure{CTI}{$C, x, \hat y, \theta, \mathcal{M}, s_\text{cti}$}
    \State $\mathcal{T} = \emptyset$ // Empty set for context-sensitive indices of $\hat y$ tokens
    \ForAll{$\hat{y}_i \in \hat{y}$}
        \ForAll{$m \in \mathcal{M}$}
            \State $m^i = m \big(P_{\text{ctx}}(\hat{y}_i), P_{\text{no-ctx}}(\hat{y}_i) \big)$
        \EndFor
        \If{$(s_{\text{cti}}(m_1^i, \dots, m_M^i) = 1$)}
            \State Store $i$ in set $\mathcal{T}$
        \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{T}$
\EndProcedure
\Procedure{CCI}{$t, C, x, \hat y, \theta, f_\text{att}, f_\text{tgt}, s_\text{cci}$}
    \State $\mathcal{C}_t = \emptyset$ // Empty set for contextual cues for target token $t$
    \State Generate constrained non-contextual target current sequence $\tilde y^*$ from $\hat y_{<t}$
    \State Use attribution method $f_\text{att}$ with target $f_\text{tgt}$ to get importance scores $A_t$
    \State Identify the subset $A_{t\,\text{ctx}}$ corresponding to tokens of context $C = \{ C_1, \dots, C_K\}$
    \ForAll{$a_i \in A_{t\,\text{ctx}} = \{a_1, \dots, a_K\}$}
        \If{$s_\text{cci}(a_i) = 1$}
            \State Store $C_i$ in $\mathcal{C}_t$
        \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{C}_t$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

{{< pagebreak >}}
\FloatBarrier


## Context Reliance Plausibility in Context-aware MT

This section describes our evaluation of [PECoRe]{.smallcaps} in a controlled setup. We experiment with several contrastive metrics and attribution methods for CTI and CCI (@sec-chap4-cti-metrics, @sec-chap4-cci-metrics), evaluating them in isolation to quantify the performance of individual components. An end-to-end evaluation is also performed in @sec-chap4-cci-metrics to establish the applicability of [PECoRe]{.smallcaps} in a naturalistic setting.

### Experimental Setup {#sec-chap4-setup}

[Evaluation Datasets]{.paragraph} Evaluating generation plausibility requires human annotations for context-sensitive tokens in target sentences and disambiguating cues in their preceding context. To our knowledge, the SCAT dataset [@yin-etal-2021-context] is the only resource matching these requirements. SCAT is an English$\rightarrow$French corpus with human annotations of anaphoric pronouns and disambiguating context on OpenSubtitles2018 dialogue translations [@lison-etal-2018-opensubtitles2018; @lopes-etal-2020-document]. SCAT examples were extracted automatically using lexical heuristics and thus contain only a limited set of anaphoric pronouns (*it, they* $\rightarrow$ *il/elle, ils/elles*), with no guarantees of contextual cues being found in preceding context.

The original SCAT test set contains 1000 examples with automatically identified context-sensitive pronouns *it/they* (marked by `<p>...</p>`) and human-annotated contextual cues aiding their disambiguation (marked by `<hon>...</hoff>`). Of these, we find 38 examples containing malformed tags and several more examples where an unrelated word containing *it* or *they* was wrongly marked as context-sensitive (e.g. `the soccer ball h<p>it</p> your chest`). Moreover, due to the original extraction process adopted for SCAT, there is no guarantee that contextual cues will be contained in the preceding context as they could also appear in the same sentence, defeating the purpose of our context usage evaluation. Thus, we prefilter the whole corpus to preserve only sentences with well-formed tags and inter-sentential contextual cues identified by original annotators. Moreover, a manual inspection procedure is carried out to validate the original cue tags and discard problematic sentences, obtaining a final set of 250 examples with inter-sentential pronoun coreference, which we name SCAT+^[SCAT+ is available on the Hugging Face Hub: [`inseq/scat`](https://hf.co/datasets/inseq/scat)].

Additionally, we manually annotate contextual cues in [DiscEval-MT]{.smallcaps} [@bawden-etal-2018-evaluating], another English$\rightarrow$French corpus containing handcrafted examples for *anaphora resolution* ([ana]{.smallcaps}) and *lexical choice* ([lex]{.smallcaps}). In the case of [DiscEval-MT]{.smallcaps}, we use minimal pairs in the original dataset to automatically mark differing tokens as context-sensitive. Then, contextual cues are manually labeled separately by two annotators with good familiarity with both English and French. Cue annotations are compared across the two splits, resulting in very high agreement due the simplicity of the corpus ($97\%$ overlap for [ana]{.smallcaps}, $90\%$ for [lex]{.smallcaps}).^[Our modified version of [DiscEval-MT]{.smallcaps} is available on the Hugging Face Hub: [`inseq/disc_eval_mt`](https://hf.co/datasets/inseq/disc_eval_mt).]

Our final evaluation set contains 250 SCAT+ and 400 [DiscEval-MT]{.smallcaps} translations across two discourse phenomena. @tbl-scat-demt-examples provides some examples for the three data splits.

{{< include ../tables/chap-4-pecore/_scat-demt-examples.qmd >}}

[Models]{.paragraph} We evaluate two bilingual Opus models [@tiedemann-thottingal-2020-opus] using the transformer base architecture [@vaswani-etal-2017-attention, Small and Large], and mBART-50 1-to-many [@tang-etal-2021-multilingual], a larger multilingual MT model supporting 50 target languages, using the ü§ó `transformers` library [@wolf-etal-2020-transformers]. We fine-tune models using extended translation units [@tiedemann-scherrer-2017-neural] with contextual inputs marked by break tags such as `source context <brk> source current` to produce translations in the format `target context <brk> target current`, where context and current target sentences are generated. We perform context-aware fine-tuning on 242k IWSLT 2017 English$\rightarrow$French examples [@cettolo-etal-2017-overview], using a dynamic context size of 0-4 preceding sentences to ensure robustness to different context lengths and allow contextless usage. To further improve models' context sensitivity, we continue fine-tuning on the SCAT training split, containing 11k examples with inter- and intra-sentential pronoun anaphora.

{{< include ../tables/chap-4-pecore/_model-accuracies.qmd >}}

[Model Disambiguation Accuracy]{.paragraph} We estimate contextual disambiguation accuracy by verifying whether annotated (gold) context-sensitive words are found in model outputs. Results before and after context-aware fine-tuning are shown in @tbl-model-accuracies. We find that fine-tuning improves translation quality and disambiguation accuracy across all tested models, with larger gains for anaphora resolution datasets closely matching fine-tuning data. To gain further insight into these results, we use context-aware models to translate examples with and without context and identify a subset of *context-sensitive translations* ([ok-cs]{.smallcaps}) for which the correct target word is generated only when input context is provided to the model. Interestingly, we find a non-negligible amount of translations that are correctly disambiguated even in the absence of input context (corresponding to [ok]{.smallcaps} minus [ok-cs]{.smallcaps} in @tbl-model-accuracies). For these examples, the correct prediction of ambiguous words aligns with model biases, such as defaulting to masculine gender for anaphoric pronouns [@stanovsky-etal-2019-evaluating] or using the most frequent sense for word sense disambiguation. Provided that such examples are unlikely to exhibit context reliance, we focus particularly on the [ok-cs]{.smallcaps} subset results in our following evaluation.

### Metrics for Context-sensitive Target Identification {#sec-chap4-cti-metrics}

The following contrastive metrics are evaluated for detecting context-sensitive tokens in the CTI step.

**Relative Context Saliency** We use contrastive gradient norm attribution [@yin-neubig-2022-interpreting] to compute input importance towards predicting the next token $\hat y_i$ with and without input context. Positive importance scores are obtained for every input token using the L2 gradient vectors norm [@bastings-etal-2022-will], and relative context saliency is obtained as the proportion between the normalized importance for context tokens $c \in C_x, C_y$ and the overall input importance, following previous work quantifying MT input contributions [@voita-etal-2021-analyzing; @ferrando-etal-2022-towards; @edman-etal-2024-character].

$$\nabla_\text{ctx} (P_\text{ctx}^{i}, P_\text{no-ctx}^{i}) = \frac{\sum_{c \in C_x, C_y} \big\| \nabla_c \big( P_\text{ctx}(\hat y_i) - P_\text{no-ctx}(\hat y_i) \big) \big\|}{\sum_{t \in X_\text{ctx}^{i}} \big\| \nabla_t \big( P_\text{ctx}(\hat y_i) - P_\text{no-ctx}(\hat y_i) \big) \big\|}$$

**Likelihood Ratio (LR)** and **Pointwise Contextual Cross-mutual Information (P-CXMI)** Proposed by @vamvas-sennrich-2021-contrastive and @fernandes-etal-2023-translation respectively, both metrics frame context dependence as a ratio of contextual and non-contextual probabilities.

$$\text{LR}(P_\text{ctx}^{i}, P_\text{no-ctx}^{i}) = \frac{P_\text{ctx}(\hat{y}_i)}{P_\text{ctx}(\hat{y}_i) + P_\text{no-ctx}(\hat{y}_i)}$$

$$\text{P-CXMI}(P_\text{ctx}^{i}, P_\text{no-ctx}^{i}) = - \log \frac{P_\text{ctx}(\hat{y}_i)}{P_\text{no-ctx}(\hat{y}_i)}$$

**KL-Divergence** [@kullback-leibler-1951-information] between $P_\text{ctx}^{i}$ and $P_\text{no-ctx}^{i}$ is the only metric we evaluate that considers the full distribution rather than the probability of the predicted token. We include it to test the intuition that the impact of context inclusion might extend beyond top-1 token probabilities.

$$D_\text{KL}(P_\text{ctx}^{i} \| P_\text{no-ctx}^{i}) = \sum_{\hat{y}_i \in \mathcal{V}} P_\text{ctx}(\hat{y}_i) \log \frac{P_\text{ctx}(\hat{y}_i)}{P_\text{no-ctx}(\hat{y}_i)}$$

### Plausibility Evaluation Metrics

In practice, the CTI and CCI steps in [PECoRe]{.smallcaps} produce a sequence of continuous scores that are later binarized using selectors $s_\text{cti}, s_\text{cci}$, introduced in [@sec-chap4-pecore-framework]. To evaluate their validity, those are compared to a sequence $I_h$ of the same length containing binary values, where 1s correspond to the cues identified by human annotators while the rest are set to 0. In our experiments, we use two common plausibility metrics introduced by @deyoung-etal-2020-eraser:

**Token-level Macro F1** is the harmonic mean of precision and recall at the token level, using $I_h$ as the ground truth and the post-selector binarized scores as predictions. Macro-averaging is used to account for the sparsity of cues in $I_h$. We use this metric in the main analysis as the discretization step is more likely to reflect a realistic plausibility performance, as it matches more closely the annotation process used to derive $I_h$. We note that Macro F1 can be considered a lower bound for plausibility, as the results depend heavily on the choice of the selector used for discretization.

**Area Under Precision-Recall Curve (AUPRC)** is computed as the area under the curve obtained by varying a threshold over token importance scores and computing the precision and recall for resulting discretized $I_m$ predictions while keeping $I_h$ as the ground truth. Contrary to Macro F1, AUPRC is selector-independent and accounts for tokens‚Äô relative ranking and degree of importance. Consequently, it can be seen as an upper bound for plausibility, as if the optimal selector was used. Results using AUPRC are presented in [@sec-pecore-appendix-cti-cci-results] for completeness, but we focus on Macro F1 in the main analysis. 

### CTI Plausibility Results {#sec-chap4-cti-results}

@fig-marian-big-f1-cti presents our metrics evaluation for CTI, with results for the full test sets and the subsets of context-sensitive sentences ([ok-cs]{.smallcaps}) highlighted in @tbl-model-accuracies. To keep our evaluation simple, we use a naive $s_\text{cti}$ selector tagging all tokens with metric scores one standard deviation above the per-example mean as context-sensitive. We also include a stratified random baseline matching the frequency of occurrence of context-sensitive tokens in each dataset. Datapoints in @fig-marian-big-f1-cti are sentence-level macro F1 scores computed for every dataset example.

![Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) using Opus Large on the full datasets (left) or on [ok-cs]{.smallcaps} context-sensitive subsets (right).](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat-target_box.pdf){#fig-marian-big-f1-cti fig-pos="t"}

Pointwise metrics (LR, P-CXMI) show high plausibility for the context-sensitive subsets [ok-cs]{.smallcaps} across all datasets and models but achieve lower performances on the full test set, especially for lexical choice phenomena less present in MT models' training. KL-Divergence performs on par or better than pointwise metrics, suggesting that distributional shifts beyond top prediction candidates can provide useful information to detect context sensitivity. On the contrary, the poor performance of context saliency indicates that context reliance in aggregate cannot reliably predict context sensitivity. A manual examination of misclassified examples reveals several context-sensitive tokens that were not annotated as such since they did not match datasets' phenomena of interest but were still identified by CTI metrics. @tbl-false-negatives-cti presents several examples showing a contextual influence of French pronoun formality, while SCAT+ examples focus solely on gender disambiguation for anaphoric pronouns. This suggests that our evaluation of CTI metrics' plausibility can be considered a lower bound for actual method accuracy, as it is restricted to the two phenomena available in the datasets we used (anaphora resolution and lexical choice), rather than the broad set of contextual dependence phenomena. These results further underscore the importance of data-driven end-to-end approaches like [PECoRe]{.smallcaps} to limit the influence of selection bias during evaluation.

{{< include ../tables/chap-4-pecore/_false-negatives-cti.qmd >}}

### Methods for Contextual Cues Imputation {#sec-chap4-cci-metrics}

The following attribution methods are evaluated for detecting contextual cues in the CCI step.

**Contrastive Gradient Norm** [@yin-neubig-2022-interpreting] estimates input tokens' contributions towards predicting a target token instead of a contrastive alternative. We use this method to explain the generation of context-sensitive tokens in the presence and absence of context.

$$A_{t\,\text{ctx}} = \{\,\| \nabla_c \big(f_\text{tgt}(P_\text{ctx}^{i}, \dots) \big)\|\,|\, \forall c \in C\}$$

For the choice of $f_\text{tgt}$, we evaluate both probability difference $P_\text{ctx}(\hat y_i) - P_\text{no-ctx}(\hat y_i)$, conceptually similar to the original formulation, and the KL-Divergence of contextual and non-contextual distributions $D_\text{KL}(P_\text{ctx}^{i} \| P_\text{no-ctx}^{i})$. We use $\nabla_\text{diff}$ and $\nabla_\text{KL}$ to identify gradient norm attribution in the two settings. $\nabla_\text{KL}$ scores can be seen as the contribution of input tokens towards the shift in probability distribution caused by the presence of input context.^[Provided that $P_\text{no-ctx}(\hat y_i)$ does not depend on context, the $\nabla_\text{KL}$ gradient is functionally equivalent to the gradient for the cross-entropy function $H(P_\text{ctx}, P_\text{no-ctx}) = - \sum_{\hat{y}_i \in \mathcal{V}} P_\text{ctx}(\hat{y}_i) \log P_\text{no-ctx}(\hat{y}_i)$).]

[Attention Weights]{.paragraph} Following previous work, we use the mean attention weight across all heads and layers (**Attention Mean**, @kim-etal-2019-document) and the weight for the head obtaining the highest plausibility per-dataset (**Attention Best**, @yin-etal-2021-context) as importance measures for CCI. Attention Best can be seen as a best-case estimate of attention performance but is not a viable metric in real settings, provided that the best attention head to capture a phenomenon of interest is unknown beforehand. Since attention weights are model byproducts unaffected by predicted outputs, we use only attention scores for the contextual setting $P_\text{ctx}^{i}$ and ignore the contextless alternative when using these metrics.

### CCI Plausibility Results {#sec-chap4-cci-results}

![Macro F1 of CCI methods over full datasets using Opus Large models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings).](../figures/chap-4-pecore/macro_f1_cci_marian-big-scat+tgt_box.pdf){#fig-marian-big-f1-cci fig-pos="t"}

We conduct a controlled CCI evaluation using gold context-sensitive tokens as the starting point to attribute contextual cues. Provided that gold context-sensitive tokens are only available in annotated reference translations, a simple option when applying CCI to those would involve using references as model generations. However, this was shown to be problematic by previous research, as it would induce a *distributional discrepancy* in model predictions [@vamvas-sennrich-2021-limits]. For this reason, we let the model generate a natural translation and instead try to align tags to this new sentence using the [awesome]{.smallcaps} aligner [@dou-neubig-2021-word] with [labse]{.smallcaps} multilingual embeddings [@feng-etal-2022-language]. While this process is not guaranteed to always result in accurate tags, it provides a good approximation of gold CTI annotations on model generation for the purpose of our assessment. This corresponds to the baseline plausibility evaluation described in @sec-chap2-attrib-eval, allowing us to evaluate attribution methods in isolation, assuming perfect identification of context-sensitive tokens. @fig-marian-big-f1-cci presents our results. Scores in the right plot are relative to the context-aware Opus Large model of @sec-chap4-cti-results using both source and target context. Instead, the left plot presents results for an alternative version of the same model that was fine-tuned using only source context (i.e. translating $C_x, x \rightarrow y$ without producing target context $C_y$). Source-only context was used in previous context-aware MT studies [@fernandes-etal-2022-quality], and we include it in our analysis to assess how the presence of target context impacts model plausibility. We finally validate the end-to-end plausibility of [PECoRe]{.smallcaps}-detected pairs using context-sensitive tokens identified by the best CTI metric from @sec-chap4-cti-results (KL-Divergence) as the starting point for CCI, and using a simple statistical selector equivalent to the one used for CTI evaluation.

First, contextual cues are more easily detected for the source-only model using all evaluated methods. This finding corroborates previous evidence highlighting how context usage issues might emerge when lengthy context is provided [@fernandes-etal-2021-measuring; @shi-etal-2023-large]. When moving from gold CTI tags to the end-to-end setting (E2E) we observe a larger drop in plausibility for the SCAT+ and [DiscEval-MT ana]{.smallcaps} datasets that more closely match the fine-tuning data of analyzed MT models. This suggests that standard evaluation practices may overestimate model plausibility for in-domain settings and that our proposed framework can effectively mitigate this issue. Interestingly, the Attention Best method suffers the most from end-to-end CCI application, while other approaches are more mildly affected. This can result from attention heads failing to generalize to other discourse-level phenomena at test time, providing further evidence of the limitations of attention as an explanatory metric [@jain-wallace-2019-attention; @bastings-filippova-2020-elephant]. While $\nabla_\text{diff}$ and $\nabla_\text{KL}$ appear as the most robust choices across the two datasets, per-example variability remains high across the board, leaving space for improvement for more plausible attribution methods in future work.

## Detecting Context Reliance in the Wild {#sec-chap4-analysis}

We continue our analysis by applying the [PECoRe]{.smallcaps} method to the popular Flores-101 MT benchmark [@goyal-etal-2022-flores], containing groups of 3-5 contiguous sentences from English Wikipedia. While in previous sections, labeled examples were used to evaluate the effectiveness of [PECoRe]{.smallcaps} components, here we apply our framework end-to-end to unannotated MT outputs and inspect resulting cue-target pairs to identify successes and failures of context-aware MT models.

Specifically, we apply [PECoRe]{.smallcaps} to the context-aware Opus Large and mBART-50 models of @sec-chap4-setup, using KL-Divergence as CTI metric and $\nabla_\text{KL}$ as CCI attribution method. We set $s_\text{cti}$ and $s_\text{cci}$ to two standard deviations above the per-example average score to focus our analysis on very salient tokens.

{{< include ../tables/chap-4-pecore/_pecore-examples-1.qmd >}}

{{< include ../tables/chap-4-pecore/_pecore-examples-2.qmd >}}

@tbl-pecore-examples-1 and @tbl-pecore-examples-2 shows some examples annotated with [PECoRe]{.smallcaps} outputs. In the first example, the acronym MS, standing for Multiple Sclerosis, is translated generically as *la maladie* (the illness) in the contextual output, but as *SEP* (the French acronym for MS, i.e. *scl√©rose en plaques*) when context is not provided. [PECoRe]{.smallcaps} shows how this choice is mostly driven by the MS mention in source context $C_x$ while the term *scl√©rose en plaques* in target context $C_y$ is not identified as influential, possibly motivating the choice for the more generic option.

In the second example, the prediction of pronoun *elles* (they, feminine) depends on the context noun phrase *mob of market women* (*foule de femmes du march√©* in French). However, the correct pronoun referent is *Le roi et Madame Elizabeth* (*the king and Madam Elizabeth*), so the pronoun should be the masculine default *ils* commonly used for mixed gender groups in French. [PECoRe]{.smallcaps} identifies this as a context-dependent failure due to an issue with the MT model's anaphora resolution.

The third example presents an interesting case of erroneous numeric format cohesion that would normally go undetected when relying on pre-defined linguistic hypotheses. In this sentence, the score *26-00* is translated as *26* in the contextless output and as *26:00* in the context-aware translation. The *10:00* time indications found by [PECoRe]{.smallcaps} in the contexts suggest this is a case of problematic lexical cohesion.

Finally, we include an example of context usage for English$\rightarrow$Turkish translation to test the contextual capabilities of the default mBART-50 model without context-aware fine-tuning. Again, [PECoRe]{.smallcaps} shows how the word *rotasyon* (rotation) is selected over *d√∂ng√º* (loop) as the correct translation in the contextual case due to the presence of the lexically similar word *rotasyonlarƒ±* in the previous context.

## Integrating [PECoRe]{.smallcaps} in Inseq {#sec-chap4-inseq-integration}

To facilitate the use of [PECoRe]{.smallcaps} in future research, a flexible implementation of the framework was included into the Inseq toolkit presented in @sec-chap-3-inseq. Since its [v0.6.0](https://github.com/inseq-team/inseq/releases/tag/v0.6.0) Inseq offers the CLI command `attribute-context`, supporting all contrastive step functions and attribution methods in the library, and compatible with any decoder-only and encoder-decoder generative language model. [@fig-chap4-inseq-api-example] provides an example employing the Inseq API to attribute a language model answer to input context paragraphs, similarly to the retrieval-augmented generation task we discuss in @sec-chap-5-mirage.^[The interface is available at: <https://huggingface.co/spaces/gsarti/pecore>.] In the example, the StableLM 2 Zephyr 1.6B language model^[[`stabilityai/stablelm-2-zephyr-1_6b`](https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b)] is prompted with contexts retrieved from Wikipedia to provide a long-form answer to a query about population in Hawaii islands. When referring to "the information provided" in ‚ìµ, [PECoRe]{.smallcaps} identifies the indices of the two documents containing relevant information as salient. The names of Ni'ihau, a small island with barely any population, is also found important when the model produces an additional remark on their population in ‚ì∂. However, we observe that the answer in the context is not found salient by [PECoRe]{.smallcaps} during generation, suggesting the model might be relying on memorization. We test hypothesis by prompting the model in a closed-book setting without context paragraphs, finding that the model can indeed respond correctly without context. Moreover, as expected the island of Ni'ihau is never mentioned in the contextless response. Additional examples of [PECoRe]{.smallcaps} usage for other generation tasks are provided in @sec-pecore-appendix-other-tasks.

![Example of context attribution for open-book QA using the Inseq-powered [PECoRe]{.smallcaps} demo. [Context-sensitive tokens]{bg-color="brand-color.lightgreen"} and [contextual cues]{bg-color="brand-color.lightblueclear"} are highlighted.](../figures/chap-4-pecore/inseq_api_example.pdf){#fig-chap4-inseq-api-example fig-pos="t" width=95%}

## Conclusion

We introduced [PECoRe]{.smallcaps}, a novel interpretability framework to detect and attribute context usage in language models' generations. [PECoRe]{.smallcaps} extends the common plausibility evaluation procedure adopted in interpretability research by proposing a two-step procedure to identify context-sensitive generated tokens and match them to contextual cues contributing to their prediction. We applied [PECoRe]{.smallcaps} to context-aware MT, finding that context-sensitive tokens and their disambiguating rationales can be detected consistently and with reasonable accuracy across several datasets, models and discourse phenomena. Moreover, an end-to-end application of our framework without human annotations revealed incorrect context usage, leading to problematic MT model outputs.

While our evaluation is mainly focused on the machine translation domain, thanks to its generality and its integration in the Inseq framework [PECoRe]{.smallcaps} can easily be applied to other context-dependent language generation tasks such as question answering and summarization, as also demonstrated in the previous section. Future applications of our methodology could investigate the usage of in-context demonstrations and chain-of-thought reasoning in large language models [@brown-etal-2020-language; @wei-etal-2022-chain], and explore [PECoRe]{.smallcaps} usage for different model architectures and input modalities. In the next chapter, we extend [PECoRe]{.smallcaps} for attributing context usage in retrieval-augmented generation tasks, where the model is expected to rely on external knowledge sources to produce answers to user queries.