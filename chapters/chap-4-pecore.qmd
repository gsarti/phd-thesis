# Quantifying Context Usage in Neural Machine Translation {#sec-pecore}

> This chapter is adapted from the paper *Quantifying the Plausibility of Context Reliance in Neural Machine Translation* [@sarti-etal-2024-quantifying].

> **Chapter Summary:** Establishing whether language models can use contextual information in a human-plausible way is important to ensure their trustworthiness in real-world settings. However, the questions of *when* and *which parts* of the context affect model generations are typically tackled separately, with current plausibility evaluations being practically limited to a handful of artificial benchmarks. To address this, we introduce **P**lausibility **E**valuation of **Co**ntext **Re**liance ([PECoRe]{.smallcaps}), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use [PECoRe]{.smallcaps} to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level phenomena. Finally, we apply our method to unannotated model translations to identify context-mediated predictions and highlight instances of (im)plausible context usage throughout generation.

## Introduction

Research in NLP interpretability defines various desiderata for rationales of model behaviors, i.e. the contributions of input tokens toward model predictions computed using feature attribution [@madsen-etal-2022-posthoc]. One of such properties is *plausibility*, corresponding to the alignment between model rationales and salient input words identified by human annotators [@jacovi-goldberg-2020-towards]. Low-plausibility rationales usually occur alongside generalization failures or biased predictions and can be useful to identify cases of models being "right for the wrong reasons" [@mccoy-etal-2019-right].

However, while plausibility has an intuitive interpretation for classification tasks involving a single prediction, extending this methodology to generative language models (LMs) presents several challenges. First, LMs have a large output space where semantically equivalent tokens (e.g. "PC" and "computer") are competing candidates for next-word prediction [@holtzman-etal-2021-surface]. Moreover, LMs generations are the product of optimization pressures to ensure independent properties such as semantic relatedness, topical coherence and grammatical correctness, which can hardly be captured by a single attribution score [@yin-neubig-2022-interpreting]. Finally, since autoregressive generation involves an iterative prediction process, model rationales could be extracted for every generated token. This raises the issue of *which generated tokens* can have plausible contextual explanations.

Recent attribution techniques for explaining language models incorporate contrastive alternatives to disentangle different aspects of model predictions (e.g. the choice of "meowing" over "screaming" for "The cat is \_\_\_" is motivated by semantic appropriateness, but not by grammaticality) [@ferrando-etal-2023-explaining; @sarti-etal-2023-inseq-fixed]. However, these studies avoid the issues above by narrowing the evaluation to a single generation step matching a phenomenon of interest. For example, given the sentence "The pictures of the cat \_\_\_", a plausible rationale for the prediction of the word "are" should reflect the role of "pictures" in subject-verb agreement. While this approach can be useful to validate model rationales, it confines plausibility assessment to a small set of handcrafted benchmarks where tokens with plausible explanations are known in advance. Moreover, it risks overlooking important patterns of context usage, including those that do not immediately match linguistic intuitions. In light of this, we suggest that identifying *which* generated tokens were most affected by contextual input information should be an integral part of plausibility evaluation for language generation tasks.

To achieve this goal, we propose a novel interpretability framework, which we dub **P**lausibility **E**valuation of **Co**ntext **Re**liance ([PECoRe]{.smallcaps}). [PECoRe]{.smallcaps} enables the end-to-end extraction of *cue-target token pairs* consisting of context-sensitive generated tokens and their respective influential contextual cues from language model generations, as shown in @fig-pecore-example. These pairs can uncover context dependence in naturally occurring generations and, for cases where human annotations are available, help quantify context usage plausibility in language models. Importantly, our approach is compatible with modern attribution methods using contrastive targets [@yin-neubig-2022-interpreting], avoids using reference translations to stay clear of problematic distributional shifts [@vamvas-sennrich-2021-limits], and can be applied on unannotated inputs to identify context usage in model generations.

![Examples of sentence-level and contextual English→Italian MT. Sentence-level translation contain [lack-of-context errors]{bg-color="brand-color.lightred"}. Instead, in the contextual case [context-sensitive source tokens]{bg-color="brand-color.lightorange"} are disambiguated using source (ⓢ) or target-based (ⓣ) [contextual cues]{color="brand-color.white" bg-color="brand-color.lightblueclear"} to produce correct [context-sensitive target tokens]{bg-color="brand-color.lightgreen"}. [PECoRe]{.smallcaps} enables the end-to-end extraction of [cue]{color="brand-color.white" bg-color="brand-color.lightblueclear"}-[target]{bg-color="brand-color.lightgreen"} pairs (e.g. [she]{color="brand-color.white" bg-color="brand-color.lightblueclear"}-[alla pastorella]{bg-color="brand-color.lightgreen"}, [le pecore]{color="brand-color.white" bg-color="brand-color.lightblueclear"}-[le]{bg-color="brand-color.lightgreen"}).](../figures/chap-4-pecore/example.pdf){#fig-pecore-example width=60%}

After formalizing our proposed approach in @sec-pecore-framework, we apply [PECoRe]{.smallcaps} to contextual machine translation (MT) to study the plausibility of context reliance in bilingual and multilingual MT models. While [PECoRe]{.smallcaps} can easily be used alongside encoder-decoder and decoder-only language models for interpreting context usage in any text generation task, we focus our evaluation on MT because of its constrained output space facilitating automatic assessment and the availability of MT datasets annotated with human rationales of context usage. We thoroughly test [PECoRe]{.smallcaps} on well-known discourse phenomena, benchmarking several context sensitivity metrics and attribution methods to identify cue-target pairs. We conclude by applying [PECoRe]{.smallcaps} to unannotated examples and showcasing some reasonable and questionable cases of context reliance in MT model translations.[^1]

In sum, we make the following contributions:

[^1]: Code: [`https://github.com/gsarti/pecore`](https://github.com/gsarti/pecore). The CLI command `inseq attribute-context` available in the Inseq library is a generalized [PECoRe]{.smallcaps} implementation: [`https://github.com/inseq-team/inseq`](https://github.com/inseq-team/inseq)

- We introduce [PECoRe]{.smallcaps}, an interpretability framework to detect and attribute context reliance in language models. [PECoRe]{.smallcaps} enables a quantitative evaluation of plausibility for language generation beyond the limited artificial settings explored in previous literature.
- We compare the effectiveness of context sensitivity metrics and feature attribution methods on the context-aware MT tasks, showing the limitations of metrics currently in use.
- We apply [PECoRe]{.smallcaps} to naturally-occurring translations to identify interesting discourse-level phenomena and discuss issues in the context usage abilities of context-aware MT models.

## Related Work {#sec-related-work}

[Context Usage in Language Generation]{.paragraph} An appropriate[^2] usage of input information is fundamental in tasks such as summarization [@maynez-etal-2020-faithfulness] to ensure the soundness of generated texts. While appropriateness is traditionally verified post-hoc using trained models [@durmus-etal-2020-feqa; @kryscinski-etal-2020-evaluating; @goyal-durrett-2021-annotating], recent interpretability works aim to gauge input influence on model predictions using internal properties of language models, such as the mixing of contextual information across model layers [@kobayashi-etal-2020-attention; @ferrando-etal-2022-measuring; @mohebbi-etal-2023-quantifying] or the layer-by-layer refinement of next token predictions [@geva-etal-2022-transformer; @belrose-etal-2023-eliciting]. Recent attribution methods can disentangle factors influencing generation in language models [@yin-neubig-2022-interpreting] and were successfully used to detect and mitigate hallucinatory behaviors [@tang-etal-2022-reducing; @dale-etal-2023-detecting; @dale-etal-2023-halomi]. Our proposed method adopts this intrinsic perspective to identify context reliance without ad-hoc trained components.

[^2]: We avoid using the term *faithfulness* due to its ambiguous usage in interpretability research.

[Context Usage in Neural Machine Translation]{.paragraph} Inter-sentential context is often fundamental for resolving discourse-level ambiguities during translation [@muller-etal-2018-large; @bawden-etal-2018-evaluating; @voita-etal-2019-good; @fernandes-etal-2023-translation]. However, MT systems are generally trained at the sentence level and fare poorly in realistic translation settings [@laubli-etal-2018-machine; @toral-etal-2018-attaining]. Despite advances in context-aware MT [@voita-etal-2018-context; @voita-etal-2019-context; @lopes-etal-2020-document; @majumder-etal-2022-baseline; @jin-etal-2023-challenges *inter alia*; surveyed by @maruf-etal-2021-survey], only a few works explored whether context usage in MT models aligns with human intuition. Notably, some studies focused on *which parts of context* inform model predictions, finding that supposedly context-aware MT models are often incapable of using contextual information [@kim-etal-2019-document; @fernandes-etal-2021-measuring] and tend to pay attention to irrelevant words [@voita-etal-2018-context], with an overall poor agreement between human annotations and model rationales [@yin-etal-2021-context]. Other works instead investigated *which parts of generated texts* are influenced by context, proposing various contrastive methods to detect gender biases, over/under-translations [@vamvas-sennrich-2021-contrastive; @vamvas-sennrich-2022-little], and to identify various discourse-level phenomena in MT corpora [@fernandes-etal-2023-translation]. While these two directions have generally been investigated separately, our work proposes a unified framework to enable an end-to-end evaluation of context-reliance plausibility in language models.

[Plausibility of Model Rationales]{.paragraph} Plausibility evaluation for NLP models has largely focused on classification models [@deyoung-etal-2020-eraser; @atanasova-etal-2020-diagnostic; @attanasio-etal-2023-ferret]. While few works investigate plausibility in language generation [@vafa-etal-2021-rationales; @ferrando-etal-2023-explaining], such evaluations typically involve a single generation step to complete a target sentence with a token connected to preceding information (e.g. subject/verb agreement, as in "The pictures of the cat [is/are]"), effectively biasing the evaluation by using a pre-selected token of interest. On the contrary, our framework proposes a more comprehensive evaluation of generation plausibility that includes the identification of context-sensitive generated tokens as an important prerequisite.

## The [PECoRe]{.smallcaps} Framework {#sec-pecore-framework}

[PECoRe]{.smallcaps} is a two-step framework for identifying context dependence in generative language models. First, *context-sensitive tokens identification* (CTI) selects which tokens among those generated by the model were influenced by the presence of the preceding context (e.g. the feminine options "alla pastorella, le" in @fig-pecore-example). Then, *contextual cues imputation* (CCI) attributes the prediction of context-sensitive tokens to specific cues in the provided context (e.g. the feminine cues "she, Le pecore" in @fig-pecore-example). **Cue-target pairs** formed by influenced target tokens and their respective influential context cues can then be compared to human rationales to assess the models' plausibility of context reliance for contextual phenomena of interest. @fig-pecore provides an overview of the two steps applied to the context-aware MT setting discussed by this work. A more general formalization of the framework for language generation is proposed in the following sections.

![The [PECoRe]{.smallcaps} framework. **Left:** Context-sensitive token identification (CTI). ⓵: A context-aware MT model translates source context ($C_x$) and current ($x$) sentences into target context ($C_{\hat y}$) and current ($\hat y$) outputs. ⓶: $\hat y$ is force-decoded in the non-contextual setting instead of natural output $\tilde y$. ⓷: Contrastive metrics are collected throughout the model for every $\hat y$ token to compare the two settings. ⓸: Selector $s_{\textsc{cti}}$ maps metrics to binary context-sensitive labels for every $\hat y_i$. **Right:** Contextual cues imputation (CCI). ⓵: Non-contextual target $\tilde y^*$ is generated from contextual prefix $\hat y_{<t}$. ⓶: Function $f_{\textsc{tgt}}$ is selected to contrast model predictions with ($\hat y_t$) and without ($\tilde y_t^*$) input context. ⓷: Attribution method $f_{\textsc{att}}$ using $f_{\textsc{tgt}}$ as target scores contextual cues driving $\hat y_t$ prediction. ⓸: Selector $s_{\textsc{cci}}$ selects relevant cues, and cue-target pairs are assembled.](../figures/chap-4-pecore/pecore.pdf){#fig-pecore}

### Notation

Let $X_{\textsc{ctx}}^{i}$ be the sequence of contextual inputs containing $N$ tokens from vocabulary $\mathcal{V}$, composed by current input $x$, generation prefix $y_{<i}$ and context $C$. Let $X_{\textsc{no-ctx}}^{i}$ be the non-contextual input in which $C$ tokens are excluded.[^3] $P_{\textsc{ctx}}^{i} = P\left(x,\, y_{<i},\,C,\,\theta\right)$ is the discrete probability distribution over $\mathcal{V}$ at generation step $i$ of a language model with $\theta$ parameters receiving contextual inputs $X_{\textsc{ctx}}^{i}$. Similarly, $P_{\textsc{no-ctx}}^{i} = P\left(x,\, y_{<i},\,\theta\right)$ is the distribution obtained from the same model for non-contextual input $X_{\textsc{no-ctx}}^{i}$. Both distributions are equivalent to vectors in the probability simplex in $\mathbb{R}^{|\mathcal{V}|}$, and we use $P_{\textsc{ctx}}(y_i)$ to denote the probability of next token $y_i$ in $P_{\textsc{ctx}}^{i}$, i.e. $P(y_i\,|\,x,\,y_{<i},\,C)$.

[^3]: In the context-aware MT example of @fig-pecore, $C$ includes source context $C_x$ and target context $C_y$.

### Context-sensitive Token Identification {#sec-cti}

CTI adapts the contrastive conditioning paradigm by @vamvas-sennrich-2021-contrastive to detect input context influence on model predictions using the contrastive pair $P_{\textsc{ctx}}^{i}, P_{\textsc{no-ctx}}^{i}$. Both distributions are relative to the **contextual target sentence** $\hat y = \{\hat y_1 \dots \hat y_n\}$, corresponding to the sequence produced by a decoding strategy of choice in the presence of input context. In @fig-pecore, the contextual target sentence $\hat y=$ "Sont-elles à l'hôtel?" is generated when $x$ and contexts $C_x, C_{\hat y}$ are provided as inputs, while **non-contextual target sentence** $\tilde y =$ "Ils sont à l'hôtel?" would be produced when only $x$ is provided. In the latter case, $\hat y$ is instead force-decoded from the non-contextual setting to enable a direct comparison of matching outputs. We define a set of **contrastive metrics** $\mathcal{M} = \{m_1, \dots, m_M\}$, where each $m: \displaystyle \Delta_{|\mathcal{V}|} \times \Delta_{|\mathcal{V}|} \mapsto \mathbb{R}$ maps a contrastive pair of probability vectors to a continuous score. For example, the difference in next token probabilities for contextual and non-contextual settings, i.e. $P_{\textsc{diff}}(\hat y_i) = P_\textsc{ctx}(\hat y_i) - P_\textsc{no-ctx}(\hat y_i)$, might be used for this purpose.[^4] Target tokens with high contrastive metric scores can be identified as *context-sensitive*, provided $C$ is the only added parameter in the contextual setting. Finally, a **selector** function $s_{\textsc{cti}}: \displaystyle \mathbb{R}^{| \mathcal{M} |} \mapsto \{0,1\}$ (e.g. a statistical threshold selecting salient scores) is used to classify every $\hat y_i$ as context-sensitive or not.

[^4]: We use $m^i$ to denote the result of $m\big( P_{\textsc{ctx}}^{i}, P_{\text{no-ctx}}^{i} \big)$. Several metrics are presented in @sec-cti-metrics.

### Contextual Cues Imputation {#sec-cci}

CCI applies the contrastive attribution paradigm [@yin-neubig-2022-interpreting] to trace the generation of every context-sensitive token in $\hat y$ back to context $C$, identifying cues driving model predictions.

::: {#def-contrastive-attribution-method}
Let $\mathcal{T}$ be the set of indices corresponding to context-sensitive tokens identified by the CTI step, such that $t \in \hat y$ and $\forall t \in \mathcal{T}, s_{\textsc{cti}}(m_1^{t}, \dots, m_M^{t}) = 1$. Let also $f_{\textsc{tgt}}: \Delta_{|\mathcal{V}|} \times \Delta_{|\mathcal{V}|} \mapsto \mathbb{R}$ be a **contrastive attribution target** function having the same domain and range as metrics in $\mathcal{M}$. The **contrastive attribution method** $f_{\textsc{att}}$ is a composite function quantifying the importance of contextual inputs to determine the output of $f_{\textsc{tgt}}$ for a given model with $\theta$ parameters.
:::

$$f_{\textsc{att}}(\hat y_{t}) = f_{\textsc{att}}(x, \hat y_{<t}, C, \theta, f_{\textsc{tgt}}) = f_{\textsc{att}}\big(x, \hat y_{<t}, C, \theta, f_{\textsc{tgt}}(P_{\textsc{ctx}}^{t}, P_{\textsc{no-ctx}}^{t})\big)$$


::: {#rem-contrastive-attribution-method}
Distribution $P_{\textsc{no-ctx}}^{t}$ in the above equation is from the contextual prefix $\hat y_{<t} = \{ \hat y_1, \dots, \hat y_{t - 1}\}$ (e.g. $\hat y_{<t} =$"Sont-" in @fig-pecore) and non-contextual inputs $X_{\textsc{no-ctx}}^{t}$. This is conceptually equivalent to predicting the next token of a new non-contextual sequence $\tilde y^*$ which, contrary to $\tilde y$, starts from a forced contextual prefix $\hat y_{<t}$ (e.g. "ils" in $\tilde y^* =$ "\underline{Sont-}ils à l'hôtel?" in @fig-pecore).
:::

::: {#rem-different-prob-for-same-target}
Provided that $P_{\textsc{ctx}}^{t}$ and $P_{\textsc{no-ctx}}^{t}$ depend respectively on contextual and non-contextual inputs $X_{\textsc{ctx}}^{t}, X_{\textsc{no-ctx}}^{t}$ despite using the same prefix $\hat y_{<t}$, probabilities $P_\textsc{ctx}(\hat y_t), P_\textsc{no-ctx}(\tilde y^*_t)$ are likely to differ even when $\hat y_t = \tilde y^*_t$, i.e. even when the next predicted token is the same, it is likely to have a different probability in the two settings, ultimately resulting in non-zero $f_\textsc{tgt}$ and $f_\textsc{att}(\hat y_t)$ scores.
:::

::: {#rem-generalized-contrastive-attribution}
Our formalization of $f_\textsc{att}$ generalizes the method proposed by @yin-neubig-2022-interpreting to support any target-dependent attribution method, such as popular gradient-based approaches [@simonyan-etal-2014-saliency; @sundararajan-etal-2017-ig], and any contrastive attribution target $f_\textsc{tgt}$.
:::

$f_\textsc{att}$ produces a sequence of attribution scores $A_t = \{a_1, \dots, a_N\}$ matching contextual input length $N$. From those, only the subset $A_{t\,\textsc{ctx}}$ of scores corresponding to context input sequence $C$ are passed to **selector** function $s_{\textsc{cci}}: \displaystyle \mathbb{R} \mapsto \{0,1\}$, which predicts a set $\mathcal{C}_{t}$ of indices corresponding to contextual cues identified by CCI, such that $\forall c \in \mathcal{C}_t, \forall a \in A_{t\,\textsc{ctx}}, s_\textsc{cci}(a_{c}) = 1$.

Having collected all context-sensitive generated token indices $\mathcal{T}$ using CTI and their contextual cues through CCI ($C_t$), [PECoRe]{.smallcaps} ultimately returns a sequence $S_\textsc{ct}$ of all identified cue-target pairs:

$$
\begin{aligned}
\mathcal{T} &= \text{CTI}(C, x, \hat y, \theta, \mathcal{M}, s_\textsc{cti}) = \{t \;|\; s_\textsc{cti}(m_1^t, \dots, m_M^t) = 1 \} \\
\mathcal{C} &= \text{CCI}(\mathcal{T}, C, x, \hat y, \theta,  f_\textsc{att}, f_\textsc{tgt}, s_\textsc{cci}) = \{ c \;|\; s_\textsc{cci}(a_c) = 1 \,\forall a_c \in A_{t\,\textsc{ctx}}, \forall t \in \mathcal{T}\} \\
S &= \texttt{PECoRe}(C, x, \theta, s_\textsc{cti}, s_\textsc{cci}, \mathcal{M}, f_\textsc{att}, f_\textsc{tgt}) = \{ (C_c, \hat y_t) \;|\; \forall t \in \mathcal{T}, \forall c \in \mathcal{C}_t, \forall \mathcal{C}_t \in \mathcal{C} \}
\end{aligned}
$$

## Context Reliance Plausibility in Context-aware MT

This section describes our evaluation of [PECoRe]{.smallcaps} in a controlled setup. We experiment with several contrastive metrics and attribution methods for CTI and CCI (@sec-cti-metrics, @sec-cci-metrics), evaluating them in isolation to quantify the performance of individual components. An end-to-end evaluation is also performed in @sec-cci-metrics to establish the applicability of [PECoRe]{.smallcaps} in a naturalistic setting.

### Experimental Setup {#sec-setup}

[Evaluation Datasets]{.paragraph} Evaluating generation plausibility requires human annotations for context-sensitive tokens in target sentences and disambiguating cues in their preceding context. To our knowledge, the only resource matching these requirements is SCAT [@yin-etal-2021-context], an English→French corpus with human annotations of anaphoric pronouns and disambiguating context on OpenSubtitles2018 dialogue translations [@lison-etal-2018-opensubtitles2018; @lopes-etal-2020-document]. SCAT examples were extracted automatically using lexical heuristics and thus contain only a limited set of anaphoric pronouns (*it, they* → *il/elle, ils/elles*), with no guarantees of contextual cues being found in preceding context. To improve our assessment, we select a subset of high-quality SCAT test examples containing contextual dependence, which we name SCAT+. Additionally, we manually annotate contextual cues in \textsc{DiscEval-MT} [@bawden-etal-2018-evaluating], another English→French corpus containing handcrafted examples for *anaphora resolution* (\textsc{ana}) and *lexical choice* (\textsc{lex}). Our final evaluation set contains 250 SCAT+ and 400 \textsc{DiscEval-MT} translations across two discourse phenomena.

[Models]{.paragraph} We evaluate two bilingual OpusMT models [@tiedemann-thottingal-2020-opus] using the Transformer base architecture [@vaswani-etal-2017-attention] (Small and Large), and mBART-50 1-to-many [@tang-etal-2021-multilingual], a larger multilingual MT model supporting 50 target languages, using the Transformers library [@wolf-etal-2020-transformers]. We fine-tune models using extended translation units [@tiedemann-scherrer-2017-neural] with contextual inputs marked by break tags such as "source context <brk> source current" to produce translations in the format "target context <brk> target current", where context and current target sentences are generated. We perform context-aware fine-tuning on 242k IWSLT 2017 English→French examples [@cettolo-etal-2017-overview], using a dynamic context size of 0-4 preceding sentences to ensure robustness to different context lengths and allow contextless usage. To further improve models' context sensitivity, we continue fine-tuning on the SCAT training split, containing 11k examples with inter- and intra-sentential pronoun anaphora.

::: {#tbl-model-accuracies}
| **Model** | **SCAT+** ||| **\textsc{DiscEval-MT} \textsc{(ana)}** ||| **\textsc{DiscEval-MT} \textsc{(lex)}** |||
|:----------|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
|           |**[bleu]{.smallcaps}**|**[ok]{.smallcaps}**|**[ok-cs]{.smallcaps}**|**[bleu]{.smallcaps}**|**[ok]{.smallcaps}**|**[ok-cs]{.smallcaps}**|**[bleu]{.smallcaps}**|**[ok]{.smallcaps}**|**[ok-cs]{.smallcaps}**|
|OpusMT Small *(default)* | 29.1 | 0.14 | - | 43.9 | 0.40 | - | 30.5 | 0.29 | - |
|OpusMT Small S+T$_{\textsc{ctx}}$ | *39.1* | *0.81* | 0.59 | *48.1* | *0.60* | 0.24 | *33.5* | *0.36* | 0.07 |
|OpusMT Large *(default)* | 29.0 | 0.16 | - | 39.2 | 0.41 | - | 31.2 | 0.31 | - |
|OpusMT Large S+T$_{\textsc{ctx}}$ | ***40.3*** | ***0.83*** | 0.58 | *48.9* | ***0.68*** | 0.31 | ***34.8*** | ***0.38*** | 0.10 |
|mBART-50 *(default)* | 23.8 | 0.26 | - | 33.4 | 0.42 | - | 24.5 | 0.25 | - |
|mBART-50 S+T$_{\textsc{ctx}}$ | *37.6* | *0.82* | 0.55 | ***49.0*** | *0.62* | 0.32 | *29.3* | *0.30* | 0.07 |

Translation quality of $\textsc{En}\rightarrow\textsc{Fr}$ MT models before *(default)* and after *(S+T*$_{\text{ctx}}$*)* context-aware MT fine-tuning. **[ok]{.smallcaps}**: % of translations with correct disambiguation for discourse phenomena. **[ok-cs]{.smallcaps}**: % of translations where the correct disambiguation is achieved only when context is provided.
:::

[Model Disambiguation Accuracy]{.paragraph} We estimate contextual disambiguation accuracy by verifying whether annotated (gold) context-sensitive words are found in model outputs. Results before and after context-aware fine-tuning are shown in @tbl-model-accuracies. We find that fine-tuning improves translation quality and disambiguation accuracy across all tested models, with larger gains for anaphora resolution datasets closely matching fine-tuning data. To gain further insight into these results, we use context-aware models to translate examples with and without context and identify a subset of *context-sensitive translations* (\textsc{ok-cs}) for which the correct target word is generated only when input context is provided to the model. Interestingly, we find a non-negligible amount of translations that are correctly disambiguated even in the absence of input context (corresponding to \textsc{ok} minus \textsc{ok-cs} in @tbl-model-accuracies). For these examples, the correct prediction of ambiguous words aligns with model biases, such as defaulting to masculine gender for anaphoric pronouns [@stanovsky-etal-2019-evaluating] or using the most frequent sense for word sense disambiguation. Provided that such examples are unlikely to exhibit context reliance, we focus particularly on the \textsc{ok-cs} subset results in our following evaluation.

### Metrics for Context-sensitive Target Identification {#sec-cti-metrics}

The following contrastive metrics are evaluated for detecting context-sensitive tokens in the CTI step.

**Relative Context Saliency** We use contrastive gradient norm attribution [@yin-neubig-2022-interpreting] to compute input importance towards predicting the next token $\hat y_i$ with and without input context. Positive importance scores are obtained for every input token using the L2 gradient vectors norm [@bastings-etal-2022-will], and relative context saliency is obtained as the proportion between the normalized importance for context tokens $c \in C_x, C_y$ and the overall input importance, following previous work quantifying MT input contributions [@voita-etal-2021-analyzing; @ferrando-etal-2022-towards; @edman-etal-2024-character].

$$\nabla_\textsc{ctx} (P_{\textsc{ctx}}^{i}, P_{\textsc{no-ctx}}^{i}) = \frac{\sum_{c \in C_x, C_y} \big\| \nabla_c \big( P_\textsc{ctx}(\hat y_i) - P_\textsc{no-ctx}(\hat y_i) \big) \big\|}{\sum_{t \in X_{\textsc{ctx}}^{i}} \big\| \nabla_t \big( P_\textsc{ctx}(\hat y_i) - P_\textsc{no-ctx}(\hat y_i) \big) \big\|}$$

**Likelihood Ratio (LR)** and **Pointwise Contextual Cross-mutual Information (P-CXMI)** Proposed by @vamvas-sennrich-2021-contrastive and @fernandes-etal-2023-translation respectively, both metrics frame context dependence as a ratio of contextual and non-contextual probabilities.

$$\text{LR}(P_{\textsc{ctx}}^{i}, P_{\textsc{no-ctx}}^{i}) = \frac{P_{\textsc{ctx}}(\hat{y}_i)}{P_{\textsc{ctx}}(\hat{y}_i) + P_{\textsc{no-ctx}}(\hat{y}_i)}$$

$$\text{P-CXMI}(P_{\textsc{ctx}}^{i}, P_{\textsc{no-ctx}}^{i}) = - \log \frac{P_{\textsc{ctx}}(\hat{y}_i)}{P_{\textsc{no-ctx}}(\hat{y}_i)}$$

**KL-Divergence** [@kullback-leibler-1951-information] between $P_{\textsc{ctx}}^{i}$ and $P_{\textsc{no-ctx}}^{i}$ is the only metric we evaluate that considers the full distribution rather than the probability of the predicted token. We include it to test the intuition that the impact of context inclusion might extend beyond top-1 token probabilities.

$$D_{\text{KL}}(P_{\textsc{ctx}}^{i} \| P_{\textsc{no-ctx}}^{i}) = \sum_{\hat{y}_i \in \mathcal{V}} P_{\textsc{ctx}}(\hat{y}_i) \log \frac{P_{\textsc{ctx}}(\hat{y}_i)}{P_{\textsc{no-ctx}}(\hat{y}_i)}$$

### CTI Plausibility Results {#sec-cti-results}

@fig-marian-big-f1-cti presents our metrics evaluation for CTI, with results for the full test sets and the subsets of context-sensitive sentences (\textsc{ok-cs}) highlighted in @tbl-model-accuracies. To keep our evaluation simple, we use a naive $s_{\textsc{cti}}$ selector tagging all tokens with metric scores one standard deviation above the per-example mean as context-sensitive. We also include a stratified random baseline matching the frequency of occurrence of context-sensitive tokens in each dataset. Datapoints in @fig-marian-big-f1-cti are sentence-level macro F1 scores computed for every dataset example.

![Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) using OpusMT Large on the full datasets (left) or on \textsc{ok-cs} context-sensitive subsets (right).](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat-target_box.pdf){#fig-marian-big-f1-cti}

Pointwise metrics (LR, P-CXMI) show high plausibility for the context-sensitive subsets \textsc{ok-cs} across all datasets and models but achieve lower performances on the full test set, especially for lexical choice phenomena less present in MT models' training. KL-Divergence performs on par or better than pointwise metrics, suggesting that distributional shifts beyond top prediction candidates can provide useful information to detect context sensitivity. On the contrary, the poor performance of context saliency indicates that context reliance in aggregate cannot reliably predict context sensitivity. A manual examination of misclassified examples reveals several context-sensitive tokens that were not annotated as such since they did not match datasets' phenomena of interest but were still identified by CTI metrics. This further underscores the importance of data-driven end-to-end approaches like [PECoRe]{.smallcaps} to limit the influence of selection bias during evaluation.

### Methods for Contextual Cues Imputation {#sec-cci-metrics}

The following attribution methods are evaluated for detecting contextual cues in the CCI step.

**Contrastive Gradient Norm** [@yin-neubig-2022-interpreting] estimates input tokens' contributions towards predicting a target token instead of a contrastive alternative. We use this method to explain the generation of context-sensitive tokens in the presence and absence of context.

$$A_{t\,\textsc{ctx}} = \{\,\| \nabla_c \big(f_\textsc{tgt}(P_{\textsc{ctx}}^{i}, P_{\textsc{no-ctx}}^{i}) \big)\|\,|\, \forall c \in C\}$$

For the choice of $f_\text{tgt}$, we evaluate both probability difference $P_\textsc{ctx}(\hat y_i) - P_\textsc{no-ctx}(\hat y_i)$, conceptually similar to the original formulation, and the KL-Divergence of contextual and non-contextual distributions $D_{\text{KL}}(P_{\textsc{ctx}}^{i} \| P_{\textsc{no-ctx}}^{i})$. We use $\nabla_\text{diff}$ and $\nabla_\text{KL}$ to identify gradient norm attribution in the two settings. $\nabla_\text{KL}$ scores can be seen as the contribution of input tokens towards the shift in probability distribution caused by the presence of input context.

[Attention Weights]{.paragraph} Following previous work, we use the mean attention weight across all heads and layers (Attention Mean, @kim-etal-2019-document) and the weight for the head obtaining the highest plausibility per-dataset (Attention Best, @yin-etal-2021-context) as importance measures for CCI. Attention Best can be seen as a best-case estimate of attention performance but is not a viable metric in real settings, provided that the best attention head to capture a phenomenon of interest is unknown beforehand. Since attention weights are model byproducts unaffected by predicted outputs, we use only attention scores for the contextual setting $P_{\textsc{ctx}}^{i}$ and ignore the contextless alternative when using these metrics.

### CCI Plausibility Results {#sec-cci-results}

We conduct a controlled CCI evaluation using gold context-sensitive tokens as a starting point to attribute contextual cues. This corresponds to the baseline plausibility evaluation described in @sec-related-work, allowing us to evaluate attribution methods in isolation, assuming perfect identification of context-sensitive tokens. @fig-marian-big-f1-cci presents our results. Scores in the right plot are relative to the context-aware OpusMT Large model of @sec-cti-results using both source and target context. Instead, the left plot presents results for an alternative version of the same model that was fine-tuned using only source context (i.e. translating $C_x, x \rightarrow y$ without producing target context $C_y$). Source-only context was used in previous context-aware MT studies [@fernandes-etal-2022-quality], and we include it in our analysis to assess how the presence of target context impacts model plausibility. We finally validate the end-to-end plausibility of [PECoRe]{.smallcaps}-detected pairs using context-sensitive tokens identified by the best CTI metric from @sec-cti-results (KL-Divergence) as the starting point for CCI, and using a simple statistical selector equivalent to the one used for CTI evaluation.

![Macro F1 of CCI methods over full datasets using OpusMT Large models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings).](../figures/chap-4-pecore/macro_f1_cci_marian-big-scat+tgt_box.pdf){#fig-marian-big-f1-cci}

First, contextual cues are more easily detected for the source-only model using all evaluated methods. This finding corroborates previous evidence highlighting how context usage issues might emerge when lengthy context is provided [@fernandes-etal-2021-measuring; @shi-etal-2023-large]. When moving from gold CTI tags to the end-to-end setting (E2E) we observe a larger drop in plausibility for the SCAT+ and \textsc{DiscEval-MT} \textsc{ana} datasets that more closely match the fine-tuning data of analyzed MT models. This suggests that standard evaluation practices may overestimate model plausibility for in-domain settings and that our proposed framework can effectively mitigate this issue. Interestingly, the Attention Best method suffers the most from end-to-end CCI application, while other approaches are more mildly affected. This can result from attention heads failing to generalize to other discourse-level phenomena at test time, providing further evidence of the limitations of attention as an explanatory metric [@jain-wallace-2019-attention; @bastings-filippova-2020-elephant]. While $\nabla_\text{KL}$ and $\nabla_\text{diff}$ appear as the most robust choice across the two datasets, per-example variability remains high across the board, leaving space for improvement for more plausible attribution methods in future work.

## Detecting Context Reliance in the Wild {#sec-analysis}

We conclude our analysis by applying the [PECoRe]{.smallcaps} method to the popular Flores-101 MT benchmark [@goyal-etal-2022-flores], containing groups of 3-5 contiguous sentences from English Wikipedia. While in previous sections, labeled examples were used to evaluate the effectiveness of [PECoRe]{.smallcaps} components, here we apply our framework end-to-end to unannotated MT outputs and inspect resulting cue-target pairs to identify successes and failures of context-aware MT models.

Specifically, we apply [PECoRe]{.smallcaps} to the context-aware OpusMT Large and mBART-50 models of @sec-setup, using KL-Divergence as CTI metric and $\nabla_\text{KL}$ as CCI attribution method. We set $s_\textsc{cti}$ and $s_\textsc{cci}$ to two standard deviations above the per-example average score to focus our analysis on very salient tokens.

::: {#tbl-pecore-examples}
| **1. Acronym Translation (English → French, correct but more generic)** |
|:---------------------------------------------------------------|
| $C_x:$ Across the United States of America, there are approximately 400,000 known cases of [Multiple Sclerosis (MS)]{bg-color="brand-color.lightblueclear"} [...] |
| $C_y:$ Aux États-Unis, il y a environ 400 000 cas connus de sclérose en plaques [...] |
| $x:$ MS affects the central nervous system, which is made up of the brain, the spinal cord and the optic nerve. |
| $\tilde y:$ [La SEP]{bg-color="brand-color.lightredclear"} affecte le système nerveux central, composé du cerveau, de la moelle épinière et du nerf optique. |
| $\hat y:$ [La maladie]{bg-color="brand-color.lightgreen"} affecte le système nerveux central, composé du cerveau, de la moelle épinière et du nerf optique. |
| **2. Anaphora Resolution (English → French, incorrect)** |
| $C_x:$ The terrified King and Madam Elizabeth were forced back to Paris by a [mob]{bg-color="brand-color.lightblueclear"} of [market women]{bg-color="brand-color.lightblueclear"}. |
| $C_y:$ Le roi et Madame Elizabeth ont été forcés à revenir à Paris par une foule de [femmes]{bg-color="brand-color.lightblueclear"} du marché. |
| $x:$ In a carriage, they traveled back to Paris surrounded by a mob of people screaming and shouting threats [...] |
| $\tilde y:$ Dans une carriole, [ils]{bg-color="brand-color.lightredclear"} sont retournés à Paris entourés d'une foule de gens hurlant et criant des menaces [...] |
| $\hat y:$ Dans une carriole, [elles]{bg-color="brand-color.lightgreen"} sont *retournées* à Paris *entourées* d'une foule de gens hurlant et criant des menaces [...] |
| **3. Numeric format cohesion (English → French, incorrect)** |
| $C_x:$ The games kicked off at [10:00]{bg-color="brand-color.lightblueclear"}am with great weather apart from mid morning drizzle [...] |
| $C_y:$ Les matchs se sont écoulés à [10:00]{bg-color="brand-color.lightblueclear"} du matin avec un beau temps à part la nuée du matin [...] |
| $x:$ South Africa started on the right note when they had a comfortable 26-00 win against Zambia. |
| $\tilde y:$ L'Afrique du Sud a commencé sur la bonne note quand ils ont eu une confortable victoire de [26]{bg-color="brand-color.lightredclear"} contre le Zambia. |
| $\hat y:$  L'Afrique du Sud a commencé sur la bonne note quand ils ont eu une confortable victoire de [26:00]{bg-color="brand-color.lightgreen"} contre le Zambia. |
| **4. Lexical cohesion (English → Turkish, correct)** |
| $C_x:$ The activity of all stars in the system was found to be driven by their luminosity, their rotation, and nothing else. |
| $C_y:$ Sistemdeki bütün ulduzların faaliyetlerinin, parlaklıkları, [rotasyonları]{bg-color="brand-color.lightblueclear"} ve başka hiçbir şeyin etkisi altında olduğunu ortaya çıkardılar. |
| $x:$ The luminosity and rotation are used together to determine a star's Rossby number, which is related to plasma flow. |
| $\tilde y:$ Parlaklık ve [döngü]{bg-color="brand-color.lightredclear"}, bir *yıldızın plazm* akışıyla ilgili Rossby sayısını belirlemek için birlikte kullanılıyor. |
| $\hat y:$ Parlaklık ve [rotasyon]{bg-color="brand-color.lightgreen"}, bir *ulduzun plazma* akışıyla ilgili Rossby sayısını belirlemek için birlikte kullanılıyor. |

Flores-101 examples with cue-target pairs identified by [PECoRe]{.smallcaps} in OpusMT Large (1,2) and mBART-50 (3,4) contextual translations. [Context-sensitive tokens]{bg-color="brand-color.lightgreen"} generated instead of their [non-contextual]{bg-color="brand-color.lightredclear"} counterparts are identified by CTI, and [contextual cues]{bg-color="brand-color.lightblueclear"} justifying their predictions are retrieved by CCI. *Other changes* in $\hat y$ are not considered context-sensitive by [PECoRe]{.smallcaps}.
:::

@tbl-pecore-examples shows some examples annotated with [PECoRe]{.smallcaps} outputs. In the first example, the acronym MS, standing for Multiple Sclerosis, is translated generically as *la maladie* (the illness) in the contextual output, but as *SEP* (the French acronym for MS, i.e. *sclérose en plaques*) when context is not provided. [PECoRe]{.smallcaps} shows how this choice is mostly driven by the MS mention in source context $C_x$ while the term *sclérose en plaques* in target context $C_y$ is not identified as influential, possibly motivating the choice for the more generic option.

In the second example, the prediction of pronoun *elles* (they, feminine) depends on the context noun phrase *mob of market women* (*foule de femmes du marché* in French). However, the correct pronoun referent is *Le roi et Madame Elizabeth* (*the king and Madam Elizabeth*), so the pronoun should be the masculine default *ils* commonly used for mixed gender groups in French. [PECoRe]{.smallcaps} identifies this as a context-dependent failure due to an issue with the MT model's anaphora resolution.

The third example presents an interesting case of erroneous numeric format cohesion that not be detected from pre-defined linguistic hypotheses. In this sentence, the score *26-00* is translated as *26* in the contextless output and as *26:00* in the context-aware translation. The *10:00* time indications found by [PECoRe]{.smallcaps} in the contexts suggest this is a case of problematic lexical cohesion.

Finally, we include an example of context usage for English→Turkish translation to test the contextual capabilities of the default mBART-50 model without context-aware fine-tuning. Again, [PECoRe]{.smallcaps} shows how the word *rotasyon* (rotation) is selected over *döngü* (loop) as the correct translation in the contextual case due to the presence of the lexically similar word *rotasyonları* in the previous context.

## Conclusion

In this work, we introduced [PECoRe]{.smallcaps}, a novel interpretability framework to detect and attribute context usage in language models' generations. [PECoRe]{.smallcaps} extends the common plausibility evaluation procedure adopted in interpretability research by proposing a two-step procedure to identify context-sensitive generated tokens and match them to contextual cues contributing to their prediction. We applied [PECoRe]{.smallcaps} to context-aware MT, finding that context-sensitive tokens and their disambiguating rationales can be detected consistently and with reasonable accuracy across several datasets, models and discourse phenomena. Moreover, an end-to-end application of our framework without human annotations revealed incorrect context usage, leading to problematic MT model outputs.

While our evaluation is focused on the machine translation domain, [PECoRe]{.smallcaps} can easily be applied to other context-dependent language generation tasks such as question answering and summarization. Future applications of our methodology could investigate the usage of in-context demonstrations and chain-of-thought reasoning in large language models [@brown-etal-2020-language; @wei-etal-2022-chain] and pave the way for trustworthy citations in retrieval-augmented generation systems [@borgeaud-etal-2022-rag].