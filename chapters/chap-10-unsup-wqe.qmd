# Unsupervised MT Error Detection and Human Disagreement {#sec-chap-10-unsup-wqe}

::: {.callout-note appearance="simple" icon="false"}

This chapter is adapted from the paper *Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement* [@sarti-etal-2025-unsupervised].

:::

::: {.callout-note icon="false"}

## Chapter Summary

Word-level quality estimation (WQE) aims to automatically identify fine-grained error spans in machine-translated outputs and has found many uses, including assisting translators during post-editing. Modern WQE techniques are often expensive, involving prompting of large language models or ad-hoc training on large amounts of human-labeled data. In this work, we investigate efficient alternatives exploiting recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. In our evaluation spanning 14 metrics across 12 translation directions, we quantify the impact of human label variation on metric performance by using multiple sets of human labels. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.
:::

## Introduction {#sec-unsup-wqe-intro}

Word-level error spans are widely used in machine translation (MT) evaluation to obtain robust and fine-grained estimates of translation quality [@lommel-etal-2014-using; @freitag-etal-2021-experts; @freitag-etal-2021-results; @kocmi-etal-2024-error].
Due to the cost of manual annotation, word-level quality estimation (WQE) was proposed for assisting in annotating error spans over MT outputs [@zouhar-etal-2025-ai].
Modern WQE approaches generally rely on costly inference with large language models (LLMs) or ad-hoc training with large amounts of human-annotated texts [@fernandes-etal-2023-devil; @kocmi-federmann-2023-large; @guerreiro-etal-2024-xcomet], making them impractical for less resourced settings [@zouhar-etal-2024-fine].

To improve the efficiency of MT quality assessment, several works explored the use of signals derived from the internals of neural MT systems [@fomicheva-etal-2020-unsupervised; @fomicheva-etal-2021-eval4nlp; @leiter-etal-2024-towards], for identifying problems in MT outputs, such as hallucinations [@guerreiro-etal-2023-optimal; @guerreiro-etal-2023-looking; @dale-etal-2023-detecting; @dale-etal-2023-halomi; @himmi-etal-2024-enhanced].
However, previous works focus on sentence-level metrics for overall translation quality, and do not evaluate performance on multiple label sets due to high annotation costs [@fomicheva-etal-2022-mlqe; @zerva-etal-2024-findings].

![Example of German$\rightarrow$English translation with two sets of human word-level error span annotations and two examples of continuous and binary WQE metrics.](../figures/chap-10-unsup-wqe/intro_fig.pdf){#fig-intro-fig fig-pos="t" width=65%}

In this work, we conduct a more comprehensive evaluation spanning 10 unsupervised metrics derived from models' inner representations and predictive distributions to identify translation errors at the word level. We test three open-source multilingual MT models and LLMs of different sizes across 12 translation directions, including typologically diverse languages and challenging textual domains.
Importantly, we focus on texts with *multiple* human annotations to measure the impact of individual annotator preferences on metric performance, setting a "human-level" baseline for the WQE task.

We address the following research questions:
- How accurate are unsupervised WQE metrics in detecting MT errors compared to trained metrics and human annotators?
- Are popular supervised WQE metrics well-calibrated?
- Are the relative performances of WQE metrics affected by the variability in human error annotations?

We conclude with recommendations for improving the evaluation and usage of future WQE systems.

## Additional Background {#sec-unsup-wqe-related-work}

[Unsupervised Quality Estimation for Machine Translation]{.paragraph} The use of unsupervised signals from MT models for the task of MT quality estimation was introduced by [@fomicheva-etal-2020-unsupervised]. Their evaluation showed that high-performing unsupervised methods could rival state-of-the-art supervised QE models in predicting translation quality at the sentence level. Since then, several evaluation campaigns assessed the quality of QE methods [@specia-etal-2021-findings; @zerva-etal-2022-findings; @blain-etal-2023-findings; @zerva-etal-2024-findings], including a shared task dedicated to explainable QE metrics [@fomicheva-etal-2021-eval4nlp]. However, such evaluations have typically focused on segment-level evaluation quality, with word-level error spans being generally obtained by attributing the predictions of supervised segment-level metrics [@rubino-etal-2021-error; @rei-etal-2023-inside]. By contrast, recent work on LLMs evaluates various metrics to detect errors from the generator model, without additional systems involved, both at the sentence [@fadeeva-etal-2023-lm] and at the token level [@fadeeva-etal-2024-fact]. Our work follows the latter approach by testing unsupervised metrics extracted from an MT model during generation, akin to out-of-distribution detection in signal processing research [@ood-detection].

[Actionable Insights from Interpretability]{.parargraph} Advances in interpretability research have elucidated multiple mechanisms underlying decision-making, knowledge representation, and biases in LMs [@ferrando-etal-2024-primer]. However, a better understanding of model's inner workings often did not translate to tangible gains in model design and other practical applications, which remain rarely explored [@mosbach-etal-2024-insights]. Some examples in this direction include using targeted machine unlearning methods for safety-critical scenarios [@barez-etal-2025-open], or the use of attribution for trustworthy context citations in LM generations [@cohenwang-etal-2024-contextcite; @sarti-etal-2024-quantifying; @qi-etal-2024-model]. In this work, signals extracted from model internals are employed to detect errors in models' generated outputs.

[Uncertainty Estimation for Language Models]{.paragraph} The estimation of uncertainty in language models has garnered increasing attention [@baan-etal-2023-uncertainty], particularly in the context of generation tasks for which the set of plausible responses is large [@giulianelli-etal-2023-comes]. Predictive uncertainty is typically decomposed into its *aleatoric* and *epistemic* components, representing respectively the irreducible variability in the modeled phenomena, and the improvable confidence in model predictions [@aleatoric-epistemic]. Popular methods for uncertainty estimation involve the calibration of predictive probabilities to reflect aleatoric uncertainty [@jiang-etal-2020-know; @ulmer-etal-2022-exploring; @zhao2023calibrating; @chen-etal-2023-close], and conformal sets prediction [@zerva-martins-2024-conformalizing; @ravfogel-etal-2023-conformal]. In this work, we exploit uncertainty signals from the predictive distribution of MT models and its internal processing for efficiently predicting the resulting generation quality at a fine-grained, token-level scale.

[Human Label Variation]{.paragraph} Human label variation is a type of uncertainty that arises from the inherent variability in human judgments [@plank-etal-2014-linguistically; @plank-2022-problem], which can be hard to disentangle from actual annotation mistakes [@snow-etal-2008-cheap; @weber-genzel-etal-2024-varierr]. The usage of multiple references was recently recommended to ensure a sound evaluation of generative LMs reflecting human-plausible levels of variability [@giulianelli-etal-2023-comes], contrary to common practices employing a single set of "gold" labels. In our analysis on QE4PE data containing multiple edits, we adopt a perspectivist approach[^perspectivist] to ensure a robust assessment of WQE metrics by accounting for annotators' disagreement [@uma2021learning].

[^perspectivist]: [pdai.info](https://pdai.info/)

## Models and Datasets {#sec-unsup-wqe-data}

We use datasets containing error annotations or post-edits on the outputs of open-source models to extract unsupervised WQE metrics on real model outputs, avoiding possible confounders.
We select the following datasets, summarized in @tbl-datasets-summary:

[DivEMT]{.paragraph} was created by @sarti-etal-2022-divemt to evaluate the impact of language typology on MT quality, and how that would influence the productivity of human post-editors working with those systems. The dataset includes out-of-English machine translations towards six typologically diverse target languages (EN$\rightarrow$AR,IT,NL,TR,UK,VI) produced by Google Translate and mBART-50 1-to-many for a subset of Wiki texts from the FLORES dataset [@goyal-etal-2022-flores], with edits made by professional translators. In this work, we evaluate unsupervised metrics on the mBART-50 1-to-many model, converting the human post-edits into token-level labels to perform a cross-lingual comparisons over a fixed set of examples. The original multilingual BART (mBART-25) model by [@liu-etal-2020-multilingual-denoising] is an encoder-decoder Transformer model pre-trained on monolingual documents in 25 languages with the BART denoising objective for sequence-to-sequence learning [@lewis-etal-2020-bart]. [@tang-etal-2021-multilingual] extended mBART-25 by including 25 additional languages during pre-training and performing multilingual translation fine-tuning across 50 languages. In this work, we employ the *one-to-many* version of the model specialized in out-of-English translation that was employed by [@sarti-etal-2022-divemt] to produce part of the translations post-edited by DivEMT annotators.[^mbart-model] The model is a standard Transformer with 12 layers of encoder and 12 layers of decoder, with model dimension of 1024 and 16 attention heads (~680M parameters).

[^mbart-model]: [`facebook/mbart-large-50-one-to-many-mmt`](https://huggingface.co/facebook/mbart-large-50-one-to-many-mmt)

[WMT24]{.paragraph} employed in this study is taken from the General Machine Translation Shared Task at WMT 2024 [@kocmi-etal-2024-findings]. It contains evaluation of several machine translation systems across English$\rightarrow${Czech, Hindi, Japanese, Chinese, Russian} (634 segments per language) and Czech$\rightarrow$Ukrainian (1954 segments). The human evaluation was done with the Error Span Annotation protocol (ESA, @kocmi-etal-2024-error), which has human annotators highlighting erroneous spans in the translation and marking them as either [minor]{.smallcaps} or [major]{.smallcaps} errors. This dataset covers the *news*, *social*, and *speech* (with automatic speech recognition) domains. We adopt the official prompting setup from the WMT24 campaign, using the Aya23 model alongside the provided prompt and 3 in-context translation examples per language to ensure uniformity with previous results.[^wmt-collect] Using WMT24 allows us to extend our evaluation to a state-of-the-art LLM, given the popularity of such systems in MT [@kocmi-etal-2023-findings]. Aya23 is a large language model introduced by @aya23 to improve the multilingual capabilities of the original Aya model [@ustun-etal-2024-aya] on a selected set of 23 languages. The model was included in the WMT24 evaluation of @kocmi-etal-2024-findings, resulting in the best translation performances among tested open-source models. The model is a decoder-only Transformer model with 40 layers, a model dimension of 8196 and 64 attention heads per layer.

[^wmt-collect]: [`wmt-conference/wmt-collect-translations`](https://github.com/wmt-conference/wmt-collect-translations)

[QE4PE]{.paragraph} The QE4PE dataset was created by [@sarti-etal-2025-qe4pe] for measuring the effect of word-level error highlights when included in real-world human post-editing workflows. The QE4PE data provides granular behavioral metrics to evaluate the speed and quality of post-editing of 12 annotators for En$\rightarrow$It and En$\rightarrow$Nl across two challenging textual domains (social posts and biomedical abstracts) and four error span highlighting modalities, including the unsupervised Surprisal MCD~var~ method and the supervised [xcomet-xxl]{.smallcaps} we also test in this study. Provided that the presence of error span highlights was found to influence the editing choices of human editors, we limit our evaluation to the six human annotators per language that post-edited sentences without any highlights (3 for the *Oracle Post-edit* task to produce initial human-based highlights, and 3 for the *No Highlight* modality in the main task). This prevents us from biasing our evaluation of WQE metrics in favor of the metrics that influenced editing choices. As for DivEMT, we use the post-edits over translations produced by the NLLB 3.3B model [@nllb-2024-scaling] to produce token-level error spans, enabling an evaluation of WQE metrics across multiple annotation sets. No Language Left Behind (NLLB) is a collection of multilingual MT models covering up to 202 languages, including low-resource directions [@nllb-2024-scaling]. The largest NLLB model available is a mixture-of-experts model with 54.4B parameters, which comes with high computational cost. In this work we employ the largest available dense variant of the model (~3.3B parameters), which was used by [@sarti-etal-2025-qe4pe] for collecting the QE4PE post-editing dataset.[^nllb-model] The model is an encoder-decoder Transformer with 24 layers for each module, a model dimension of 2048 and 16 attention heads per layer.

[^nllb-model]: [`facebook/nllb-200-3.3B`](https://huggingface.co/facebook/nllb-200-3.3B)

{{< include ../tables/chap-10-unsup-wqe/_datasets-summary.qmd >}}

## Evaluated Metrics {#sec-unsup-wqe-metrics}

The following metrics were evaluated using the Inseq library [@sarti-etal-2023-inseq; @sarti-etal-2024-democratizing].

[Predictive Distribution Metrics]{.paragraph} We use the **Surprisal** of the predicted token $t^{*}$, as negative log-probablity $-\log p(t^{*}_i|t_{<i})$, and the **Entropy** $H$ of the output distribution $P_N$ over vocabulary $V$, $-\sum_{i=1}^{|V|} p(t_i|t_{<i}) \log_2 p(t_i|t_{<i})$, as simple metrics to quantify pointwise and full prediction uncertainty [@fomicheva-etal-2020-unsupervised]. For surprisal, we also compute its expectation (**MCD~avg~**) and variance (**MCD~var~**) with $n=10$ steps of Monte Carlo Dropout (MCD, @gal-ghahramani-2016-dropout) to obtain a robust estimate and a measure of epistemic uncertainty in predictions, respectively.^[Epistemic uncertainty reflects models' lack of knowledge rather than data ambiguity. MCD is tested only on encoder-decoder models since Aya layers do not include dropout.] **Monte Carlo Dropout (MCD)** is a technique introduced by [@gal-ghahramani-2016-dropout] for estimating model uncertainty at inference time. MCD uses the dropout mechanism in neural networks [@dropout], commonly employed for regularization during training, at inference time to produce a set of noisy predictions from a unique model, approximating Bayesian inference. For a given input $x$, $T$ forward passes are performed through the network. In each pass $t \in T$, a different random dropout mask $\Theta_t$ is applied, resulting in a slightly different output probabilities $p(x \mid \Theta_t)$. The set of $T$ predictions $\{p(x \mid \Theta_1), \dots, p(x \mid \Theta_T)\}$ can be seen as samples from an approximate posterior distribution. In this work, we employ the mean of the negative log probabilities as a robust estimate of surprisal:

$$\text{Surprisal MCD}_{\text{avg}} = \hat y_{\text{MCD}} = \frac{1}{T} \sum_{t=1}^{T} - \log p(x | \Theta_t)$$

Moreover, we estimate predictive uncertainty by calculating the variance of predictive probabilities under the same setup:

$$\text{Surprisal MCD}_{\text{var}} = \frac{1}{T} \sum_{t=1}^{T} \big(- \log p(x | \Theta_t) - \hat y_{\text{MCD}} \big)$$

[Vocabulary Projections]{.paragraph} The Logit Lens (LL, @logitlens) is an interpretability technique used to understand the internal workings of Transformer models, particularly how their predictions evolve layer by layer. Activations $h_l$ produced by the model layer $l$ are projected to vocabulary space using the model unembedding matrix, $W_U$, normally used to produce output logits. We use the Logit Lens to extract probability distributions $P_0, \dots, P_{N-1}$ over $V$ from intermediate activations at every layer $l_0, \dots, l_{N-1}$ of the decoder.
We use the surprisal for the final prediction at every layer (**LL-Surprisal**) to assess the presence of layers with high sensitivity to wrong predictions. For the NLLB and mBART-50 models we also apply a final layer normalization before the projection, following the model architecture, while for the Aya model we scale logits by $0.0625$ (the default `logit_scale` defined in the model configuration). Following the residual stream view of the Transformer model [@elhage-etal-2021-mathematical], the resulting logits provide a view into the model predictive confidence at that specific depth of processing. Then, we compute the KL divergence between every layer distribution and the final distribution $P_N$, e.g. $\text{KL}(P_{N-1}\|P_N)$, to highlight trends in the shift in predictive probability produced by the application of remaining layers (**LL KL-Div**). Finally, we adapt the approach of @prediction-depth and use the number of the first layer for which the final prediction corresponds to the top logit as a metric of model confidence, $l \;\text{s.t.}\;\arg \max P_l = t^{*}$ and $\arg \max P_i \neq t^{*} \;\forall i<l$ (**LL Pred. Depth**).

[Context mixing]{.paragraph} Several works studied the mixing of contextual information across language model layers to attribute model predictions to specific input properties [@ferrando-etal-2022-measuring; @mohebbi-etal-2023-quantifying; @ferrando-etal-2023-explaining]. In this work we employ simple estimates of context relevance using attention weights produced during the Transformer attention operation. More specifically, for every attention head at every layer of the decoder module, we extract a score for every token in the preceding context. We then use the entropy of the distribution of attention weights^[For encoder-decoder model, self-attention and cross-attention weights are concatenated and renormalized.] over previous context as a simple measure of information locality during inference [@ferrando-etal-2022-measuring; @mohebbi-etal-2023-quantifying]. Following @fomicheva-etal-2020-unsupervised, we experiment with using the mean and the maximum entropy across all attention heads of all layers as separate metrics (**Attn. Entropy~avg/max~**). Finally, we evaluate the Between Layer OOD method by @jelenic-etal-2024-blood, employing gradients to estimate layer transformation smoothness for OOD detection (**BLOOD**).

[Supervised baselines]{.paragraph} We also test the state-of-the-art supervised WQE model [xcomet]{.smallcaps} [@guerreiro-etal-2024-xcomet]. [xcomet]{.smallcaps} is a suite of MT evaluation metrics introduced by @guerreiro-etal-2024-xcomet extending the popular [comet]{.smallcaps} metric [@rei-etal-2020-comet] to combine combines sentence-level and word-level error span prediction for improved explainability of results. [xcomet]{.smallcaps} metrics come in a 3.5B (XL) and 10.7B (XXL) size and support both reference-based and reference-less usage, hence enabling usage for quality estimation purposes. Concretely, [xcomet]{.smallcaps} models are Transformer encoders fine-tuned from pre-trained XLMR encoders [@goyal-etal-2021-larger] using a mix of sentence-level Direct Assessment scores and word-level MQM error spans. In this work, we focus on their word-level error span prediction capabilities in a quality estimation setup, where the model classifies every input token according to MQM severity levels {[ok]{.smallcaps}, [minor]{.smallcaps}, [major]{.smallcaps}, [critical]{.smallcaps}} with a learned linear layer.[^xcomet-library] Contrary to the continuous metrics from the previous section, binary labels from [xcomet]{.smallcaps} cannot be easily calibrated to match subjective annotation propensity. Hence, we propose to adapt the [xcomet]{.smallcaps} metric to use the sum of probability for all error types as a token-level continuous confidence metric, $s(t^{*}) = p(\text{minor}) + p(\text{major}) + p(\text{critical})$, which we dub **XCOMET~conf~**.

[^xcomet-library]: The default [xcomet]{.smallcaps} metric was used with the `unbabel-comet` library (`v2.2.6`).

[Human Editors]{.paragraph} For QE4PE, we report the min/mean/max agreement between each annotator's edited spans and those of the other five editors as a less subjective "human-level" quality measure.

## Experiments {#sec-unsup-wqe-experiments}

**How Accurate are Unsupervised WQE Metrics?** @tbl-datasets-perf reports the average metrics performance across all translation directions across tested datasets.^[Full breakdown available in the Appendix.] We report Average Precision (AP) as a general measure of metric quality across the full score range, and we estimate calibrated metric performance as the best F1 score (F1*) across all thresholds for binarizing continuous metric scores into pos./neg. labels matching human annotation.^[Random baseline AP values match the proportion of tokens marked as errors, which can vary greatly.] Our results show that, despite high variability in error span prevalence across different models, languages and annotators, metric rankings remain generally consistent, suggesting the presence of **robust relations between various signals sourced from models' inner workings and translation errors**.

{{< include ../tables/chap-10-unsup-wqe/_datasets-perf.qmd >}}

Among unsupervised metrics, we find those based on the output distribution to be most effective at identifying error spans, in line with previous segment-level QE results [@fomicheva-etal-2020-unsupervised]. Notably, the Surprisal MCD~var~ shows strong performances in line with the default XCOMET models. For the multi-label QE4PE dataset, we find that the best supervised metrics score on par with the average human annotator consensus (Hum. Editors~avg~), while unsupervised metrics generally obtain lower performances.

[Confidence Weighting Enables XCOMET Calibration]{.paragraph} From @tbl-datasets-perf results, default XCOMET metrics underperform compared to the best unsupervised techniques, a surprising result given their ad-hoc tuning. On the contrary, our XCOMET~conf~ method consistently reaches better results across all tested sets. @fig-pr-curves-main shows the precision-recall tradeoff for these metrics on the EN$\rightarrow$IT subset of the DivEMT dataset.^[Results for all datasets in the Appendix.] In their default form commonly used for evaluation via the `unbabel-comet` library, XCOMET metrics consistently outperform Surprisal MCD~var~ in terms of precision (51-60%, compared to 34% optimal precision for MCD~var~), but identify only 32-26% of tokens annotated as errors, resulting in lower AP.

:::{#fig-pr-curves-main layout="[-15,50,20,-15]" align="center"fig-pos="t"}

![](../figures/chap-10-unsup-wqe/divemt_metrics_pr_curves_ita.pdf)

![](../figures/chap-10-unsup-wqe/divemt_metrics_pr_curves_ita_legend.pdf)

Precision-Recall tradeoff for binary and confidence-weighted XCOMET variants and the Surprisal MCD~var~ metric for DivEMT EN$\rightarrow$IT.
:::

The low recall of these metrics might be problematic in WQE applications where omitting an error might result in oversights from human post-editors trusting the comprehensiveness of WQE predictions. On the contrary, the confidence-weighted XCOMET~conf~ show strong performances across the whole recall range, resulting in consistent improvements in both F1* and AP @tbl-datasets-perf. Concretely, these results confirm that default XCOMET performance does not reflect the full capacity of the metric, and **operating with granular confidence scores can be beneficial when calibration is possible**.

[Metrics Performance for Multiple Annotations]{.paragraph} While our evaluation so far employed human error span annotations as binary labels, we set out to assess how more granular labeling schemes impact metrics' performance. Given $L$ sets of binary labels (up to 6 per language for QE4PE), we assign a score $s \in \{1,\dots,L\}$ to every MT token using the number of annotators that marked it as an error, resulting in edit counts reflecting human agreement rate.^[An example is available in the Appendix.]

@fig-annotators-perf presents the correlation of various metrics when the number of annotators available is increased, with median values and confidence bounds are obtained from edit counts across all combinations of $L$ label sets.^[$x$=1 corresponds to binary labels from previous sections.] The increasing trend for correlations across all reported metrics indicates that these methods reflect well the *aleatoric uncertainty* in error span labels, i.e. the disagreement between various annotators. In particular, the Surprisal MCD~var~ metric sees a steeper correlation increase than other well-performing metrics, surpassing default XCOMET supervised approaches for higher correlation bins. This suggests the epistemic uncertainty derived from noisy model predictions might be a promising way to anticipate the aleatoric uncertainty across human annotators for WQE. We observe that 95% confidence intervals for high-scoring metrics are largely overlapping when a single set of labels is used, indicating that **rankings of metric performance are subject to change depending on subjective choices of the annotator**. While this poses a problem when attempting a robust evaluation of WQE metrics, we remark that including multiple annotations largely mitigates this issue. As a result, we recommend to explicitly account for human label variation by including multiple error annotations in future WQE evaluations to ensure generalizable findings.

![Spearman correlation between WQE metric scores and human edit counts across multiple annotation sets for QE4PE EN$\rightarrow$IT (left) and EN$\rightarrow$NL (right).](../figures/chap-10-unsup-wqe/qe4pe_correlations.pdf){#fig-annotators-perf fig-pos="t" width=70%}

## Conclusion {#sec-unsup-wqe-conclusion}

We conducted a comprehensive evaluation of supervised and unsupervised WQE metrics across multiple languages and annotation sets. Our results show that, while unsupervised metrics generally lag behind state-of-the-art supervised systems, some uncertainty quantification methods based on the predictive distribution show promising correlation with human label variation. Moreover, we find that popular supervised WQE metrics have generally low levels of recall, and can benefit from confidence weighting to when calibration is possible. Finally, individual annotator preferences are key confounders in WQE evaluations and can be mitigated by making use of multiple annotation sets.

We offer the following practical recommendations for evaluating WQE systems:

- Use agreement between multiple human annotations to control the effect of subjective preferences and rank WQE metrics robustly.
- Employ an in-distribution calibration set of error spans before testing to ensure fair metric comparisons, and favor evaluations accounting for precision-recall tradeoffs to ensure their usability across various confidence levels.
- Previous work showed the effectiveness of visualization reflecting prediction confidence [@vasconcelos-etal-2025-generation], such as highlights for various error severity levels [@sarti-etal-2025-qe4pe]. Consider using continuous WQE metrics in real-world applications such as WQE-augmented post-editing to convey fine-grained confidence variations.

## Limitations {#sec-unsup-wqe-limitations}

Our findings are accompanied by a number of limitations. Firstly, our choice of tested datasets was limited by the availability of annotated outputs generated by open-source MT models. While several other datasets matching these criteria exist [@fomicheva-etal-2022-mlqe; @yang-etal-2023-rethinking; @dale-etal-2023-halomi], we restricted our assessment to a sufficient subset to ensure diversity across languages and tested models to support our findings. To facilitate comparison with other datasets, our evaluation for WMT24 treats available error spans as binary labels and does not directly account for error severity in human-annotated spans. Our choice of unsupervised metrics was largely driven by previous work on uncertainty quantification in MT, and ease of implementation for popular methods in mechanistic interpretability literature [@ferrando-etal-2024-primer]. However, our choices in the latter category were limited since most methods are nowadays developed and tested specifically for decoder-only transformer models. Finally, despite their strong performance, we found unsupervised methods based on MCD to require substantial computational resources, and as such we could not evaluate them on Aya23 35B. While our main focus was to establish baseline performances across various popular methods, future work should leverage the latest insights from more advanced techniques requiring, for example, the tuning of vocabulary projections [@belrose-etal-2023-eliciting; @yom-din-etal-2024-jump] or the identification of "confidence neurons" modulating predictive entropy [@confidence-neurons].