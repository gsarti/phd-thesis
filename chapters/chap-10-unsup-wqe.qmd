# Unsupervised MT Error Detection and Human Disagreement {#sec-chap-10-unsup-wqe}

::: {.callout-note icon="false"}

## Chapter Summary

This final experimental chapter presents our comprehensive evaluation of unsupervised word-level quality estimation methods exploiting interpretability and uncertainty quantification methods to identify translation errors in model outputs. In our evaluation spanning 14 metrics across 12 translation directions, we also quantify the impact of human label variation on metric performance, using multiple edit sets from the [DivEMT]{.smallcaps} and QE4PE studies of the previous chapters. Our results highlight the untapped potential of unsupervised metrics, the shortcomings of supervised methods when faced with label uncertainty, and the brittleness of single-annotator evaluation practices.

\vspace{7pt}\noindent

This chapter is adapted from the paper *Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement* [@sarti-etal-2025-unsupervised].
:::

> *So, you see, translators do not so much deliver a message as the rewrite the original. And herein lies the difficulty---rewriting is still writing, and writing always reflects the authors ideology and biases.*
>
> *-- Rebecca F. Kuang, Babel (2022)*

## Introduction {#sec-unsup-wqe-intro}

Word-level error spans are widely used in machine translation evaluation to obtain robust and fine-grained estimates of translation quality [@burchardt-2013-multidimensional; @freitag-etal-2021-experts; @freitag-etal-2021-results; @kocmi-etal-2024-error].
Due to the cost of manual annotation, word-level quality estimation (WQE) was proposed for assisting in annotating error spans over MT outputs [@zouhar-etal-2025-ai].
Modern WQE approaches generally rely on costly inference with large language models or ad-hoc training with large amounts of human-annotated texts [@fernandes-etal-2023-devil; @kocmi-federmann-2023-large; @guerreiro-etal-2024-xcomet], making them impractical for less resourced settings [@zouhar-etal-2024-fine].

To improve the efficiency of MT quality assessment, several works explored the use of signals derived from the internals of neural MT systems [@fomicheva-etal-2020-unsupervised; @fomicheva-etal-2021-eval4nlp; @leiter-etal-2024-towards], for identifying problems in MT outputs, such as hallucinations [@guerreiro-etal-2023-optimal; @guerreiro-etal-2023-looking; @dale-etal-2023-detecting; @dale-etal-2023-halomi; @himmi-etal-2024-enhanced].
However, previous works have focused on sentence-level metrics for overall translation quality, and do not evaluate performance on multiple label sets due to high annotation costs [@fomicheva-etal-2022-mlqe; @zerva-etal-2024-findings].

![Example of German$\rightarrow$English translation with two sets of human word-level error span annotations and two examples of continuous and binary WQE metrics.](../figures/chap-10-unsup-wqe/intro_fig.pdf){#fig-intro-fig fig-pos="t" width=65%}

In this chapter, we conduct a more comprehensive evaluation spanning 10 unsupervised metrics derived from models' inner representations and predictive distributions to identify translation errors at the word level. We test three open-source multilingual MT models and LLMs of varying sizes across 12 translation directions, including typologically diverse languages and challenging textual domains.
Importantly, we focus on texts with *multiple* human annotations to measure the impact of individual annotator preferences on metric performance, setting a "human-level" baseline for the WQE task.

We address the following research questions:

- How accurate are unsupervised WQE metrics in detecting MT errors compared to trained metrics and human annotators?

- Are popular supervised WQE metrics well-calibrated?

- Are the relative performances of WQE metrics affected by the variability in human error annotations?

We conclude with recommendations for improving the evaluation and usage of future WQE systems.

## Related Work {#sec-unsup-wqe-related-work}

[Actionable Insights from Interpretability]{.paragraph} Advances in interpretability research have elucidated multiple mechanisms underlying decision-making, knowledge representation, and biases in LMs [@ferrando-etal-2024-primer]. However, a better understanding of model's inner workings often did not translate to tangible gains in model design and other practical applications, which remain rarely explored [@mosbach-etal-2024-insights]. Some examples in this direction include using targeted machine unlearning methods for safety-critical scenarios [@barez-etal-2025-open], or the use of attribution for trustworthy context citations in LM generations [@cohenwang-etal-2024-contextcite; @sarti-etal-2024-quantifying; @qi-sarti-etal-2024-model]. In this study, unsupervised metrics extracted from an MT model during generation are employed to detect errors in models' generated outputs, following the unsupervised QE paradigm introduced in @sec-chap2-qe. This can be seen as a variant of out-of-distribution detection in signal processing research [@ood-detection].

[Uncertainty Estimation for Language Models]{.paragraph} The estimation of uncertainty in language models has garnered increasing attention [@baan-etal-2023-uncertainty], particularly in the context of generation tasks for which the set of plausible responses is large [@giulianelli-etal-2023-comes]. Predictive uncertainty is typically decomposed into its *aleatoric* and *epistemic* components, representing respectively the irreducible variability in the modeled phenomena, and the improvable confidence in model predictions [@aleatoric-epistemic]. Popular methods for uncertainty estimation involve the calibration of predictive probabilities to reflect aleatoric uncertainty [@jiang-etal-2020-know; @ulmer-etal-2022-exploring; @zhao2023calibrating; @chen-etal-2023-close], and conformal sets prediction [@zerva-martins-2024-conformalizing; @ravfogel-etal-2023-conformal]. In this study, we exploit uncertainty signals from the predictive distribution of MT models and its internal processing for efficiently predicting the resulting generation quality at a fine-grained, token-level scale.

[Human Label Variation]{.paragraph} Human label variation is a type of uncertainty that arises from the inherent variability in human judgments [@plank-etal-2014-linguistically; @plank-2022-problem], which can be hard to disentangle from actual annotation mistakes [@snow-etal-2008-cheap; @weber-genzel-etal-2024-varierr]. The usage of multiple references was recently recommended to ensure a sound evaluation of generative LMs reflecting human-plausible levels of variability [@giulianelli-etal-2023-comes], contrary to common practices employing a single set of "gold" labels. In our analysis on QE4PE data containing multiple edits, we adopt a perspectivist approach^[<https://pdai.info/>] to ensure a robust assessment of WQE metrics by accounting for annotators' disagreement [@uma2021learning].

## Models and Datasets {#sec-unsup-wqe-data}

We use datasets containing error annotations or post-edits on the outputs of open-source models to extract unsupervised WQE metrics on real model outputs, avoiding possible confounders. We select the following datasets, summarized in @tbl-datasets-summary:

[DivEMT]{.paragraph} We reuse the DivEMT dataset, introduced in @sec-chap-8-divemt, including out-of-English machine translations towards six typologically diverse target languages (EN$\rightarrow$AR,IT,NL,TR,UK,VI) produced by Google Translate and mBART-50 1-to-many for a subset of Wiki texts from the FLORES dataset [@goyal-etal-2022-flores], with edits made by professional translators. In this study, we evaluate unsupervised metrics on the mBART-50 1-to-many model, converting the human post-edits into token-level labels to perform a cross-lingual comparisons over a fixed set of examples.

[WMT24]{.paragraph} The WMT24 dataset is taken from the General Machine Translation Shared Task at WMT 2024 [@kocmi-etal-2024-findings]. It contains evaluation of several machine translation systems across English$\rightarrow${Czech, Hindi, Japanese, Chinese, Russian} (634 segments per language) and Czech$\rightarrow$Ukrainian (1954 segments). The human evaluation was done with the Error Span Annotation protocol (ESA, @kocmi-etal-2024-error), which has human annotators highlighting erroneous spans in the translation and marking them as either [minor]{.smallcaps} or [major]{.smallcaps} errors. This dataset covers the *news*, *social*, and *speech* (with automatic speech recognition) domains. We adopt the official prompting setup from the WMT24 campaign, using the Aya23 model alongside the provided prompt and three in-context translation examples per language to ensure uniformity with previous results.^[<https://github.com/wmt-conference/wmt-collect-translations>] Aya23 is a large language model introduced by @aya23 to improve the multilingual capabilities of the original Aya model [@ustun-etal-2024-aya] on a selected set of 23 languages. The model was included in the WMT24 evaluation of @kocmi-etal-2024-findings, resulting in the best translation performances among tested open-source models. The model is a decoder-only transformer model with 40 layers, a model dimension of 8196 and 64 attention heads per layer. Using WMT24 allows us to extend our evaluation to a state-of-the-art LLM, given the popularity of such systems in MT [@kocmi-etal-2023-findings].

[QE4PE]{.paragraph} The QE4PE dataset, introduced in @sec-chap-9-qe4pe, was created to measure the effect of word-level error highlights when included in real-world human post-editing workflows. The QE4PE data provides granular behavioral metrics to evaluate the speed and quality of post-editing of 12 annotators for En$\rightarrow$It and En$\rightarrow$Nl across two challenging textual domains (social posts and biomedical abstracts) and four error span highlighting modalities, including the unsupervised Surprisal MCD~var~ method and the supervised [xcomet-xxl]{.smallcaps} we also test in this study. Provided that the presence of error span highlights was found to influence the editing choices of human editors, we limit our evaluation to the six human annotators per language that post-edited sentences without any highlights (3 for the *Oracle Post-edit* task to produce initial human-based highlights, and 3 for the *No Highlight* modality in the main task). This prevents us from biasing our evaluation of WQE metrics in favor of the metrics that influenced editing choices. As for [DivEMT]{.smallcaps}, we use the post-edits over translations---in this case, those of the NLLB 3.3B model [@nllb-2024-scaling]---to produce token-level error spans, enabling an evaluation of WQE metrics across multiple annotation sets.

{{< include ../tables/chap-10-unsup-wqe/_datasets-summary.qmd >}}

## Evaluated Metrics {#sec-unsup-wqe-metrics}

The following metrics were evaluated using the Inseq library introduced in @sec-chap-3-inseq [@sarti-etal-2023-inseq-fixed; @sarti-etal-2024-democratizing].

[Predictive Distribution Metrics]{.paragraph} We use the **Surprisal** of the predicted token $t^{*}$, as negative log-probability $-\log p(t^{*}_i|t_{<i})$, and the **Entropy** $H$ of the output distribution $P_N$ over vocabulary $\mathcal{V}$, $-\sum_{i=1}^{|\mathcal{V}|} p(t_i|t_{<i}) \log_2 p(t_i|t_{<i})$, as simple metrics to quantify pointwise and full prediction uncertainty [@fomicheva-etal-2020-unsupervised]. For surprisal, we also compute its expectation (**MCD$_\text{avg}$**) and variance (**MCD$_\text{var}$**) with $n=10$ steps of Monte Carlo Dropout [MCD, @gal-ghahramani-2016-dropout] to obtain a robust estimate and a measure of epistemic uncertainty in predictions, respectively. Intuitively, epistemic uncertainty reflects models' lack of knowledge rather than data ambiguity.^[MCD is tested only on encoder-decoder models since Aya layers do not include dropout. The MCD$_\text{var}$ setting corresponds to the [Unsupervised]{color="brand-color.unsupervised"} setting from @sec-chap-9-qe4pe.]  We employ the mean of the negative log probabilities as a robust estimate of surprisal:

$$\text{Surprisal MCD}_{\text{avg}} = \hat y_{\text{MCD}} = \frac{1}{T} \sum_{t=1}^{T} - \log p(x | \Theta_t)$$

Moreover, we estimate predictive uncertainty by calculating the variance of predictive probabilities under the same setup:

$$\text{Surprisal MCD}_{\text{var}} = \frac{1}{T} \sum_{t=1}^{T} \big(- \log p(x | \Theta_t) - \hat y_{\text{MCD}} \big)$$

[Vocabulary Projections]{.paragraph} We use the Logit Lens method [LL, @logitlens], introduced in @sec-chap2-nlm-lm, to extract probability distributions $P_0, \dots, P_{N-1}$ over $V$ from intermediate activations at every layer $l_0, \dots, l_{N-1}$ of the decoder.
We use the surprisal for the final prediction at every layer (**LL-Surprisal**) to assess the presence of layers with high sensitivity to incorrect predictions. For the NLLB and mBART-50 models we also apply a final layer normalization before the projection, following the model architecture, while for the Aya model we scale logits by $0.0625$ (the default `logit_scale` defined in the model configuration). Following the residual stream view of the transformer model [@elhage-etal-2021-mathematical], the resulting logits provide a view into the model predictive confidence at that specific depth of processing. Then, we compute the KL divergence between every layer distribution and the final distribution $P_N$, e.g. $\text{KL}(P_{N-1}\|P_N)$, to highlight trends in the shift in predictive probability produced by the application of remaining layers (**LL KL-Div**). Finally, we adapt the approach of @prediction-depth and use the number of the first layer for which the final prediction corresponds to the top logit as a metric of model confidence, $l \;\text{s.t.}\;\arg \max P_l = t^{*}$ and $\arg \max P_i \neq t^{*} \;\forall i<l$ (**LL Pred. Depth**).

[Context mixing]{.paragraph} We employ simple estimates of context relevance using attention weights produced during the transformer attention operation. More specifically, for every attention head at every layer of the decoder module, we extract a score for every token in the preceding context. We then use the entropy of the distribution of attention weights^[For encoder-decoder model, self-attention and cross-attention weights are concatenated and renormalized.] over previous context as a simple measure of information locality during inference [@ferrando-etal-2022-measuring; @mohebbi-etal-2023-quantifying]. Following @fomicheva-etal-2020-unsupervised, we experiment with using the mean and the maximum entropy across all attention heads of all layers as separate metrics (**Attn. Entropy~avg/max~**). Finally, we evaluate the Between Layer OOD method by @jelenic-etal-2024-blood, employing gradients to estimate layer transformation smoothness for OOD detection (**BLOOD**).

[Supervised baselines]{.paragraph} We also test the state-of-the-art supervised WQE model [xcomet]{.smallcaps} [@guerreiro-etal-2024-xcomet], introduced in @sec-chap2-qe. In this chapter, we focus on their word-level error span prediction capabilities in a quality estimation setup, where the model classifies every input token according to MQM severity levels {[ok]{.smallcaps}, [minor]{.smallcaps}, [major]{.smallcaps}, [critical]{.smallcaps}} with a learned linear layer.[^xcomet-library] Contrary to the continuous metrics from the previous section, binary labels from [xcomet]{.smallcaps} cannot be easily calibrated to match subjective annotation propensity. Hence, we propose to adapt the [xcomet]{.smallcaps} metric to use the sum of probability for all error types as a token-level continuous confidence metric, $s(t^{*}) = p(\text{minor}) + p(\text{major}) + p(\text{critical})$, which we dub **XCOMET~conf~**.

[^xcomet-library]: The default [xcomet]{.smallcaps} metric was used with the `unbabel-comet` library (`v2.2.6`).

[Human Editors]{.paragraph} For QE4PE, we report the min/mean/max agreement between each annotator's edited spans and those of the other five editors as a less subjective "human-level" quality measure.

## Experiments {#sec-unsup-wqe-experiments}

### Setup

[Token-level Evaluation]{.paragraph} Error spans used as labels in our evaluation are defined at the character level, while metric scores depend on the tokenization employed by either the MT model (for unsupervised metrics) or [xcomet]{.smallcaps} (for supervised metrics). To allow for comparison, we label tokens as being part of an error span if at least one character contained in it was marked as an error or edited by an annotator. @tbl-example-qe4pe-ita and @tbl-example-wmt24-encs provide examples of various segmentations for the same MT output.

{{< include ../tables/chap-10-unsup-wqe/_example-qe4pe-ita.qmd >}}

{{< include ../tables/chap-10-unsup-wqe/_example-wmt24-encs.qmd >}}

[Constraining generation]{.paragraph} Evaluating metrics at the word level can be challenging due to the need for perfect uniformity between model generations and annotated spans. For this reason we extract unsupervised metrics during generation while force-decoding the annotated outputs from the MT model to ensure perfect adherence with annotated error spans. In general, such an approach could introduce a problematic confounder in the evaluation, since observed results could be the product of constraining a model towards an unnatural generation, rather than reflecting underlying phenomena. However, in this study, we carefully ensure that the generation setup matches exactly the one of previous works where the annotated translations were produced, using the same MT model and the same inputs.^[Generation parameters are not relevant in this setting, provided that they only alter the selection of the following output token, which we do via force-decoding.] Hence, the constraining process is a simple insurance of conformity in light of potential discrepancies introduced by different decoding strategies, and does not affect the soundness of our method.

### Results

[How Accurate are Unsupervised WQE Metrics?]{.paragraph} @tbl-datasets-perf reports the average metrics performance across all translation directions across tested datasets.^[Full breakdown available in [@tbl-qe4pe-ita-all-results;@tbl-qe4pe-nld-all-results;@tbl-divemt-all-results;@tbl-wmt24esa-all-results].] We report Average Precision (AP) as a general measure of metric quality across the full score range, and we estimate calibrated metric performance as the best F1 score (F1*) across all thresholds for binarizing continuous metric scores into pos./neg. labels matching human annotation.^[Random baseline AP values match the proportion of tokens marked as errors, which can vary greatly.] Our results show that, despite high variability in error span prevalence across different models, languages and annotators, metric rankings remain generally consistent, suggesting the presence of **robust relations between various signals sourced from models' inner workings and translation errors**.

{{< include ../tables/chap-10-unsup-wqe/_datasets-perf.qmd >}}

Among unsupervised metrics, we find those based on the output distribution to be most effective at identifying error spans, in line with previous segment-level QE results [@fomicheva-etal-2020-unsupervised]. Notably, the Surprisal MCD~var~ shows strong performances in line with the default XCOMET models. For the multi-label QE4PE dataset, we find that the best supervised metrics score on par with the average human annotator consensus (Hum. Editors~avg~), while unsupervised metrics generally obtain lower performances.

[Confidence Weighting Enables XCOMET Calibration]{.paragraph} From @tbl-datasets-perf results, default XCOMET metrics underperform compared to the best unsupervised techniques, a surprising result given their ad-hoc tuning. On the contrary, our XCOMET~conf~ method consistently reaches better results across all tested sets. @fig-pr-curves-main shows the precision-recall tradeoff for these metrics on the EN$\rightarrow$IT subset of the DivEMT dataset.^[Results for all datasets in [@fig-qe4pe-ita-pr-curves;@fig-qe4pe-nld-pr-curves;@fig-divemt-pr-curves;@fig-wmt24esa-pr-curves].] In their default form commonly used for evaluation via the `unbabel-comet` library, XCOMET metrics consistently outperform Surprisal MCD~var~ in terms of precision (51-60%, compared to 34% optimal precision for MCD~var~), but identify only 32-26% of tokens annotated as errors, resulting in lower AP.

:::{#fig-pr-curves-main layout="[-15,50,20,-15]" align="center"fig-pos="t"}

![](../figures/chap-10-unsup-wqe/divemt_metrics_pr_curves_ita.pdf)

![](../figures/chap-10-unsup-wqe/divemt_metrics_pr_curves_ita_legend.pdf)

Precision-Recall tradeoff for binary and confidence-weighted XCOMET variants and the Surprisal MCD~var~ metric for DivEMT EN$\rightarrow$IT.
:::

The low recall of these metrics might be problematic in WQE applications where omitting an error might result in oversights from human post-editors trusting the comprehensiveness of WQE predictions. On the contrary, the confidence-weighted XCOMET~conf~ show strong performances across the whole recall range, resulting in consistent improvements in both F1* and AP @tbl-datasets-perf. Concretely, these results confirm that default XCOMET performance does not reflect the full capacity of the metric, and **operating with granular confidence scores can be beneficial when calibration is possible**.

[Metrics Performance for Multiple Annotations]{.paragraph} While our evaluation so far employed human error span annotations as binary labels, we set out to assess how more granular labeling schemes impact metrics' performance. Given $L$ sets of binary labels (up to 6 per language for QE4PE), we assign a score $s \in \{1,\dots,L\}$ to every MT token using the number of annotators that marked it as an error, resulting in edit counts reflecting human agreement rate, as shown in @tbl-example-qe4pe-ita.

@fig-annotators-perf presents the correlation of various metrics when the number of annotators available is increased, with median values and confidence bounds are obtained from edit counts across all combinations of $L$ label sets.^[$x$=1 corresponds to binary labels from previous sections.] The increasing trend for correlations across all reported metrics indicates that these methods reflect well the *aleatoric uncertainty* in error span labels, i.e. the disagreement between various annotators. In particular, the Surprisal MCD~var~ metric sees a steeper correlation increase than other well-performing metrics, surpassing default XCOMET supervised approaches for higher correlation bins. This suggests the epistemic uncertainty derived from noisy model predictions might be a promising way to anticipate the aleatoric uncertainty across human annotators for WQE. We observe that 95% confidence intervals for high-scoring metrics are largely overlapping when a single set of labels is used, indicating that **rankings of metric performance are subject to change depending on subjective choices of the annotator**. While this poses a problem when attempting a robust evaluation of WQE metrics, we remark that including multiple annotations largely mitigates this issue. As a result, we recommend to explicitly account for human label variation by including multiple error annotations in future WQE evaluations to ensure generalizable findings.

![Spearman correlation between WQE metric scores and human edit counts across multiple annotation sets for QE4PE EN$\rightarrow$IT (left) and EN$\rightarrow$NL (right).](../figures/chap-10-unsup-wqe/qe4pe_correlations.pdf){#fig-annotators-perf fig-pos="t" width=70%}

## Limitations {#sec-unsup-wqe-limitations}

Our findings are accompanied by a number of limitations. Firstly, our choice of tested datasets was limited by the availability of annotated outputs generated by open-source MT models. While several other datasets matching these criteria exist [@fomicheva-etal-2022-mlqe; @yang-etal-2023-rethinking; @dale-etal-2023-halomi], we restricted our assessment to a sufficient subset to ensure diversity across languages and tested models to support our findings. To facilitate comparison with other datasets, our evaluation for WMT24 treats available error spans as binary labels and does not directly account for error severity in human-annotated spans. Our choice of unsupervised metrics was largely driven by previous work on uncertainty quantification in MT, and ease of implementation for popular methods in mechanistic interpretability literature [@ferrando-etal-2024-primer]. However, our choices in the latter category were limited since most methods are nowadays developed and tested specifically for decoder-only transformer models. Finally, despite their strong performance, we found unsupervised methods based on MCD to require substantial computational resources, and as such we could not evaluate them on Aya23 35B. While our main focus was to establish baseline performances across various popular methods, future work should leverage the latest insights from more advanced techniques requiring, for example, the tuning of vocabulary projections [@belrose-etal-2023-eliciting; @yom-din-etal-2024-jump] or the identification of "confidence neurons" modulating predictive entropy [@confidence-neurons].

## Conclusion {#sec-unsup-wqe-conclusion}

We conducted a comprehensive evaluation of supervised and unsupervised WQE metrics across multiple languages and annotation sets. Our results show that, while unsupervised metrics generally lag behind state-of-the-art supervised systems, some uncertainty quantification methods based on the predictive distribution show promising correlation with human label variation. Moreover, we find that popular supervised WQE metrics have generally low levels of recall, and can benefit from confidence weighting to when calibration is possible. Finally, individual annotator preferences are key confounders in WQE evaluations and can be mitigated by making use of multiple annotation sets.

We offer the following practical recommendations for evaluating WQE systems:

- Use agreement between multiple human annotations to control the effect of subjective preferences and rank WQE metrics robustly.

- Employ an in-distribution calibration set of error spans before testing to ensure fair metric comparisons, and favor evaluations accounting for precision-recall tradeoffs to ensure their usability across various confidence levels.

- Previous work showed the effectiveness of visualization reflecting prediction confidence [@vasconcelos-etal-2025-generation], such as highlights for various error severity levels [@sarti-etal-2025-qe4pe]. Consider using continuous WQE metrics in real-world applications such as WQE-augmented post-editing to convey fine-grained confidence variations.

This final assessment concludes our investigation on the potential of model processing signals for improving the downstream verification of machine translated contents, converting interpretability methods commonly used for model analysis into practical tools for improving decision-making in real-world human-AI interaction settings.