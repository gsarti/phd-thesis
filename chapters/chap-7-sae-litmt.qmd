# Steering Language Models for Personalized Machine Translation {#sec-chap-7-sae-litmt}

::: {.callout-note icon="false"}

## Chapter Summary

This chapter expands our evaluation of machine translation conditioning approaches by assessing the effectiveness of efficient inference-time interventions on model internals for personalizing large language models' outputs. Focusing on the challenging literary translation domain, we explore prompting strategies and inference-time interventions using sparse autoencoders to steer model generations toward personalized translator styles. We propose a contrastive framework exploiting interpretable latent concepts from SAEs to identify salient personalization properties, and show that its strong personalization accuracy, comparable and at times better than few-shot prompting, does not come at the cost of translation quality. Our analyses further reveal that successful SAE steering and multi-shot prompting impact similar model layers, suggesting similar mechanisms at play.

\vspace{7pt}\noindent

This chapter is adapted from the paper *Steering Large Language Models for Machine Translation Personalization* [@scalena-sarti-etal-2025-steering].

:::

> *I don't speak, I operate a machine called language. It creaks and groans, but is mine own.*
>
> *-- Frank Herbert, Dune Messiah (1969)*

## Introduction

When we read a translated book, we do not simply read the story in a new language; we also experience the translator's personal voice by means of their stylistic choices. Past efforts in the automatic translation of literary works have historically been constrained by the limited capabilities and flexibility of machine translation systems. The recent popularization of MT systems based on large language models has greatly improved their capacity in handling the long contexts typical of literary translations, but mimicking the creative and rich language that characterize the translators' own style remains an open issue. In this context, several works explored the usage of prompting and tuning-based strategies to ensure translations are stylistically appropriate [@michel-neubig-2018-extreme; @wang-etal-2021-towards]. However, their influence on model internal representations is rarely explored, so that their impact is less controllable and often unpredictable. Building upon the prompting techniques demonstrated in @sec-chap-6-ramp, this chapter tackles the more complex challenge of personalizing machine translation to match individual translator styles. While [Ramp]{.smallcaps} focused on explicit attributes like formality and gender, literary translation requires capturing the subtle, implicit stylistic preferences that characterize individual translators' voices.For this purpose, we compare prompting approaches with *steering* methods proposed in interpretability literature. These techniques can be used to surgically intervene on LLMs' intermediate representation to generate personalized translations when few examples are available, using the [Par3]{.smallcaps} dataset [@thai-etal-2022-exploring] with multiple human translations for novels translated into English from 7 typologically diverse languages.

We begin with preliminary assessments by verifying whether translators' styles are discernible by automatic systems, finding that trained classifiers can distinguish writing styles with high accuracy, while the task is notoriously challenging for human annotators [@youyou-etal-2015-computer; @flekova-etal-2016-analyzing]. We also find a simple prompting setting with in-context personalization examples to improve the style accuracy of LLM translation, suggesting personalized translation styles are reproducible. We connect the conditioning induced by prompting to the inner workings of the model, identifying activations with high discriminative capacity for style differences in intermediate model layers. We then propose a contrastive steering approach based on sparse autoencoders (SAEs, @huben-etal-2024-sparse) to condition model generations by upweighting sparse, interpretable latents at inference time. We validate the effectiveness of our method across three LLMs of various sizes on [Par3]{.smallcaps} novels, comparing our results with established prompting and steering methods.

![We compare [prompt-based approaches]{.promptingC} with [steering techniques]{.steeringC} intervening on model internals for personalizing MT outputs in literary machine translation, employing MT quality metrics and style classifiers to disentangle the effect of steering on outputs fluency and personalization adequacy.](../figures/chap-7-sae-litmt/teaser.pdf){#fig-chap7-teaser width=65% fig-pos="t"}

Our results show that contrastive SAE steering is a promising approach for MT personalization, leading to translations that are not only more in line with general human translation features but also more aligned with the desired personalized style compared to other methods. Importantly, these results are achieved with no translation quality degradation according to established MT quality metrics. We conclude by comparing the impact of our method on model representations with the outcome of multi-shot prompting, finding that probes trained on prompt-conditioned activations can predict the effectiveness of SAE steering with high precision. These results confirm that tested prompting and steering techniques converge to similar solutions for conditioning model behavior, enabling future investigations into the mechanistic impact of prompting through the study of learned SAE latents and other interpretable components.

## Related Work {#sec-chap7-sota}

[Machine Translation of Literary Texts]{.paragraph} The literary domain has historically been challenging for automatic MT systems due to their limited ability in handling rich linguistic and cultural contexts [@matusov-2019-challenges] and their propensity to produce overly literal outputs [@guerberof-toral-2022-creativity]. Automatic literary translation has a long history dating back to pre-neural MT approaches [@voigt-jurafsky-2012-towards; @toral-way-2015-translating; @toral-way-2018-what; @moorkens-etal-2018-translators] with two recent dedicated evaluation campaigns [@wang-etal-2023-findings; @wang-etal-2024-findings]. The advent of LLMs brought new opportunities in the processing of longer context for document-level translation [@wang-etal-2023-document-level; @briakou-etal-2024-translating; @wu-etal-2024-perhaps], but critical errors requiring human translator's intervention nonetheless persist [@karpinska-iyyer-2023-large]. Here, we use the [Par3]{.smallcaps} dataset [@thai-etal-2022-exploring] containing multiple human translations of novels to evaluate MT personalization in the literary domain.

[Personalization for Machine Translation]{.paragraph} Advances in MT quality recently led to a growing interest in personalization approaches to ensure a consistent format and appropriate stylistic choices in model generations [@rabinovich-etal-2017-personalized; @lin-etal-2021-towards]. Previous approaches for controlling attributes such as formality [@sennrich-etal-2016-controlling; @niu-etal-2017-study; @nadejde-etal-2022-cocoa] or gender [@vanmassenhove-etal-2018-getting; @saunders-byrne-2020-reducing] typically required tuning existing models on pre-defined properties of interest, with few works attempting a real data-driven adaptation from unlabeled demonstrations [@michel-neubig-2018-extreme; @wang-etal-2021-towards; @zhang-etal-2022-building]. More recently, several studies employed prompting [@garcia-firat-2022-using; @sarti-etal-2023-ramp] or preference optimization from post-editing behavior [@lee-etal-2023-pepe; @berger-etal-2024-post] to render MT personalization more effective and data-efficient. We complement established prompt methodologies with steering approaches to personalize MT outputs using few user-provided examples.

## Preliminaries {#sec-chap7-preliminary}

{{< include ../tables/chap-7-sae-litmt/_data-examples.qmd >}}

Before testing the effectiveness of personalization strategies, we validate some key assumptions: **i)** Whether the personalized translation style is *discernible*, i.e., if it is possible to tell apart human- and machine-generated translations; **ii)** Whether different translation styles are automatically *reproducible*, i.e., if LLMs can mimic a specific translator's style when provided with some examples; and **iii)** Whether style distinctions are reflected in the model's internal representations, to motivate the interest in steering approaches for personalization.

We use the [Par3]{.smallcaps} dataset by @thai-etal-2022-exploring, which contains multiple non-English novels, as a benchmark to evaluate personalization. Novels are segmented into paragraphs with translations into English by two professional literary translators. To ensure a diverse and representative evaluation, we select novels spanning a variety of linguistic families and cultural backgrounds. Our dataset includes Romance languages such as Italian (*Pinocchio*) and French (*Around the World in Eighty Days*), as well as Germanic languages like Dutch (*The Diary of a Young Girl*) and German (*Beware of Pity*). To evaluate our setup on non-Latin scripts and distinct linguistic structures, we also include Russian (*Crime and Punishment*), Japanese (*No Longer Human*), and Chinese (*Dream of the Red Chamber*). @tbl-chap7-novels-details summarizes the number of paragraphs employed in the evaluation of each language.

{{< include ../tables/chap-7-sae-litmt/_novels-details.qmd >}}

Examples for a subset of languages are shown in @tbl-chap7-data-examples. We name the two available human translations [H$_1$]{color="brand-color.darkgreen"} and [H$_2$]{color="brand-color.lightbluedim"}, and compare them with [MT]{color="brand-color.darkorange"} outputs produced by LLMs, which we denote as $\text{MT}_{\text{model}}$. We use three LLMs, namely Llama 3.1 8B Instruct [@grattafiori2024llama3herdmodels] and Gemma 2 [@gemmateam2024gemma2improvingopen] in its 2B and 9B instruction-tuned variants. Our model selection is motivated by our steering requirements, discussed in @sec-chap7-exp.

### Are Personalized Translations Discernible? {#sec-chap7-discern}

Following prior work on personalization [@wang-etal-2024-m4gt; @liu-etal-2023-coco], we train a series of classifiers based on multilingual XLM transformer encoders [@conneau-etal-2020-unsupervised] to distinguish between [H$_1$]{color="brand-color.darkgreen"}, [H$_2$]{color="brand-color.lightbluedim"}, and [MT]{color="brand-color.darkorange"} translations. If those systems can reliably separate these three classes, it suggests the presence of reasonably distinct stylistic signals differentiating them. In particular, the ability to distinguish between [H$_1$]{color="brand-color.darkgreen"} and [H$_2$]{color="brand-color.lightbluedim"} would denote not only the possibility to discern a human-like style from human-made and automatic translations, but also a *personalized style* from different human translators.

We train a classifier for each language and each model in our evaluation suite. All classifiers are fine-tuned from the `xlm-roberta-large` model^[[`FacebookAI/xlm-roberta-large`](https://huggingface.co/facebookai/xlm-roberta-large)], using a linear classification head. Training is conducted for 6 epochs with a learning rate of 2e-5 and a batch size of 32, selecting the best model checkpoint based on validation accuracy. Training data only includes generations from models and the translator without any source text. It is also perfectly balanced, as each paragraph provides one instance for all three labels: [H$_1$]{color="brand-color.darkgreen"}, [H$_2$]{color="brand-color.lightbluedim"}, and [MT]{color="brand-color.darkorange"}. The total size of the training set varies depending on the number of paragraphs in the chosen novel. On average, we obtain approximately 830 instances, resulting in a total of around 2,490 labeled examples for training (see @tbl-chap7-novels-details). Validation and test sets are strictly held out and never seen during training. Additionally, they do not include the small 20-example subsets used for prompting or steering. Results in @tbl-chap7-classifiers-perf indicate that translation styles are discernible with high accuracy. On average across all models and languages, the classifiers reach an accuracy between 77% (Japanese) and 99% (Chinese), with an average of 86%. These results suggest that personalization information is abundant in the literary setting and can plausibly be exploited for modeling. These findings corroborate previous results showing the high learnability of this task by machines while remaining intrinsically difficult for human annotators [@youyou-etal-2015-computer; @flekova-etal-2016-analyzing; @wang-etal-2024-semeval-2024].^[Two human annotators were asked to label 100 translated paragraphs from the novel Pinocchio (IT$\rightarrow$EN) as either human or MT, resulting in an accuracy of $\sim60\%$.]

{{< include ../tables/chap-7-sae-litmt/_classifiers-perf.qmd >}}

### Can LLMs Reproduce Human Translation Styles? {#sec-chap7-produce}

To confirm whether MT personalization can be achievable, we test LLMs ability to mimic the stylistic choices of a particular translator in a multi-shot (MS) prompting setup. For each translator available across tested novels, we provide the model with 20 in-context examples selected from the original pool of translated paragraphs by that translator, asking it to generate a consistent translation. We compare MS results with the default zero-shot (ZS) prompting without any example from the translator to quantify the effect of in-context examples. @tbl-chap7-zs-ms-perf presents results for *personalization accuracy*, automatically evaluated using our high-scoring classifiers from the previous section; and *translation quality*, estimated via the widely used [comet]{.smallcaps} MT metric [@rei-etal-2020-comet]. The proportion of outputs categorized as matching the translator's style is increased two- to four-fold following MS prompting, suggesting that LLMs can employ implicit clues in small sets of user examples to produce personalized translations. Stable scores for [comet]{.smallcaps} also confirm that translation quality is maintained during style adaptations.

{{< include ../tables/chap-7-sae-litmt/_zs-ms-perf.qmd >}}

### Finding Personalization Information in LLM Representations {#sec-chap7-probing}

In light of these results, we set out to test how the model encodes information reflecting a stylistic shift when style-appropriate examples are provided. To this purpose, we train *linear probes* [@belinkov-2022-probing] using model activations as input features to predict the style label ([MT]{color="brand-color.darkorange"}, [H$_1$]{color="brand-color.darkgreen"}, or [H$_2$]{color="brand-color.lightbluedim"}) that the style classifier (from @sec-chap7-discern) would assign to the eventual translation, based purely on the prompt's internal representation. Probing accuracy is measured by testing their accuracy in predicting the classified outcome before the generation, using only the prompt representation formed by the model. Given a test set of human-translated paragraphs, we train our probes on a set of examples using an MS prompt with 20 in-context examples. The set is balanced between prompts showcasing personalization with gold in-context examples from a human translator, and non-personalized prompts with MT-generated examples previously produced by the same tested model in a ZS setup. Test examples are selected from the respective novels to ensure for the classifier prediction shifts from [MT]{color="brand-color.darkorange"} in the ZS setting to the style of in-context examples when MS is used, signaling a causal influence of demonstrations on output personalization.^[Examples are resampled for every test paragraph to prevent the probe from overfitting on spurious prompt features.] This balanced setup prevents the leaking of task information, e.g., number of in-context examples, to learned probes, ensuring that stylistic differences among human and MT-generated in-context examples are the sole factor determining differences in model activations. We focus specifically on Gemma models, extracting activations after the attention block at each model layer for the last token of the prompt, which was previously shown to encode key task-relevant information [@hendel-etal-2023-context; @todd-etal-2024-function; @scalena-etal-2024-multi]. @fig-chap7-probing reports probe accuracies across all layers of Gemma 2 2B and 9B. We find a peak in probe accuracy of $\sim95\%$ around intermediate model layers, suggesting that these layers encode stylistic information with near-perfect precision.^[We find probes for layers 13 and 21 to perform best for the 2B and 9B models, respectively.] These results confirm that personalization is discernible from LLMs' internal representation, motivating our experiments towards the design of inference-time interventions to steer models towards personalized MT outputs.

:::{#fig-chap7-probing layout="[45,-5, 45]" layout-valign="bottom" fig-pos="t"}

![](../figures/chap-7-sae-litmt/probing_gemma-2-2b-it.pdf)

![](../figures/chap-7-sae-litmt/probing_gemma-2-9b-it.pdf)

Probing classifier performance on the human translation detection task across Gemma 2 2B (left) and 9B (right) layers. Activations in intermediate layers are found to capture translation style information with high precision.
:::

## Methods

We begin by introducing the prompting and steering methods that we use as baselines and outline our own proposed SAE-based steering approach for personalized translation.

### Prompting Baselines {#sec-chap7-prompting}

[Zero-Shot (ZS)]{.paragraph} The ZS setup used in our main experiment correspond to the one from @sec-chap7-discern, in which the model is simply asked to produce a translation with no conditioning from examples or explanations towards the target translation style. We use this setting to establish a baseline style and translation quality performance for the models.

[Zero-Shot Explain (ZS-Exp)]{.paragraph} Building upon the ZS setting, we experiment with a prompting strategy where LLMs are provided with detailed explanations of the most salient elements that characterize the desired translation style. We obtain such descriptions by prompting a capable proprietary model, GPT-4o [@openai-2023-gpt4], with 20 translations matching the desired style, asking it to synthesize a set of guidelines to matching the examples. We evaluate two contrastive variants of this approach, providing GPT-4o with either MT examples (ZS-Exp$_\text{HT}$) or alternative human translations ((ZS-Exp$_\text{PT}$)) alongside examples matching the desired style, and asking to describe what characterizes the latter compared to the former. To avoid data leakage, all generated explanations are manually reviewed to ensure they do not contain any verbatim content or direct excerpts from the input examples.^[Details on the prompt templates are in @sec-sae-litmt-explain.] Tested models are then prompted with GPT-4o explanations in a ZS setting, to verify whether interpretable directives synthesized from a set of examples matching the desired behavior can produce reliable personalization results.

[Multi-Shot (MS)]{.paragraph} Following @sec-chap7-produce's findings, we adopt the same MS setup using 20 in-context translation examples matching the style of a target human translator ([H$_1$]{color="brand-color.darkgreen"} or [H$_2$]{color="brand-color.lightbluedim"}).

### Steering Baselines

We employ the Activation Addition (ActAdd) and Representation Fine-tuning ([ReFT]{.smallcaps}) methods introduced in @sec-chap2-steer-activations as baselines for comparing the effectiveness of our proposed method.
For ActAdd, we employ the standard contrastive formulation by @rimsky-etal-2024-steering and @scalena-etal-2024-multi to extract two sets of style-relevant ($\{z\}^+$) and default ($\{z\}^-$) activations from a given model layer using 20 in-context examples demonstrating default behavior ([MT]{color="brand-color.darkorange"}) and the desired behavior ([H$_1$]{color="brand-color.darkgreen"} or [H$_2$]{color="brand-color.lightbluedim"} translations), respectively. We then compute the average $\Delta$ steering vector between the two sets of activations, scale it by a factor of $\alpha$ = 2 which was found effective by previous research [@scalena-etal-2024-multi] and apply it additively to the same model layer during inference. For [ReFT]{.smallcaps}, we apply learned interventions to the same personalization-relevant layers identified in @sec-chap7-probing and limit confounding factors by tuning ReFT interventions with the set of 20 examples used for MS prompting.

### Contrastive SAE Steering

Given the SAE formulation we present in @sec-chap2-steer-activations, our primary interest lies in the sparse latents $h(z_l) \in \mathbb{R}^m$ learned by the SAE encoder, which were empirically found to capture monosemantic and interpretable properties of model inputs.

[Contrastive prompt setup]{.paragraph} Given a set of paragraphs $\mathcal{D}$ for a novel in the [Par3]{.smallcaps} dataset, each instance in it is a tuple:

$$
\mathcal{D} = \left\{ \left< s, \text{H}_1, \text{H}_2, \text{MT}_{\text{model}} \right> \right\}
$$

with s being the non-English source sentence, $\text{H}_1$ and $\text{H}_2$ translations from two distinct human translators and $\text{MT}_{\text{model}}$ the machine translation from the model under evaluation. Similar to previous methods, we employ a contrastive approach to extract SAE latents that are most active in the presence of the desired personalization style, while simultaneously controlling for more generic features capturing generic properties of the task. We define two sets of contrastive prompts:

$$
\mathcal{D}^+ = \left\{ \left< s, e^{+} \right> \right\}\;\;\text{and}\;\;\mathcal{D}^- = \left\{ \left< s, e^{-} \right> \right\}
$$

capturing the personalized style of interest and baseline properties of the task, respectively. Similarly to the ZS-Exp setup from @sec-chap7-prompting, we explore two $\mathcal{D}^-$ configurations using either $e^- =$ [MT]{color="brand-color.darkorange"} (SAE Cont.$_\text{HT}$) or $e^- =$ [H$_2$]{color="brand-color.lightbluedim"} (or [H$_1$]{color="brand-color.darkgreen"}, if [H$_2$]{color="brand-color.lightbluedim"} is the personalization target; SAE Cont.$_\text{PT}$) to assess the effect of baseline choice on steering effectiveness.

[Feature extraction]{.paragraph} First, we gather activations $z^+_l$ and $z^-_l$ by prompting the model with inputs from the two contrastive sets $\mathcal{D}^+$ and $\mathcal{D}^-$. Activations are extracted at the last prompt token position from its most informative layer, as identified in @sec-chap7-probing. Activations are then converted into sparse latent representations $x^+ = h(z^+)$ and $x^- = h(z^-)$, with $x^+, x^- \in \mathbb{R}^m$ by the SAE encoder. This procedure is repeated across 20 contrastive examples, resulting in two collections of SAE latent vectors for positive/negative examples:

$$
\begin{aligned}
\mathcal{X}^+ &= \left\{x^{+}_1, x^{+}_2, \dots, x^{+}_{20} \right\} \\
\mathcal{X}^- &= \left\{x^{-}_1, x^{-}_2, \dots, x^{-}_{20} \right\}
\end{aligned}
$$

[Relevance-based Feature Selection]{.paragraph} To identify discriminative features for personalization in the large set of latents, we employ an information-theoretic approach adapted from @zhao-etal-2025-steering. For each of the inputs, we identify the subset of size $n < m$ including only the SAE active features, i.e. latent dimensions for which the logit is $> 0$. We consider logit values in this subset as instances of a random variable $X_i \in x$, and calculate the mutual information $I(X_i, Y)$ between each feature $X_i$ and the target binary variable $Y = \{+, -\}$ corresponding to the style of the provided examples (personalized or non-personalized). A higher $I(X_i, Y)$ indicates that the $i$-th feature is more informative for discriminating between personalized and default inputs, and can hence be used for steering. A representative sample of 40 latents showing the highest mutual information scores for both personalized ($\{X_i\}^+$) and non-personalized ($\{X_i\}^-$) examples is selected using this procedure.^[By contrast, traditional SAE-based steering methods only employ features associated with the positive class [@chalnev-etal-2024-improving; @arditi-etal-2024-refusal].] For every selected latent, we compute its expected logit when personalization is present or absent in provided examples, i.e. $\mathbb{E}^+[X_i]$ and $\mathbb{E}^-[X_i]$.

[Inference-time intervention]{.paragraph} Finally, activations are steered by setting selected latents to their expected value whenever their observed score is below (for the promoted personalized case) or above (for the demoted non-personalized case) the pre-computed average. Hence, in the SAE Cont.$_\text{HT}$ setting we enhance the features relevant to a target personalized style, e.g. $\{X_i\}^{H_1}$ for [H$_1$]{color="brand-color.darkgreen"}, and suppress the features $\{X_i\}^{MT}$, corresponding to the model's default [MT]{color="brand-color.darkorange"}. In SAE Cont.$_\text{PT}$, instead, we promote the same [H$_1$]{color="brand-color.darkgreen"}-related latents while suppressing $\{X_i\}^{H_2}$ to steer the model towards [H$_1$]{color="brand-color.darkgreen"} personal style. Additionally, we modulate the magnitude of the resulting vector with an $\alpha$ *coefficient*, which was found to play an essential role in steering effectiveness in previous research [@scalena-etal-2024-multi; @ferrando-etal-2025-do]. @alg-saesteer outlines the procedure for our proposed latent-based steering. It enhances features identified as relevant to personalization while simultaneously suppressing those negatively correlated with the task.

```pseudocode
#| label: alg-saesteer
#| pdf-placement: "ht!"

\begin{algorithm}
\caption{Contrastive SAE Steering}
\begin{algorithmic}
\Require Input activation $z$, SAE model, target latents expected value $\mathbb{E}^+[X_i]$, contrast latents expected value $\mathbb{E}^-[X_i]$, steering coefficient $\alpha$
\Ensure Steered activation $z_{\text{new}}$
\Procedure{ContrastiveSteering}{$z, \text{SAE}, \mathbb{E}^+[X_i], \mathbb{E}^-[X_i], \alpha$}
    \State $x = \text{SAE.encode}(z)$
    \State $m = \text{length}(x)$
    \For{$i \gets 1$ to $m$}
        \If{$\mathbb{E}^+[X_i] > x[i]$}
            \State $x[i] = \mathbb{E}^+[X_i]$
        \EndIf
        \If{$\mathbb{E}^-[X_i] < x[i]$}
            \State $x[i] =\mathbb{E}^-[X_i]$
        \EndIf
    \EndFor
    \State $z_{\text{new}} = \alpha \cdot \text{SAE.decode}(x)$
    \State \textbf{return} $z_{\text{new}}$
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

## Experiments {#sec-chap7-exp}

### Setup

[Model selection]{.paragraph} We evaluate our methods on the same three models used for our preliminary evaluation of @sec-chap7-preliminary. Our selection is guided by the availability of open-source pre-trained SAEs, which can be otherwise computationally expensive to train. For Gemma models, we employ SAEs from the GemmaScope suite [@lieberum-etal-2024-gemma]; for the Llama 3.1 model we employ the SAE released by @mcgrath-etal-2024-understanding. GemmaScope SAEs are available for every model layer, enabling us to steer Gemma models on their most informative layers for the task, which we identified in @sec-chap7-probing. On the contrary, a single SAE for the 19th layer is available for Llama, hence limiting our evaluation of SAE steering and potentially producing sub-optimal steering results for that model.

{{< include ../tables/chap-7-sae-litmt/_results-averaged.qmd >}}

[Metrics]{.paragraph} We evaluate our approaches on a held-out test set sourced from the [Par3]{.smallcaps} dataset for personalization and output quality. For personalization, we use the classifiers described in @sec-chap7-discern. We define three submetrics employing the classifier probability distribution over the three classes ([MT]{color="brand-color.darkorange"}, [H$_1$]{color="brand-color.darkgreen"}, [H$_2$]{color="brand-color.lightbluedim"}) to better analyze different aspects of classifiers predictions. First, we compute **H** accuracy as the classifier's total probability assigned to human-like translations, $p($[H$_1$]{color="brand-color.darkgreen"}$) + p($[H$_2$]{color="brand-color.lightbluedim"}$)$, thereby measuring the generic *human-like* style of the text. To measure personalization, we employ the personalization **P**, corresponding only to the human translation currently selected as target ([H$_1$]{color="brand-color.darkgreen"} or [H$_2$]{color="brand-color.lightbluedim"}). Finally, the more stringent **P$_\text{flip}$** metric measures the proportion of examples for which the applied conditioning procedure (either prompting or steering) causally influences the resulting classifier prediction, identifying examples for which the label flips from [MT]{color="brand-color.darkorange"} to the desired target.

To ensure that our interventions do not result in a degradation of overall translation quality, we also employ [comet]{.smallcaps}^[[`Unbabel/wmt22-comet-da`](https://huggingface.co/Unbabel/wmt22-comet-da`)] [@rei-etal-2020-comet] using the personalized translation as reference.

[Quality-accuracy trade-off]{.paragraph} We begin by verifying the optimal steering intensity $\alpha$ for our SAE steering technique. We primarily focus on results from Gemma 2 2B, for which we ran a comprehensive sweep over all relevant hyperparameters.^[Larger models were evaluated using a subset of the best-performing configurations. Details in @sec-sae-litmt-all-models.] @fig-chap7-alpha-sel illustrates the influence of $\alpha$ on MT personalization accuracy and fluency averaged across all translators for all tested languages. For values of $\alpha \leq 3$, performance remains close to that of the MS baseline, indicating that the contrastive method is effectively isolating latents associated with human-like style. As $\alpha$ increases, performance generally exceeds the MS approach, achieving greater control and flexibility in guiding the model's output with next to no impact of translation quality. However, for $\alpha \geq 10$, we observe a major degradation in Comet, suggesting an important drop in translation fluency. @tbl-chap7-extreme-alphas shows some examples of models generating output aligned with the Human translator according to the classifier but with a low [comet]{.smallcaps} score corresponding to an almost unreadable output due to extreme $\alpha$ values. False positive classifications in such settings suggest that steering methods and classifiers are aligned with potentially spurious stylistic features, which are not necessarily indicative of high-quality translations. We leave the investigation of these spurious features to future work, focusing here on the trade-off between personalization and translation quality.

{{< include ../tables/chap-7-sae-litmt/_extreme-alphas.qmd >}}

Following @ferrando-etal-2025-do, which also employ SAEs for steering, we experiment with very high alpha values (up to 150), finding the classifier's **H** accuracy approaching 100% for some languages. While this indicates that the contrastive steering is aggressively optimizing toward classifier preferences (@fig-chap7-alpha-high), the consequent drop in [comet]{.smallcaps} scores reveals a steep decline in translation quality, often resulting in incoherent or nonsensical generations from a human perspective. Ultimately, we identify $\alpha$ = 5 as an appropriate steering intensity to balance personalization and fluency, and employ it for our main evaluation.

![Personalization **P** and [comet]{.smallcaps} across various steering intensity $\alpha$ for SAE Cont.$_\text{HT}$ on Gemma 2 2B. The performance of prompting baselines (ZS, MS, ZS-Exp$_\text{HT}$) is also reported.](../figures/chap-7-sae-litmt/gemma-2-2b-it_improvements.pdf){#fig-chap7-alpha-sel fig-pos="t" width=60%}

![[comet]{.smallcaps} and **H** accuracy across $\alpha$ steering intensity values for Gemma 2 2B, showing a major drop in translation quality for very high intensities ($\alpha \geq 50$).](../figures/chap-7-sae-litmt/gemma-2-2b-it_alpha_vs_H_accuracy_and_COMET.pdf){#fig-chap7-alpha-high fig-pos="t" width=60%}

### Results and Discussion

@tbl-chap7-results-averaged presents performances of tested models across prompting and steering setups, averaged across all languages and personalization targets ([H$_1$]{color="brand-color.darkgreen"} and [H$_2$]{color="brand-color.lightbluedim"} for each language). We find that *our SAE Cont.$_\text{HT}$ and SAE Cont.$_\text{PT}$ methods generally achieve the best trade-off between personalization accuracy and translation quality*, especially for the smaller Gemma 2 2B model. This could be due to the larger models' superior ability to incorporate in-context information naturally, reducing the relative benefit of explicit steering. Comparing the two contrastive setups (HT and PT) for the ZS-Exp and SAE Cont. methods, we find that *using different human demonstrations as a contrastive baseline in PT generally produces better results for larger models*. As for general performance, we conjecture this could be due to the larger models' improved ability to disentangle personalization-critical factors without explicit guidance. For the smaller Gemma 2 2B, the difference between the two approaches is minimal, suggesting the model cannot fully exploit the examples' differences.

[Do SAE Steering and MS Prompting Impact Activations in a Similar Way?]{.paragraph} Since SAE-based approaches perform on par or better than MS, we set out to investigate whether the two methods result in a similar impact on model representations. We collect the modified activations $z_{\text{steer}}$ obtained from the SAE Cont.$_\text{HT}$ steering setting and evaluate them using the *probing classifier* trained on MS-conditioned activations we introduced in @sec-chap7-probing for detecting personalization information. @tbl-chap7-prober-steer shows probe accuracy in detecting the positive impact of SAE steering across the three possible outcomes of the steering procedure. We find that the *probe* trained on the SAE layer effectively distinguishes between activations corresponding to successful and unsuccessful SAE steering, despite having been exposed only to MS conditioning during training. This includes both instances for which the classifier prediction is flipped after steering (MT $\to$ H*), and settings where the conditioning fails (MT $\to$ MT). In settings where the original output already matches human style (H* $\to$ H*), the probe obtains lower accuracy with broader confidence intervals, denoting higher uncertainty. These findings suggest that the **SAE's latents we extract through our contrastive method are meaningfully connected to the stylistic patterns embedded in the multi-shot examples**, providing evidence that our intervention influences the internal representations of the model, aligning them to the natural effect of the MS approach.

{{< include ../tables/chap-7-sae-litmt/_prober-steer.qmd >}}

## Limitations

While we demonstrates the potential of steering LLMs for MT personalization using sparse autoencoders, we acknowledge several limitations.

Firstly, our findings' generalizability is constrained by the scope of our experiments. We focused on literary translation into English from seven specific source languages and evaluated three LLMs of relatively small size. Consequently, the observed effectiveness of SAE-based steering and the identified optimal layers for intervention may not directly transfer to other language pairs, significantly different model architectures or sizes, or distinct domains beyond literary texts. Further research is needed to assess the robustness of our approach across a broader range of linguistic and modeling contexts.

Secondly, the computational overhead associated with sparse autoencoders presents a practical challenge. Although we utilized pre-trained SAEs in our study, the initial training of these components is resource-intensive. This could limit the accessibility and scalability of our proposed method, particularly for researchers or practitioners with limited computational resources or when frequent retraining for new models or tasks is required. The current availability of pre-trained SAEs also restricts model choice, as seen with the Llama 3.1 8B model where an SAE was only available for a potentially sub-optimal layer.

Finally, our investigation primarily focused on downstream performance and the impact of various personalization strategies on model representations. However, we did not pursue a mechanistic understanding of the "personalization circuits" within the LLMs. Future work could adopt a more fine-grained, mechanistic interpretability approach to study how specific SAE latents or combinations thereof encode and manipulate nuanced stylistic features, thereby providing deeper insights into the underlying processes of LLM personalization.

## Conclusion

We conducted a broad evaluation of various prompting and steering approaches for personalizing LLM-generated translations. Our evaluation targets a practical, real-world application of literary translation and addresses the underexplored challenge of steering LLM generations in a linguistically rich and stylistically sensitive domain. Through comprehensive evaluation across multiple languages, novels, and models, we demonstrate that our proposed SAE-based approach outperforms prompting and alternative steering techniques.

Although faithfully replicating individual human translation styles remains a highly challenging task, our approach achieves strong alignment with human translation quality, as reflected in both general human-likeness and translator-specific personalization metrics. These results highlight the method's robustness and its potential to support high-fidelity translation workflows in real-world settings. Concretely, these results have important implications in the development of personalized MT systems based on LLMs. In particular, the notable effectiveness of our proposed approach on smaller models might enable MT personalization in lower-resource settings, facilitating further research on how personalization information is encoded and produced by language models. Despite their effectiveness, the interpretability of the learned SAE latents and their possible use of with larger LLMs---where increased capacity may further enhance the precision and fluency of personalized translations---remain open questions for future investigations.

The success of SAE-based steering for personalized translation highlights the effectiveness of internals-based interventions for controlling model generation. However, the practical deployment of such approaches requires a careful evaluation of their impact on users' trust and behaviors. While the first two parts of this thesis focused on developing methods for analyzing and steering model generation, we conclude by focusing on the users of machine translation systems, in particular professional post-editors and translators, to explore how their interactions with machine-translated content are shaped by factors like language similarity and translation quality. Finally, we investigate whether these interactions can be improved using the trove of information available from the inner workings of MT models.