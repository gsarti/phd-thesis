# Machine Translation Post-editing for Typologically Diverse Languages {#sec-chap-8-divemt}

::: {.callout-note appearance="simple" icon="false"}

This chapter is adapted from the paper *DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages* [@sarti-etal-2022-divemt].

:::

::: {.callout-note icon="false"}

## Chapter Summary

We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset to foster new research on the translation capabilities of NMT systems for typologically diverse languages.

:::

## Introduction

Recent advances in neural language modeling and multilingual training have prompted a widespread adoption of machine translation (MT) technologies across an unprecedented range of world languages.
While the benefits of state-of-the-art MT for cross-lingual information access are undisputed [@gene-2021-post], its usefulness as an aid to professional translators varies considerably across domains, subjects and language combinations [@zouhar-etal-2021-neural]. In the last decade, the MT community has been including an increasing number of languages in its automatic and human evaluation efforts [@bojar-etal-2013-findings; @wmt-2021-machine].
However, the results of these evaluations are typically not directly comparable across different language pairs for various reasons.
First, reference-based automatic quality metrics are hardly comparable across different target languages [@bugliarello-etal-2020-easier].
Secondly, human judgments are collected independently for different language pairs, making their cross-lingual comparison vulnerable to confounding factors such as tested domains and training data sizes.
Similarly, recent work on NMT post-editing efficiency has focused on specific language pairs such as English-Czech [@zouhar-etal-2021-neural], German-Italian, German-French [@laubli-etal-2019-post] and English-Hindi [@ahsan-etal-2021-assessing], but a controlled comparison across a set of typologically diverse languages is still lacking.

![The DivEMT data collection process. For every English source document, 18 professional translators are tasked to translate it from scratch (HT) or post-edit NMT systems' outputs (PE$_1$/PE$_2$) into six typologically diverse target languages. Behavioral data and qualitative assessments are collected during and after the process respectively.](../figures/chap-8-divemt/divemt.pdf){#fig-divemt-overview fig-pos="t"}

In this work, we assess the usefulness of state-of-the-art NMT in professional translation with a strictly controlled cross-language setup ([@fig-divemt-overview]). Specifically, professionals were asked to translate the same English documents into six typologically different languages (Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese) using the same platform and guidelines.
Three *translation modalities* were adopted: human translation from scratch (HT), post-editing of Google Translate's translation (PE$_1$), and post-editing of mBART-50's translation (PE$_2$), the latter being a state-of-the-art open-source, multilingual NMT system.
In addition to post-editing results, subjects' fine-grained editing behavior --- including keystrokes and time information --- was logged to measure productivity and effort across languages, systems and translation modalities. Finally, translators were asked to complete a qualitative assessment regarding their perceptions of MT quality and post-editing effort.
The resulting DivEMT dataset is to our best knowledge the first public resource allowing a direct comparison of professional translators' productivity and fine-grained editing information across a set of typologically-diverse languages.
DivEMT is publicly released alongside this paper as a unique resource to study the language- and system-dependent nature of NMT advances in real-world translation scenarios.

## Related Work

[Cross-lingual MT Evaluation]{.paragraph} Before the advent of NMT, @birch-etal-2008-predicting studied how various language properties affected the quality of Statistical MT (SMT) across a sizeable sample of European language pairs.
The comparison, however, was solely based on BLEU, which is in fact not comparable across different target languages [@bugliarello-etal-2020-easier].
Recent work on neural models introduced more principled ways to measure the intrinsic difficulty of language-modeling [@gerz-etal-2018-relation; @cotterell-etal-2018-languages; @mielke-etal-2019-kind] and machine-translating [@bugliarello-etal-2020-easier; @bisazza-etal-2021-difficulty] different languages.
However, achieving this reliably without any human evaluation remains an open research question.
Human evaluations of MT quality are routinely conducted during campaigns such as WMT [@ws-2006-statistical; @akhbardeh-etal-2021-findings] and IWSLT [@cettolo-etal-2016-iwslt; @cettolo-etal-2017-overview] among others, but their focus is on language- and domain-specific ranking of MT systems --- often leveraging non-professional annotators [@freitag-etal-2021-experts] --- rather than cross-lingual quality comparisons.
Concurrently to this work, @licht-etal-2022-consistent proposed a new human evaluation protocol to improve consistency in cross-lingual MT quality assessment.

[Post-editing NMT]{.paragraph} Measuring post-editing effort across its *temporal, cognitive, and technical* dimensions [@krings-2001-repairing] is a well-established way to assess the effectiveness and efficiency of MT as a component of specialized translation workflows.
Seminal post-editing studies highlighted an increase in translators' productivity following MT adoption [@guerberof-2009-productivity; @green-etal-2013-efficacy; @laubli-etal-2013-assessing; @plitt-masselot-2010-productivity; @parra-escartin-arcedillo-2015-machine].
However, they also struggled to identify generalizable findings due to confounding factors like output quality, content domains, and high variance across language pairs and human subjects.
With the advent of NMT, productivity gains of the new approach were extensively compared to those of SMT, the highly-customized dominant paradigm at the time [@castilho-etal-2017-neural; @bentivogli-etal-2016-neural; @toral-etal-2018-postediting; @laubli-etal-2019-post]. Initial results were promising for NMT due to its better fluency and overall results.
Moreover, translators were shown to prefer NMT over SMT for post-editing, although a pronounced productivity increase was not always present.
More recent work highlighted the productivity gains driven by NMT post-editing in a wider array of languages that were previously challenging for MT,
such as English-Dutch [@daems-etal-2017-translation], English-Hindi [@ahsan-etal-2021-assessing],
English-Greek [@stasimioti-sosoni-2020-translation],
English-Finnish and English-Swedish [@koponen-etal-2020-mt],
all showing a considerable variance among language pairs and subjects.
Interestingly, @zouhar-etal-2021-neural found NMT post-editing speed to be comparable to translation from scratch in English-Czech, and highlighted a disconnect between moderate increases in automatic MT quality metrics and better post-editing productivity.
In sum, research on post-editing NMT generally reports increased fluency and output quality, but productivity gains are hardly generalizable across language pairs and domains.
Importantly, to our knowledge, no previous work has studied NMT post-editing over a set of typologically different languages while controlling for the effects of content types and domains, NMT engines, and translation interfaces.

## The DivEMT Dataset

DivEMT's main purpose is to assess the usefulness of state-of-the-art NMT for professional translators and to study how this usefulness varies across target languages with different typological properties.
We present below our data collection setup, which strikes a balance between simulating a realistic professional translation workflow and maximizing the comparability of results across languages.

### Subjects and Task Scheduling

To control for the effect of individual translators' preferences and styles, we involve a total of 18 subjects (three per target language). During the experiment, each subject receives a series of short *documents* (3 to 5 sentences each) where the source text is presented in isolation (HT) or alongside a translation proposal produced by one of the NMT systems (PE$_1$, PE$_2$). The experiment comprises two phases:
During the **warm-up phase** a set of 5 documents is translated by all subjects following the same, randomly sampled sequence of modalities (HT, PE$_1$ or PE$_2$). This phase allows the subjects to get used to the setup and enables us to spot possible issues in the logged behavioral data before moving forward.[^8-1]
In the **main collection phase**, each subject is asked to translate documents in a pseudo-random sequence of modalities. This time, however, the sequence is different for each translator and chosen so that each document gets translated in all three modalities.
This allows us to measure translation productivity independently from the subject's productivity and document-specific difficulties.
A graphical overview of this process is shown in [@fig-divemt-overview], with additional details given in [@sec-divemt-modality-scheduling].
As productivity and other behavioral metrics can only be estimated with a sizable sample, we prioritize the number of documents over the number of subjects per language during budget allocation. A larger set of post-edited documents also provides more insight in the error type distribution of NMT systems across different language pairs, an analysis which we leave to future work.

All subjects are professional translators with at least 3 years of professional experience, at least one year of post-editing experience and strong proficiency with CAT tools.[^8-2]
Translators were provided with links to the source articles to facilitate contextualization, were asked to produce translations of publishable quality and were instructed not to use any external MT engine to produce their translations. Assessing the final quality of the post-edited material is out of the scope of the current study, although we realize that this is an important consideration to assess usability in a professional context. A summary of our translation guidelines is provided in [@sec-divemt-guidelines].

[^8-1]: Warm-up data are excluded from the analysis of [@sec-pe_effort].
[^8-2]: Additional subjects' details are available in [@sec-divemt-subject-info].

### Choice of Source Texts

The selected documents represent a subset of the FLORES-101 benchmark [@goyal-etal-2022-flores] consisting of sentences taken from English Wikipedia, and covering a mix of topics and domains.[^8-3] While professional translators generally specialize in one or a few domains, we opt for a mix-domain dataset to minimize domain adaptation efforts by the subjects and maximize the generalizability of our results.
Importantly, FLORES-101 includes high-quality human translations into 101 languages, which makes it possible to automatically estimate NMT quality and discard excessively low-scoring models or language pairs before our experiment.
FLORES-101 also provides useful metadata, e.g. source URL, which allows us to ensure the absence of public translations of the selected contents, which could be leveraged by translators and compromise the validity of our setup.
The documents used for our study are fragments of contiguous sentences extracted from Wikipedia articles that compose the original FLORES-101 corpus. Even if small, the context provided by document structure allows us to simulate a more realistic translation workflow if compared to out-of-context sentences.

Based on our available budget, we select 112 English documents from the *devtest* portion of FLORES-101 corresponding to 450 sentences and 9626 words. More details on the data selection process are provided in [@sec-divemt-doc-select].

[^8-3]: We use a balanced sample of articles sourced from WikiNews, WikiVoyage and WikiBooks.

{{< include ../tables/chap-8-divemt/_languages-small.qmd >}}

### Choice of Languages {#sec-choice-languages}

Training data is among the most important factors in defining the quality of a NMT system.
Unfortunately, using strictly comparable or multi-parallel datasets, like Europarl [@koehn-2005-europarl] or the Bible corpus [@mayer-cysouw-2014-creating], would dramatically restrict the diversity of languages available to our study, or imply a prohibitively low translation quality on general-domain text.
In order to minimize the effect of training data disparity while maximizing language diversity, we choose representatives of six different language families for which comparable amounts of training data are available in our open-source model, namely **Arabic**, **Dutch**, **Italian**, **Turkish**, **Ukrainian**, and **Vietnamese**. As shown in [@tbl-languages-small], our language sample ensures a good diversity in terms of language family and relatedness to English, type of morphological system, morphological complexity --- measured by mean size of paradigm (MSP, @xanthos-etal-2011-role) --- and script. We also report type-token ratio (TTR), the only language property that was found to correlate significantly with translation difficulty in a sample of European languages [@bugliarello-etal-2020-easier].
While the amount of language-specific parallel sentence pairs used for the multilingual fine-tuning of mBART-50 varies widely (4K<$N$<45M), all our selected language pairs fall within the 100K-250K range (mid-resourced, see [@tbl-flores-perf]), enabling a fair cross-lingual performance comparison.

{{< include ../tables/chap-8-divemt/_flores-perf.qmd >}}

{{< include ../tables/chap-8-divemt/_data_example_small.qmd >}}

### Choice of MT Systems {#sec-choice-systems}

While most of the best-performing general-domain NMT systems are commercial, experiments based on such systems are not replicable as their back-ends get silently updated over time. Moreover, without knowing the exact training specifics, we cannot attribute differences in the cross-lingual results to intrinsic language properties.
We balance these observations by including two NMT systems in our study: **Google Translate** (GTrans)[^8-4] as a representative of commercial quality, and **mBART-50 one-to-Many**[^8-5] [@tang-etal-2021-multilingual] as a representative of state-of-the-art open-source multilingual NMT technology. The original multilingual BART model [@liu-etal-2020-multilingual-denoising] is an encoder-decoder transformer model pre-trained on monolingual documents in 25 languages. @tang-etal-2021-multilingual extend mBART by further pre-training on 25 new languages and performing *multilingual translation fine-tuning* for the full set of 50 languages, producing three configurations of multilingual NMT models: many-to-one, one-to-many, and many-to-many.
Our choice of mBART-50 is largely motivated by its maneageable size, its good performances across the set of evaluated languages (see [@tbl-flores-perf]) and its adoption for other NMT [@liu-etal-2021-continual] and post-editing [@fomicheva-etal-2022-mlqe] studies.
Although mBART-50 performances are usually comparable or slightly worse than the ones of tested bilingual NMT models,[^8-6] using a multilingual model allows us to evaluate the downstream effectiveness of a single, unified system trained on pairs evenly distributed across tested languages. Finally, adopting two systems with marked differences in automatic evaluation scores allows us to estimate how a significant increase in metrics such as BLEU, ChrF and [comet]{.smallcaps} [@papineni-etal-2002-bleu; @popovic-2015-chrf; @rei-etal-2020-comet] impacts downstream productivity across languages in a realistic post-editing scenario.

[^8-4]: Evaluation performed in October 2021.
[^8-5]: [`https://huggingface.co/facebook/mbart-large-50-one-to-many`](https://huggingface.co/facebook/mbart-large-50-one-to-many)
[^8-6]: See [@sec-divemt-other] for automatic MT quality results by five different models over a larger set of 10 target languages.

### Translation Platform and Collected Data

Translators were asked to use PET [@aziz-etal-2012-pet], a computer-assisted translation tool that supports both translating from scratch and post-editing.
This tool was chosen because (i) it logs information about the post-editing process, which we use to assess effort (see [@sec-pe_effort]); and (ii) it is a mature research-oriented tool that has been successfully used in several previous studies [@koponen2012post; @toral-etal-2018-postediting].
We collect three types of data:

- **Resulting translations** produced by translators in either HT or PE modes, constituting a multilingual corpus with one source text and 18 translations (one per language-modality combination) exemplified in [@tbl-data_example_small].
- **Behavioral data** for translated sentences, including editing time, amount and type of keystrokes (content, navigation, erase, etc.), and number and duration of pauses above 300/1000 milliseconds [@lacruz-etal-2014-cognitive].
- **Pre- and post-task questionnaire**. The former focuses on demographics, education, and work experience with translation and post-editing. The latter elicits subjective assessments of post-editing quality, effort and enjoyability compared to translating from scratch.

## Post-Editing Effort Across Languages {#sec-pe_effort}

![Temporal effort across languages and translation modalities, measured in seconds per processed source word. Each point represents a document, with higher scores denoting slower editing. $\uparrow$: amount of data points per language not shown in the plot.](../figures/chap-8-divemt/time_per_src_word_small.pdf){#fig-time-per-src-word width="65%"}

In this section, we use the DivEMT dataset to quantify the post-editing effort of professional translators across our diverse set of target languages.
We consider two main objective indicators of editing effort, namely *temporal measurements* (and related productivity gains) and
*post-editing rates*, measured by the Human-targeted Translation Edit Rate (HTER, @snover-etal-2006-study).
Finally, we assess the subjective perception of PE gains by examining the post-task questionnaires. We reiterate that all scores in this section are computed on the same set of source sentences for all languages, resulting in a faithful cross-lingual comparison of post-editing effort thanks to DivEMT's controlled setup.

### Temporal Effort and Productivity Gains

We start by comparing *task time* (seconds per processed source word) across languages and modalities. For this purpose, edit times are computed for every document in every language without considering the presence of multiple translators for every language.
As shown in [@fig-time-per-src-word], translation time varies considerably across languages even when no MT system is involved (HT), suggesting an intrinsic variability in translation complexity for different subjects and language pairs.
Indeed, for the HT modality, the time required for the `slowest' target languages (Italian, Ukrainian) is roughly double the `fastest' one (Turkish). This pattern cannot be easily explained and contrasts with factors commonly tied to MT complexity, such as source-target morphological richness and language relatedness [@birch-etal-2008-predicting; @belinkov-etal-2017-neural].
On the other hand, we find the relation PE$_1$ > PE$_2$ > HT (PE$_1$ fastest, PE$_2$ medium speed, HT slowest) to hold for all the evaluated languages.

{{< include ../tables/chap-8-divemt/_productivity.qmd >}}

For a measure of productivity gains that is easier to interpret and more in line with translation industry practices, we turn to *productivity* expressed in source words processed per minute and compute the *speed-up* induced by the two post-editing modalities over translating from scratch ($\Delta$HT). [@tbl-productivity] presents our results.
**Across systems**, we find that *large* differences among automatic MT quality metrics indeed reflect on post-editing effort, suggesting a nuanced picture that is complementary to the findings of @zouhar-etal-2021-neural. While post-editing time gains were observed to quickly saturate for slight changes in high-quality MT, we find that moving from medium-quality to high-quality MT yields meaningful productivity improvements across most evaluated languages.
**Across languages**, too, the magnitude of productivity gains ranges widely, from doubling in some languages (Dutch PE$_1$, Italian PE$_1$ and PE$_2$) to only about 10% (Arabic, Turkish and Ukrainian PE$_2$).
When only considering the better performing system (PE$_1$), post-editing remains clearly beneficial in all languages despite the high variability in $\Delta$HT scores.
Results are more nuanced for the open-source system (PE$_2$), with three out of six languages displaying only marginal gains (<15% in Arabic, Turkish and Ukrainian).
Despite its overall lower performance, mBART-50 (PE$_2$) is the only system enabling a fair comparison across languages (from the point of view of training data size and architecture, see [@sec-choice-systems]). Interestingly, if we focus on the gains induced by this system, factors like language relatedness and morphological complexity become relevant. Specifically, Italian (+95%), Dutch (+61%) and Ukrainian (+14%) are genetically and syntactically related to English, but Ukrainian has a richer morphology (see [@tbl-languages-small]).
On the other hand, Vietnamese (+23%), Turkish (+12%) and Arabic (+10%) all belong to different families. However, Vietnamese is isolating (little to no morphology), while Turkish and Arabic have very rich morphological systems (resp. agglutinative and introflexive, the latter of which is especially problematic for subword segmentation, @amrhein-sennrich-2021-suitable-subword).
Other differences are however harder to explain. For instance, Dutch is closely related to English and has a simpler morphology than Italian, but its productivity gain with mBART-50 is lower (61% vs 95%). This finding is accompanied by an important gap in BLEU and [comet]{.smallcaps} scores achieved by mBART-50 on the two languages (22.6 vs 24.4 BLEU and 0.532 vs 0.648 [comet]{.smallcaps} for Dutch vs Italian, resp.) which cannot be explained by training data size.

In summary, our findings confirm the overall positive impact of NMT post-editing on translation productivity observed in previous PE studies. However, we note how **the magnitude of this impact is highly variable across systems and languages**, with inter-subject variability also playing an important role, in line with previous studies [@koponen-etal-2020-mt] (see [@sec-limitations] for more details).
The small size of our language sample does not allow us to draw direct causal links between specific typological properties and post-editing efficiency.
That said, we believe these results have important implications on the claimed `universality' of current state-of-the-art MT and NLP systems, mostly based on the Transformer architecture [@vaswani-etal-2017-attention] and BPE-style subword segmentation techniques [@sennrich-etal-2016-neural].

#### Modeling Temporal Effort

{{< include ../tables/chap-8-divemt/_significance.qmd >}}

Given the high variability among translators, segments and translation modalities, we assess the validity of our observations via statistical analysis of temporal effort using a linear mixed-effects regression model (LMER, @lindstrom-bates-1988-lmer), following @green-etal-2013-efficacy and @toral-etal-2018-postediting. We fit our model on $n=7434$ instances, corresponding to 413 sentences translated by 18 translators[^8-7], using translation time as the dependent variable.
Our fixed predictors include translation modality, target language, their interaction and length of source segment in characters.[^8-8]
Our random effects structure includes random intercepts for different segments (nested with documents) and translators, as well as a random slope for modality over individual segments.[^8-9] [@tbl-significance] presents the set of predictors included in the final model, an estimate of their impact on edit times and their significance.
We find both PE modalities to significantly reduce translation times ($p<0.001$), with PE$_1$ being significantly faster than PE$_2$ ($p<0.001$) across all languages.
Taking the language for which HT is slowest (Ukrainian) as the reference level,
the reduction in time brought by Google is significantly more pronounced for Italian, Dutch ($p<0.001$), and Turkish ($p<0.05$).
For mBART-50, however, we only observe significantly more pronounced increases in productivity for Italian and Dutch ($p<0.001$) compared to the reference. We find these results to corroborate the observations of the previous section.

[^8-7]: Outliers were removed beforehand, see Appendix D.
[^8-8]: The document processing order was originally included to identify possible longitudinal effects but was removed due to a lack of significant improvements.
[^8-9]: Additional modeling details available in [@sec-divemt-modeling].

::: {.content-visible when-format="html" layout-ncol=2 fig-pos="t"}
![Human-targeted Translation Edit Rate (HTER) for Google Translate and mBART-50 post-editing across available languages.](../figures/chap-8-divemt/hter_per_system.pdf){#fig-hter}

![Distribution of error-less machine translation sentence outputs (no edits performed during post-editing) for each translator and every language.](../figures/chap-8-divemt/errorless_outputs_small.pdf){#fig-errorless-sentences}
:::

::: {.content-visible when-format="pdf" layout-ncol=1 fig-pos="t"}
![Human-targeted Translation Edit Rate (HTER) for Google Translate and mBART-50 post-editing across available languages.](../figures/chap-8-divemt/hter_per_system.pdf){#fig-hter width="65%"}

![Distribution of error-less machine translation sentence outputs (no edits performed during post-editing) for each translator and every language.](../figures/chap-8-divemt/errorless_outputs_small.pdf){#fig-errorless-sentences width="65%"}
:::

### Post-Editing Rate {#sec-pe-rate}

We proceed to study the post-editing patterns using the widely-adopted Human-targeted Translation Edit Rate (HTER, @snover-etal-2006-study), computed as the length-normalized sum of word-level substitutions, insertions, deletions and shift operations performed during post-editing.[^8-10]

As shown in [@fig-hter], PE$_1$ required less editing than PE$_2$ for all languages, and a high variability is observed across the two systems and all languages. Since translators were not informed about the presence of two MT systems, we exclude the possibility that these results reflect an over-reliance or distrust towards a specific MT system.
For Google Translate, Ukrainian shows the heaviest edit rate, followed by Vietnamese, whereas Arabic, Dutch, Italian and Turkish all show relatively low amounts of edits.
Focusing again on mBART-50 for a fairer cross-lingual comparison, Ukrainian is by far the most heavily edited language, followed by a medium-tier group composed of Vietnamese, Arabic and Turkish, and finally by Dutch and Italian as low-edit languages. Results show that several of our observations on the linguistic relatedness and type of morphology also apply to edit rates, with languages less related to English or having richer morphology requiring more post-edits on average.

[@fig-errorless-sentences] visualizes the large gap in edit rates across languages and subjects by presenting the amount of ``errorless'' MT sentences that were accepted directly, i.e. without any post-editing. We note again how the NMT system heavily influences the rate of occurrence of such sentences but nonetheless shows how Dutch and Italian generally present more errorless sentences than Ukrainian and Vietnamese. In particular, for Google Translate outputs, the average rate of error-less sentences is roughly 25% for the former target languages, while for the latter, it accounts only for the 3% of total translations. Surprisingly, the English-Turkish pair also fares well, despite the low source-target relatedness.

Finally, we note that post-editing effort appears to correlate poorly with the automatic MT quality metrics reported in [@tbl-flores-perf] (e.g. see high scores of Vietnamese and low scores of Dutch PE$_1$), highlighting a difficulty in predicting the benefits of MT post-editing over HT for new language pairs.

[^8-10]: See [@sec-divemt-other] for extra results with a character-level variant of HTER.

![Perceived productivity gains (PPG) between the HT and PE translation modalities, assessed for all subjects after task completion.](../figures/chap-8-divemt/perceived_pe_speedup.pdf){#fig-perceived-pe-speedup width="70%" fig-pos="t"}

### Perception of Productivity Gain

We conclude our analysis by examining the post-task questionnaires, in which participants expressed their perception of MT quality and translation speed across HT and PE modalities (HT$_s$, PE$_s$)[^8-11] using a 1-7 Likert scale (1 slowest, 7 fastest). We use these to compute the Perceived Productivity Gain (PPG) as $\textrm{PPG} = \textrm{PE}_s - \textrm{HT}_s$
and visualize it in [@fig-perceived-pe-speedup]. We observe that Italian and Dutch, the only target languages with marked productivity gains ($\Delta$HT) regardless of the PE system in [@tbl-productivity], are also the only ones having consistently high ($\geq 2$) PPG scores across all subjects.
Moreover, we remark how PPG for target languages with a large gap in $\Delta$HT scores between high-PE$_1$ and low-PE$_2$ (Arabic, Ukrainian) are hardly distinguishable from those of languages in which $\Delta$HT is low for both PE systems (Turkish, Vietnamese). Notably, 4 out of 18 subjects attribute negative PPGs to the PE modality, even though productivity gains were reported across all subjects and languages.
These results suggest that worst-case usage scenarios may play an important role in driving PPG, i.e. that *subjects' perception of quality is largely shaped by particularly challenging or unsatisfying interactions with the NMT system, rather than the average case*.
Finally, from the post-task questionnaire, PPG scores exhibit a strong positive correlation with the perception of MT adequacy ($\rho$=0.66), fluency ($\rho$=0.46) and overall quality ($\rho$=0.69), and more generally with a higher enjoyability of PE ($\rho$=0.60), while being inversely correlated with the perception of problematic mistranslations ($\rho$=$-$0.60).

[^8-11]: We reemphasize that subjects were unaware of the presence of two distinct MT systems.

## Conclusions

In this work we introduced DivEMT, the outcome of a post-editing study spanning two state-of-the-art NMT systems, 18 professional translators and six typologically diverse target languages under a unified setup.

We leveraged DivEMT's behavioral data to perform a controlled cross-language analysis of NMT post-editing effort along its temporal and editing effort dimensions.
The analysis reveals that NMT drives significant improvements in productivity across all the evaluated languages, but the magnitude of these improvements depends heavily on the language and the underlying NMT system.
In this setting, productivity measurements across modalities were found to be generally consistent with the recorded editing patterns. Our results indicate that translators working on language pairs with significant post-editing productivity gains, on average, perform fewer edits and accept more machine-generated translations without any editing.
We also observed a disconnect between post-editing productivity gains and MT quality metrics collected for the same NMT systems. Finally, low source-language relatedness and target morphological complexity seem to hinder productivity when NMT is adopted, even in settings where system architecture and training data are controlled for.

In our qualitative analysis, translators' perception of post-editing usefulness was found to be strongly shaped by problematic mistranslations. Languages showing large productivity gains for both NMT systems were the only ones associated with a positive perception of PE-mediated gains, as opposed to mixed or negative opinions for other translation directions.

In future work, a more fine-grained analysis of the types of edits conducted by the translators, and their differences across languages, could shed more light on our current findings.

## Limitations {#sec-limitations}

The subjective component introduced by the presence of multiple translators is an important confounding factor in our setup, especially due to the relatively small number of subjects for each language. In our study, we tried to balance a thorough control of other noise components with a faithful reproduction of a realistic translation scenario. However, we realize that the combination of limited document context provided by FLORES-101, the variety of topics covered in the texts and the experimental nature of the PET platform constitutes an atypical setting that may have impacted the translators' natural productivity.
Moreover, variability in the content of mBART-50 fine-tuning data, despite the comparable sizes, may have played a role in the variability observed for automatic MT evaluation and PE gains across languages.
