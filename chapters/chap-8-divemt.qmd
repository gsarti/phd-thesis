# Machine Translation Post-editing for Typologically Diverse Languages {#sec-chap-8-divemt}

::: {.callout-note icon="false"}

## Chapter Summary

This chapter begins our investigation into the application of interpretability methods in user-facing translation settings. As an initial step in this direction, we introduce [DivEMT]{.smallcaps}, the first publicly available post-editing dataset spanning six typologically diverse target languages. We evaluate the impact of MT quality and translation directions on post-editing effectiveness in a controlled setup involving 18 professional editors through comprehensive behavioral logging of edits, keystrokes, timing, and pauses. While we find that post-editing machine translation is consistently faster than translation from scratch, our results show significant disparities across languages with different typological relationships to English, even when controlling for system architecture and data size, highlighting the need for tailored approaches in MT for diverse languages.

\vspace{7pt}\noindent

This chapter is adapted from the paper *DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages* [@sarti-etal-2022-divemt].

:::

> *Language was just difference. A thousand different ways of seeing, of moving through the world. No, a thousand worlds within one. And translation, a necessary endeavor however futile, to move between them.*
>
> *-- Rebecca F. Kuang, Babel (2022)*

## Introduction

Recent advances in neural language modeling and multilingual training have led to the widespread adoption of machine translation (MT) technologies across an unprecedented range of languages worldwide. While the benefits of state-of-the-art MT for cross-lingual information access are undisputed [@gene-2021-post], its usefulness as an aid to professional translators varies considerably across domains, subjects and language combinations [@zouhar-etal-2021-neural]. In the last decade, the MT community has been including an increasing number of languages in its automatic and human evaluation efforts [@bojar-etal-2013-findings; @wmt-2021-machine]. However, the results of these evaluations are typically not directly comparable across different language pairs for several reasons. First, reference-based automatic quality metrics are hardly comparable across different target languages [@bugliarello-etal-2020-easier]. Second, human judgments are collected independently for different language pairs, making their cross-lingual comparison vulnerable to confounding factors such as tested domains and training data sizes. Similarly, recent work on NMT post-editing efficiency has focused on specific language pairs such as English-Czech [@zouhar-etal-2021-neural], German-Italian, German-French [@laubli-etal-2019-post] and English-Hindi [@ahsan-etal-2021-assessing]. However, a controlled comparison across a set of typologically diverse languages is still lacking.

![The [DivEMT]{.smallcaps} data collection process. For every English source document, 18 professional translators are tasked with translating it from scratch (HT) or post-editing NMT systems' outputs (PE$_1$/PE$_2$) into six typologically diverse target languages. Behavioral data and qualitative assessments are collected during and after the process, respectively.](../figures/chap-8-divemt/divemt.pdf){#fig-divemt-overview fig-pos="t"}

In this chapter, we conduct an initial assessment of the usefulness of state-of-the-art NMT in professional translation with a strictly controlled cross-language setup ([@fig-divemt-overview]). Specifically, professionals were asked to translate the same English documents into six typologically distinct languages---Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese---using the same platform and guidelines. Three **translation modalities** were adopted: human translation from scratch (HT), post-editing of Google Translate's translation (PE$_1$), and post-editing of mBART-50's translation (PE$_2$), the latter being a state-of-the-art open-source, multilingual NMT system. In addition to post-editing results, subjects' fine-grained editing behavior, including keystrokes and time information, was logged to measure productivity and effort across languages, systems and translation modalities. Finally, translators were asked to complete a qualitative assessment regarding their perceptions of MT quality and post-editing effort. The resulting [DivEMT]{.smallcaps} dataset, to our best knowledge, is the first public resource that allows a direct comparison of professional translators' productivity and fine-grained editing information across a set of typologically diverse languages. All collected data are publicly released^[[`GroNLP/divemt`](https://huggingface.co/datasets/GroNLP/divemt)] alongside this paper to foster further research in the language- and system-dependent nature of NMT advances in real-world translation scenarios.

## Related Work

[Cross-lingual MT Evaluation]{.paragraph} Before the advent of NMT, @birch-etal-2008-predicting studied how various language properties affected the quality of Statistical MT (SMT) across a sizeable sample of European language pairs.
The comparison, however, was solely based on BLEU, which is not directly comparable across different target languages [@bugliarello-etal-2020-easier]. Recent work on neural models introduced more principled ways to measure the intrinsic difficulty of language-modeling [@gerz-etal-2018-relation; @cotterell-etal-2018-languages; @mielke-etal-2019-kind] and machine-translating [@bugliarello-etal-2020-easier; @bisazza-etal-2021-difficulty] different languages. However, reliably achieving this without human evaluation remains an open research question. Concurrently to our research, @licht-etal-2022-consistent proposed a new human evaluation protocol to improve consistency in cross-lingual MT quality assessment.

[Post-editing NMT]{.paragraph} Recent work highlighted the productivity gains driven by NMT post-editing on a broader array of languages that were previously challenging for MT, such as English-Dutch [@daems-etal-2017-translation], English-Hindi [@ahsan-etal-2021-assessing], English-Greek [@stasimioti-sosoni-2020-translation], English-Finnish and English-Swedish [@koponen-etal-2020-mt], all showing a considerable variance among language pairs and subjects. Interestingly, @zouhar-etal-2021-neural found that NMT post-editing speed was comparable to translation from scratch in English-Czech, and highlighted a disconnect between moderate increases in automatic MT quality metrics and improved post-editing productivity. In summary, research on post-editing NMT generally reports increased fluency and output quality; however, productivity gains are hardly generalizable across language pairs and domains. Importantly, to our knowledge, no previous work has studied NMT post-editing over a set of typologically different languages while controlling for the effects of content types and domains, NMT engines, and translation interfaces.

## The [DivEMT]{.smallcaps} Dataset

[DivEMT]{.smallcaps}'s primary purpose is to assess the usefulness of state-of-the-art NMT for professional translators and to study how this usefulness varies across target languages with different typological properties. We present below our data collection setup, which strikes a balance between simulating a realistic professional translation workflow and maximizing the comparability of results across languages.

### Subjects and Task Scheduling

To control for the effect of individual translators' preferences and styles, we involve a total of 18 subjects (three per target language). During the experiment, each subject receives a series of short *documents* (3 to 5 sentences each) where the source text is presented in isolation (HT) or alongside a translation proposal produced by one of the NMT systems (PE$_1$, PE$_2$). The experiment comprises two phases: during the **warm-up phase** a set of 5 documents is translated by all subjects following the same, randomly sampled sequence of modalities (HT, PE$_1$ or PE$_2$). This phase allows the subjects to become accustomed to the setup and enables us to identify potential issues in the logged behavioral data before proceeding.[^8-1] In the **main collection phase**, each subject is asked to translate documents in a pseudo-random sequence of modalities. This time, however, the sequence is different for each translator and chosen so that each document gets translated in all three modalities. This allows us to measure translation productivity independently from the subject's productivity and document-specific difficulties.

[@tbl-schedule] shows an example of the adopted modality scheduling. The modality of document docM$_i$ for translator T$_j$ in the main task is picked randomly among the two modalities that were not seen by the same translator for docM$_{i-1}$, enforcing consecutive documents given to the same translator to be assigned different modalities to avoid periodicity in repetition and enable same-language comparisons. Importantly, although all three modes were collected for every document, we did not enforce mode consistency across the same translator identifier across languages (i.e. T$_1$ for Italian does not have the same sequence of modalities of translator T$_1$ in Arabic, for example). For this reason, individual subjects are not directly comparable across languages. This is relevant since comparable editing behavior should be attributed to similar personal preferences rather than an identical modality assignment of the same sentences. Despite modality scheduling, we have no guarantees that translators consistently follow the order of documents presented in PET, and thus possibly operate on documents assigned to the same modality consecutively. However, this possibility reduces to random guessing due to a lack of any identifying information related to the modality until the document is entered for editing. The sequence of modalities for the warmup task is fixed and is: HT, PE$_2$, PE$_1$, HT, PE$_2$.

{{< include ../tables/chap-8-divemt/_schedule.qmd >}}

As productivity and other behavioral metrics can only be estimated with a sizable sample, we prioritize the number of documents over the number of subjects per language during budget allocation. In future analyses, a larger set of post-edited documents would also provide more insight into the error type distribution of NMT systems across different language pairs.

All subjects are professional translators with at least 3 years of professional experience, including at least 1 year of post-editing experience, and strong proficiency in CAT tools.[^8-2] Translators were provided with links to the source articles to facilitate contextualization, were asked to produce translations of publishable quality and were instructed not to use any external MT engine to produce their translations. Assessing the final quality of the post-edited material is out of the scope of the current study, although we realize that this is an important consideration to assess usability in a professional context.^[A summary of our translation guidelines is provided in [@sec-divemt-guidelines].]

[^8-1]: Warm-up data are excluded from the analysis of [@sec-pe_effort].
[^8-2]: Additional subjects' details are available in [@sec-divemt-subject-info].

### Choice of Source Texts

The selected documents represent a subset of the FLORES-101 benchmark [@goyal-etal-2022-flores] consisting of sentences taken from English Wikipedia, and covering a mix of topics and domains.[^8-3] While professional translators generally specialize in one or a few domains, we opt for a mixed-domain dataset to minimize domain adaptation efforts by the subjects and maximize the generalizability of our results. Importantly, FLORES-101 includes high-quality human translations into 101 languages, which enables the automatic estimation of NMT quality and the discarding of excessively low-scoring models or language pairs before our experiment. FLORES-101 also provides valuable metadata, e.g. source URL, which allows us to ensure the absence of public translations of the selected contents, which could be leveraged by translators and compromise the validity of our setup. The documents used for our study are fragments of contiguous sentences extracted from Wikipedia articles that compose the original FLORES-101 corpus. Even if small, the context provided by document structure allows us to simulate a more realistic translation workflow if compared to out-of-context sentences.

Based on our available budget, we selected 112 English documents from the *devtest* portion of FLORES-101, corresponding to 450 sentences and 9,626 words. More details on the data selection process are provided in [@sec-divemt-doc-select].

[^8-3]: We use a balanced sample of articles sourced from WikiNews, WikiVoyage and WikiBooks.

{{< include ../tables/chap-8-divemt/_languages-small.qmd >}}

### Choice of Languages {#sec-choice-languages}

Training data is one of the most important factors in determining the quality of an NMT system.
Unfortunately, using strictly comparable or multi-parallel datasets, such as Europarl [@koehn-2005-europarl] or the Bible corpus [@mayer-cysouw-2014-creating], would dramatically restrict the diversity of languages available to our study or imply prohibitively low translation quality on general-domain text.
In order to minimize the effect of training data disparity while maximizing language diversity, we choose representatives of six different language families for which comparable amounts of training data are available in our open-source model, namely **Arabic**, **Dutch**, **Italian**, **Turkish**, **Ukrainian**, and **Vietnamese**. As shown in [@tbl-languages-small], our language sample exhibits a good diversity in terms of language family, relatedness to English, type of morphological system, morphological complexity, measured by the mean size of paradigm [MSP, @xanthos-etal-2011-role], and script. We also report the type-token ratio (TTR), the only language property found to correlate significantly with translation difficulty in a sample of European languages [@bugliarello-etal-2020-easier].
While the amount of language-specific parallel sentence pairs used for the multilingual fine-tuning of mBART-50 varies widely (4K $< N <$ 45M), all our selected language pairs fall within the 100K-250K range (mid-resourced, see [@tbl-flores-perf]), enabling a fair cross-lingual performance comparison.

{{< include ../tables/chap-8-divemt/_flores-perf.qmd >}}

### Choice of MT Systems {#sec-choice-systems}

While most of the best-performing general-domain NMT systems are commercial, experiments based on such systems are not replicable, as their backends are silently updated over time. Moreover, without knowing the exact training specifics, we cannot attribute differences in the cross-lingual results to intrinsic language properties.
We balance these observations by including two NMT systems in our study: **Google Translate** (GTrans)[^8-4] as a representative of commercial quality, and **mBART-50 one-to-Many**[^8-5] [@tang-etal-2021-multilingual] as a representative of state-of-the-art open-source multilingual NMT technology. The original multilingual BART model [@liu-etal-2020-multilingual-denoising] is an encoder-decoder transformer model pre-trained on monolingual documents in 25 languages. @tang-etal-2021-multilingual extend mBART by further pre-training on 25 new languages and performing *multilingual translation fine-tuning* for the full set of 50 languages, producing three configurations of multilingual NMT models: many-to-one, one-to-many, and many-to-many.
Our choice of mBART-50 is primarily motivated by its manageable size, good performance across the set of evaluated languages (see [@tbl-flores-perf]), and its adoption for other NMT studies [@liu-etal-2021-continual] and post-editing evaluations [@fomicheva-etal-2022-mlqe].
Although mBART-50 performances are usually comparable or slightly worse than those of tested bilingual NMT models,[^8-6] using a multilingual model allows us to evaluate the downstream effectiveness of a single, unified system trained on pairs evenly distributed across tested languages. Finally, adopting two systems with marked differences in automatic evaluation scores allows us to estimate how a significant increase in metrics such as BLEU, ChrF and [comet]{.smallcaps} [@papineni-etal-2002-bleu; @popovic-2015-chrf; @rei-etal-2020-comet] impacts downstream productivity across languages in a realistic post-editing scenario.

[^8-4]: Evaluation performed in October 2021.
[^8-5]: [`mbart-large-50-one-to-many`](https://huggingface.co/facebook/mbart-large-50-one-to-many)
[^8-6]: See [@sec-divemt-other] for automatic MT quality results by five different models over a larger set of 10 target languages.

### Translation Platform and Collected Data

Translators were asked to use PET [@aziz-etal-2012-pet], a computer-assisted translation tool that supports both translating from scratch and post-editing.
This tool was chosen because (i) it logs information about the post-editing process, which we use to assess effort (see [@sec-pe_effort]); and (ii) it is a mature research-oriented tool that has been successfully used in several previous studies [@koponen2012post; @toral-etal-2018-postediting], and we modify it slightly to support right-to-left languages like Arabic.
Using PET, we collect three types of data:

- **Resulting translations** produced by translators in either HT or PE modes, constituting a multilingual corpus with one source text and 18 translations (one per language-modality combination) exemplified in [@tbl-data_example_small].
- **Behavioral data** for translated sentences, including editing time, amount and type of keystrokes (content, navigation, erase, etc.), and number and duration of pauses above 300/1000 milliseconds [@lacruz-etal-2014-cognitive].
- **Pre- and post-task questionnaire**. The former focuses on demographics, education, and work experience with translation and post-editing. The latter elicits subjective assessments of post-editing quality, effort and enjoyability compared to translating from scratch.

{{< include ../tables/chap-8-divemt/_data_example_small.qmd >}}

## Post-Editing Effort Across Languages {#sec-pe_effort}

In this section, we use the [DivEMT]{.smallcaps} dataset to quantify the post-editing effort of professional translators across our diverse set of target languages. We consider two main objective indicators of editing effort: *temporal measurements* (and related productivity gains) and *post-editing rates*, measured by the Human-targeted Translation Edit Rate (HTER, @snover-etal-2006-study). Finally, we assess the subjective perception of PE gains by examining the post-task questionnaires. We reiterate that all scores in this section are computed on the same set of source sentences for all languages, resulting in a faithful cross-lingual comparison of post-editing effort thanks to [DivEMT]{.smallcaps}'s controlled setup.

### Temporal Effort and Productivity Gains

We begin by comparing *task time* (seconds per processed source word) across languages and modalities. For this purpose, edit times are computed for every document in every language without considering the presence of multiple translators for every language. As shown in [@fig-time-per-src-word], translation time varies considerably across languages even when no MT system is involved (HT), suggesting an intrinsic variability in translation complexity for different subjects and language pairs.
Indeed, for the HT modality, the time required for the "slowest" target languages (Italian, Ukrainian) is roughly twice that of the "fastest" one (Turkish). This pattern cannot be easily explained and contrasts with factors commonly tied to MT complexity, such as source-target morphological richness and language relatedness [@birch-etal-2008-predicting; @belinkov-etal-2017-neural]. On the other hand, we find that the relation PE$_1$ < PE$_2$ < HT (where PE$_1$ is the fastest, PE$_2$ has a medium speed, and HT is the slowest) holds for all the evaluated languages.

![Temporal effort across languages and translation modalities, measured in seconds per processed source word. Each point represents a document, with higher scores denoting slower editing. $\uparrow$: amount of data points per language not shown in the plot.](../figures/chap-8-divemt/time_per_src_word_small.pdf){#fig-time-per-src-word width="65%"}

For a measure of productivity gains that is easier to interpret and more in line with translation industry practices, we turn to *productivity* expressed in source words processed per minute and compute the *speed-up* induced by the two post-editing modalities over translating from scratch ($\Delta$HT). [@tbl-productivity] presents our results.
**Across systems**, we find that *large* differences among automatic MT quality metrics indeed reflect post-editing effort, suggesting a nuanced picture that complements the findings of @zouhar-etal-2021-neural. While post-editing time gains were observed to quickly saturate for slight changes in high-quality MT, we find that moving from medium-quality to high-quality MT yields meaningful productivity improvements across most evaluated languages.
**Across languages**, too, the magnitude of productivity gains ranges widely, from doubling in some languages (Dutch PE$_1$, Italian PE$_1$ and PE$_2$) to only about 10% (Arabic, Turkish and Ukrainian PE$_2$).
When only considering the better-performing system (PE$_1$), post-editing remains clearly beneficial in all languages despite the high variability in $\Delta$HT scores.
Results are more nuanced for the open-source system (PE$_2$), with three out of six languages displaying only marginal gains (<15% in Arabic, Turkish and Ukrainian).
Despite its overall inferior performance, mBART-50 (PE$_2$) is the only system that enables a fair comparison across languages (in terms of training data size and architecture, see [@sec-choice-systems]). Interestingly, when focusing on the productivity gains achieved by this system, factors such as language relatedness and morphological complexity become relevant. Specifically, Italian (+95%), Dutch (+61%) and Ukrainian (+14%) are genetically and syntactically related to English, but Ukrainian has a richer morphology (see [@tbl-languages-small]).
On the other hand, Vietnamese (+23%), Turkish (+12%) and Arabic (+10%) all belong to different families. However, Vietnamese is isolating (little to no morphology), while Turkish and Arabic have rich morphological systems (respectively agglutinative and introflexive, the latter of which is especially problematic for subword segmentation, @amrhein-sennrich-2021-suitable-subword).
Other differences, however, are more difficult to explain. For instance, Dutch is closely related to English and has a simpler morphology than Italian, but its productivity gain with mBART-50 is lower (61% vs 95%). This finding is accompanied by an important gap in BLEU and [comet]{.smallcaps} scores achieved by mBART-50 on the two languages (22.6 vs 24.4 BLEU and 0.532 vs 0.648 [comet]{.smallcaps} for Dutch vs Italian, resp.), which cannot be explained by training data size.

{{< include ../tables/chap-8-divemt/_productivity.qmd >}}

In summary, our findings confirm the overall positive impact of NMT post-editing on translation productivity observed in previous PE studies. However, we note that *the magnitude of this impact is highly variable across systems and languages*, with inter-subject variability also playing an important role, in line with previous studies [@koponen-etal-2020-mt] (see [@sec-limitations] for more details).
The small size of our language sample does not allow us to draw direct causal links between specific typological properties and post-editing efficiency.
That said, we believe these results have important implications for the claimed `universality' of current state-of-the-art MT and NLP systems, primarily based on the transformer architecture [@vaswani-etal-2017-attention] and BPE-style subword segmentation techniques [@sennrich-etal-2016-neural].

#### Modeling Temporal Effort

Given the high variability among translators, segments and translation modalities, we assess the validity of our observations via statistical analysis of temporal effort using a linear mixed-effects regression model (LMER, @lindstrom-bates-1988-lmer), following @green-etal-2013-efficacy and @toral-etal-2018-postediting. Linear Mixed Effects models (LMER) are used for regression analyses involving dependent data, such as longitudinal studies with multiple observations per subject. We fit our model on $n=7434$ instances, corresponding to 413 sentences translated by 18 translators, using translation time as the dependent variable, and translation modality, target language, their interaction and length of source segment in characters as fixed predictors:

```python
edit_time ~ src_len_chr + lang_id * task_type +
(1|subject_id) +
(1 | document_id/item_id) +
(0 + task_type | document_id/item_id)
```

We log-transform the dependent variable, edit time in seconds, given its long right tail. The models are built by adding one element at a time and checking whether such an addition leads to a significantly better model, as indicated by a reduction in AIC (i.e., a decrease of at least 2). Our random effects structure includes random intercepts for different segments (nested with documents) and translators, as well as a random slope for modality over individual segments. We start with an initial model that includes only the two random intercepts (by-translator and by-segment) and proceed by (i) finding significance for nested document/segment random effect; (ii) adding fixed predictors one by one; (iii) adding interactions between fixed predictors; and (iv) adding the random slopes.[^8-8]

[@tbl-significance] presents the set of predictors included in the final model, along with an estimate of their impact on edit times and their corresponding significance. We find that both PE modalities significantly reduce translation times ($p < 0.001$), with PE$_1$ being significantly faster than PE$_2$ ($p < 0.001$) across all languages. Considering Ukrainian---the language for which HT is slowest---as the reference level, the reduction in time brought by Google is significantly more pronounced for Italian, Dutch ($p<0.001$), and Turkish ($p<0.05$). For mBART-50, however, we only observe significantly more pronounced increases in productivity for Italian and Dutch ($p<0.001$) compared to the reference. We find these results to corroborate the observations of the previous section.

{{< include ../tables/chap-8-divemt/_significance.qmd >}}

### Post-Editing Rate {#sec-pe-rate}

We proceed to study the post-editing patterns using the widely adopted Human-targeted Translation Edit Rate (HTER, @snover-etal-2006-study), which is computed as the length-normalized sum of word-level substitutions, insertions, deletions, and shift operations performed during post-editing.

As shown in [@fig-hter], PE$_1$ required less editing than PE$_2$ for all languages, and a high variability is observed across the two systems and all languages. Because translators were not informed about the presence of two MT systems, we exclude the possibility that these results reflect an over-reliance or distrust towards a specific MT system.
For Google Translate, Ukrainian shows the heaviest edit rate, followed by Vietnamese, whereas Arabic, Dutch, Italian and Turkish all show relatively low amounts of edits.
Focusing again on mBART-50 for a more fair cross-lingual comparison, Ukrainian is by far the most heavily edited language, followed by a medium-tier group composed of Vietnamese, Arabic and Turkish, and finally by Dutch and Italian as low-edit languages. Results show that several of our observations on linguistic relatedness and morphology type also apply to edit rates, with languages less related to English or having richer morphology requiring more post-edits on average.

::: {.content-visible when-format="html" layout-ncol=2 fig-pos="t"}
![Human-targeted Translation Edit Rate (HTER) for Google Translate and mBART-50 post-editing across available languages.](../figures/chap-8-divemt/hter_per_system.pdf){#fig-hter}

![Distribution of error-less machine translation sentence outputs (no edits performed during post-editing) for each translator and every language.](../figures/chap-8-divemt/errorless_outputs_small.pdf){#fig-errorless-sentences}
:::

::: {.content-visible when-format="pdf" layout-ncol=1 fig-pos="t"}
![Human-targeted Translation Edit Rate (HTER) for Google Translate and mBART-50 post-editing across available languages.](../figures/chap-8-divemt/hter_per_system.pdf){#fig-hter width="65%"}

![Distribution of error-less machine translation sentence outputs (no edits performed during post-editing) for each translator and every language.](../figures/chap-8-divemt/errorless_outputs_small.pdf){#fig-errorless-sentences width="65%"}
:::

[@fig-errorless-sentences] visualizes the large gap in edit rates across languages and subjects by presenting the amount of 'errorless' MT sentences that were accepted directly, i.e. without any post-editing. We note again how the NMT system significantly influences the rate of occurrence of such sentences, yet nonetheless shows that Dutch and Italian generally present more error-free sentences than Ukrainian and Vietnamese. In particular, for Google Translate outputs, the average rate of error-free sentences is roughly 25% for the former target languages, while for the latter, it accounts for only 3% of total translations. Surprisingly, the English-Turkish pair also fares well, despite the low relatedness between the source and target languages. We note that post-editing effort appears to correlate poorly with the automatic MT quality metrics reported in [@tbl-flores-perf] (e.g., see the high scores of Vietnamese and the low scores of Dutch PE$_1$), highlighting a difficulty in predicting the benefits of MT post-editing over HT for new language pairs.

While HTER is a standard metric adopted in both academic and industrial settings, we also evaluated its character-level variant, CharacTER [@wang-etal-2016-character], to assess whether it could better account for the editing process of morphologically rich languages. [@fig-character] presents the CharacTER results. When comparing this plot to the HTER one (@fig-hter), we notice that CharacTER preserves the overall trends but slightly improves the edit rate for Arabic and Turkish compared to other languages. Nevertheless, we find that HTER correlates slightly better with productivity scores across all tested languages, both at the sentence and document levels.

![Character-level Human-targeted Translation Edit Rate (CharacTER) for Google Translate and mBART-50 post-editing across available languages.](../figures/chap-8-divemt/cer_per_system.pdf){#fig-character width="60%" fig-pos="t"}

### Perception of Productivity Gain

We conclude our analysis by examining the post-task questionnaires, in which participants expressed their perceptions of MT quality and translation speed across HT and PE modalities (HT$_s$, PE$_s$)[^8-11] using a 1-7 Likert scale (where 1 is the slowest and 7 is the fastest). We use these to compute the Perceived Productivity Gain (PPG) as $\textrm{PPG} = \textrm{PE}_s - \textrm{HT}_s$ and visualize it in [@fig-perceived-pe-speedup]. We observe that Italian and Dutch, the only target languages with marked productivity gains ($\Delta$HT) regardless of the PE system in [@tbl-productivity], are also the only ones having consistently high ($\geq 2$) PPG scores across all subjects. Moreover, we remark how PPG for target languages with a wide gap in $\Delta$HT scores between high-PE$_1$ and low-PE$_2$ (Arabic, Ukrainian) are hardly distinguishable from those of languages in which $\Delta$HT is low for both PE systems (Turkish, Vietnamese). Notably, 4 out of 18 subjects attribute negative PPGs to the PE modality, despite productivity gains being reported across all subjects and languages. These results suggest that worst-case usage scenarios may play an important role in driving PPG, i.e. that *subjects' perception of quality is shaped mainly by particularly challenging or unsatisfying interactions with the NMT system, rather than the average case*. Finally, from the post-task questionnaire, PPG scores exhibit a strong positive correlation with the perception of MT adequacy ($\rho$=0.66), fluency ($\rho$=0.46) and overall quality ($\rho$=0.69), and more generally with a higher enjoyability of PE ($\rho$=0.60), while being inversely correlated with the perception of problematic mistranslations ($\rho$=-0.60).

![Perceived productivity gains (PPG) between the HT and PE translation modalities, assessed for all subjects after task completion.](../figures/chap-8-divemt/perceived_pe_speedup.pdf){#fig-perceived-pe-speedup width="70%" fig-pos="t"}

[^8-11]: We reemphasize that subjects were unaware of the presence of two distinct MT systems.

## Limitations {#sec-limitations}

The subjective component introduced by the presence of multiple translators is an important confounding factor in our setup, particularly given the relatively small number of subjects for each language. In our study, we aimed to strike a balance between thorough control of other noise components and faithful reproduction of a realistic translation scenario. However, we recognize that the combination of the limited document context provided by FLORES-101, the variety of topics covered in the texts, and the experimental nature of the PET platform constitutes an atypical setting that may have impacted the translators' natural productivity. Moreover, variability in the content of mBART-50 fine-tuning data, despite their comparable sizes, may have played a role in the observed variability in automatic MT evaluation and PE gains across languages.

## Conclusions

We introduced [DivEMT]{.smallcaps}, the outcome of a post-editing study that spanned two state-of-the-art NMT systems, involved 18 professional translators, and employed six typologically diverse target languages under a unified setup. We leveraged [DivEMT]{.smallcaps}'s behavioral data to perform a controlled cross-language analysis of NMT post-editing effort along its temporal and editing effort dimensions. The analysis reveals that NMT drives significant improvements in productivity across all evaluated languages; however, the magnitude of these improvements depends heavily on the language and the underlying NMT system. In this setting, productivity measurements across modalities were found to be generally consistent with the recorded editing patterns. Our results indicate that translators working on language pairs with significant post-editing productivity gains, on average, perform fewer edits and accept more machine-generated translations without any editing. We have also observed a disconnect between post-editing productivity gains and MT quality metrics collected for the same NMT systems. Finally, low source-language relatedness and target morphological complexity seem to hinder productivity when NMT is adopted, even in settings where system architecture and training data are controlled for.

In our qualitative analysis, translators' perception of post-editing usefulness was found to be strongly shaped by problematic mistranslations. Languages showing large productivity gains for both NMT systems were the only ones associated with a positive perception of PE-mediated gains, as opposed to mixed or negative opinions for other translation directions.

Overall, our findings reveal significant variation in post-editing effectiveness across languages and systems, highlighting the need for fine-grained quality assessment tools. In the next chapter, we build upon these insights by conducting a second study with professional post-editors, assessing the impact of word-level error detection methods---including unsupervised approaches that leverage model internals---on the quality and productivity of human post-editing.