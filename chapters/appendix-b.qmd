# Conditioning Generation for Personalized Machine Translation {#sec-appendix-b}

## Attribute-Controlled Translation using Retrieval and Marking {#sec-ramp-appendix}

### Prompt Templates {#sec-ramp-prompt-templates}

**Formality-Controlled Translation** *Here is a sentence: {`src`} Here is its `lang` translation written in a `attr` style: {`tgt`} The translated sentence conveys a `attr` style by using words such as '`w1`', '`w2`'.

**Gender-Controlled Translation** Here is a sentence: {`src`} Here is its `lang` translation in which the person is `attr`: {`tgt`} In the translation, the `attr` gender of the person is made explicit by words such as '`w1`', '`w2`'.

### Full Per-language Results {#sec-ramp-full-scores}

- @tbl-chap6-preliminary-cocoa-results: Detailed scores of same-language prompting on [CoCoA-MT]{.smallcaps} (preliminary evaluation).[^b-4]
- @tbl-chap6-same-language-cocoa-details: Full results of same-language prompting on [CoCoA-MT]{.smallcaps} (full evaluation).
- @tbl-chap6-same-language-geneval-details: Full results of same-language prompting on [MT-GenEval]{.smallcaps} (full evaluation).
- @tbl-chap6-cross-lingual-cocoa-details: Full results of cross-lingual prompting on [CoCoA-MT]{.smallcaps}.
- @tbl-chap6-cross-lingual-geneval-details: Full results of cross-lingual prompting on [MT-GenEval]{.smallcaps}.

[^b-4]: We set maximum output length as 50 tokens in the preliminary evaluation, while we use 100 tokens in the main evaluation. Early truncating leads to slightly lower scores in @tbl-chap6-preliminary-cocoa-results than in @tbl-chap6-full-results.

{{< include ../tables/chap-6-ramp/_preliminary-cocoa-results.qmd >}}

{{< include ../tables/chap-6-ramp/_same-language-cocoa-details.qmd >}}

{{< include ../tables/chap-6-ramp/_same-language-geneval-details.qmd >}}

{{< include ../tables/chap-6-ramp/_cross-lingual-cocoa-details.qmd >}}

{{< include ../tables/chap-6-ramp/_cross-lingual-geneval-details.qmd >}}

### Error Analysis of Cross-Lingual Prompting {#sec-ramp-analysis-zeroshot}

@tbl-chap6-cross-lingual-analysis shows two examples where [Ramp]{.smallcaps} performs significantly worse than the base model in terms of [comet]{.smallcaps}. 
In the first example, having multiple in-context examples containing *"million"* led the model to mis-translate *"billion"* to *"million"*.
In the second example, we observe that the color related in-context examples led the model to produce hallucinated output about clothing colors.

Repeated misleading in-context examples are less observed on [MT-GenEval]{.smallcaps} and in the same-language setting because (1) [CoCoA-MT]{.smallcaps} translates the same set of English sentences to different languages while [MT-GenEval]{.smallcaps} collects English sentences independently; (2) There are no duplicated source (English) sentences for each language. (Therefore, if [Ramp]{.smallcaps} retrieves duplicated English sentences as in @tbl-chap6-cross-lingual-analysis, their reference translations are guaranteed to be in different languages.)

{{< include ../tables/chap-6-ramp/_cross-lingual-analysis.qmd >}}

{{< pagebreak >}}
\FloatBarrier

## Steering Language Models for Machine Translation Personalization {#sec-sae-litmt-appendix}

### Experiments Reproducibility {#sec-sae-litmt-reproducibility}

In this section, we provide every parameter we use for the reproducibility of our experiments setups.

#### Base Prompt {#sec-sae-litmt-base-prompt}

We use the same prompt template across all methods: ZS (which corresponds to the original model translation), ZS-Exp.$_\text{HT}$, ZS-Exp.$_\text{PT}$ (detailed in @sec-sae-litmt-explain), MS, ActAdd, ReFT, and SAE-based contrastive setups. This prompt, shown in @lst-chap7-explain-final, instructs the model to translate the source sentence while explicitly preventing it from adding any explanations about the translation process. Since all test models are Instruction Tuned, we utilize their native chat templates to preprocess the input accordingly. For multi-shot examples, the *user* and *assistant* turns are repeated for each example, always using the same prompt structure.

#### ReFT Training {#sec-sae-litmt-reft-training}

ReFT training was conducted using the PyReFT toolkit from the original authors^[<https://github.com/stanfordnlp/pyreft>]. We applied the intervention at the same hook point used by other steering methods - specifically, the layer output corresponding to the residual stream at the selected layer. The training configuration includes a `low_rank_dimension` of 4, `lora_alpha` set to 32, and a `lora_dropout` of 0.05. ReFT was trained on the same 20 prompts used in the MS setup, for a total of 100 epochs.

#### SAE Cont.$_\text{HT}$ and SAE Cont.$_\text{PT}$ {#sec-sae-litmt-cont-methods}

We use the NNsight library [@fiottokaufman-etal-2024-nnsight] to extract and manipulate model activations for all steering experiments. The source code is publicly available in the repository linked in the main body of this paper. For consistency, we use the same set of contrastive examples employed in the MS approach.

#### ZS-Exp.$_\text{HT}$ and ZS-Exp.$_\text{PT}$ {#sec-sae-litmt-explain}

For both the ZS-Exp.$_\text{HT}$ and ZS-Exp.$_\text{PT}$ setups, we used GPT-4o (June 2025) to generate explanations detailing the stylistic differences between a base translation and a target human translation. The prompt template used for this task is shown in @lst-chap7-explain-template, using the same 20 examples as in the MS, SAE Cont.$_\text{HT}$, and SAE Cont.$_\text{PT}$ setups.

All outputs were manually inspected to ensure no verbatim excerpts from the provided examples were present, avoiding any risk of data leakage. Example outputs for different novels are shown in @lst-chap7-explain-example.

Finally, these generated guidelines are used to prompt the evaluated models, following the template shown in @lst-chap7-explain-final.

::: {#lst-chap7-explain-template}

```markdown
Objective - Identify stylistic choices in translations for personalization purposes.

You will be provided with a source text, a standard translation, and a target translation by a specific translator whose style we want to emulate.
Your task is to analyze the 'Target translation' by comparing it to the 'Base translation' and the 'Source text'.
Identify and list the distinctive stylistic patterns, choices, and preferences exhibited in the Target translation.

These stylistic cues should help another translator (or an AI) to adapt their translations to match the style of the target translator.

Source text: ____
Base translation: ____
Target translation: ____

[... Repeat Source, Base and Target]

Please extract a concise list of key stylistic cues. Focus on aspects such as vocabulary choices, sentence structure, tone and register, handling of cultural nuances, punctuation/formatting preferences and overall creativity.

Output a short list of stylistic cues as bullet points. Write the list as if you were directly giving the guidelines to the translator and avoid using specific examples.
```

Prompt template used to get GPT 4o explanation using translation examples.
:::

::: {#lst-chap7-explain-example}

```markdown
[When comparing H1 and MT for Beware of Pity (German)]
- Maintain a tone that is professional, thoughtful, and subtly persuasive.
- Avoid overly technical jargon unless necessary; explain specialized terms briefly if used.
- Preserve the author's voice, keeping a balance between academic rigor and narrative engagement.
- Ensure smooth transitions between sentences and paragraphs to support coherent argumentation.
- Translate idiomatic expressions in a way that retains their intended effect, even if the wording differs.

[When comparing H1 and H2 for Beware of Pity (German)]
- Keep the tone warm, welcoming, and direct - avoid overly formal or distant language.
- Use active voice wherever possible to maintain energy and engagement.
- Avoid idioms or expressions that may not translate culturally; aim for universal accessibility.
- Maintain consistent tone and register throughout, adapting to the intended audience's familiarity with the subject.
- Respect the rhythm and structure of the original, but feel free to adjust sentence length for readability.

[When comparing H1 and H2 for Crime and Punishment (Russian)]
- Prefer dynamic over formal vocabulary: Opt for vivid or emotionally charged words when available.
- Add tonal nuance and emotional shading: Enrich dialogues and narration with subtle shifts in tone, especially sarcasm, understatement, or irony, to match character voice or mood.
- Use contractions and familiar phrasing: Employ contractions and relaxed expressions to preserve spoken character.
- Expand or rephrase for clarity and voice: Don't hesitate to slightly reword or elaborate if it strengthens tone, clarifies intent, or enhances character differentiation.
- Favor rhythmic, flowing sentence structure: Break long, formal sentences into multiple shorter clauses or use punctuation (dashes, ellipses) for dramatic or emotional effect.
[...]
```

Examples of explanation obtained from GPT 4o when comparing different translations from different novels.
:::

::: {#lst-chap7-explain-final}

```markdown
Translate the following sentence between the angular parentheses into English.

[if setup == ZS-Exp]
[Follow the following guidelines when translating: <explanations here>]

The original sentence is: ____.

Remember to write only the translation, without any additional text or explanation.
```

Zero shot template template when prompting language models with different setups
:::

### All Models Results {#sec-sae-litmt-all-models}

#### Full Prompting and Steering Results {#sec-sae-litmt-steering-results}

We present detailed plots of the results for each novel across the three evaluated models in @fig-chap7-gemma2b-all (Gemma 2 2B), @fig-chap7-gemma9b-all (Gemma 2 9B), and @fig-chap7-llama8b-all (Llama 3.1 8B). These plots display the performance of all evaluated methods, reporting the three submetrics: **H** accuracy (general human-likeness), **P** accuracy (translator-specific accuracy), and **P$_\text{flip}$** (personalized flip accuracy), alongside the corresponding [comet]{.smallcaps} scores measuring translation quality.

![Results for every language on Gemma 2 2B.](../figures/chap-7-sae-litmt/app_gemma-2-2b-it_novels_avg.pdf){#fig-chap7-gemma2b-all fig-pos="t"}

![Results for every language on Gemma 2 9B.](../figures/chap-7-sae-litmt/app_gemma-2-9b-it_novels_avg.pdf){#fig-chap7-gemma9b-all fig-pos="t"}

![Results for every language on Llama 3.1 8B.](../figures/chap-7-sae-litmt/app_llama-3.1-8b-it_novels_avg.pdf){#fig-chap7-llama8b-all fig-pos="t"}

### Dataset and Generation Examples {#sec-sae-litmt-output-examples}

We present in @tbl-chap7-examples-all-zh and @tbl-chap7-examples-all-it a selection of examples from two different languages, showcasing outputs from each of the tested setups. For each example, we also report the corresponding classification label predicted by the classifier and the associated [comet]{.smallcaps} score. Additionally 

{{< include ../tables/chap-7-sae-litmt/_examples-all-zh.qmd >}}

{{< include ../tables/chap-7-sae-litmt/_examples-all-it.qmd >}}

{{< pagebreak >}}