# Context Usage in Multilingual NLP {#sec-appendix-a}

## Attributing Language Model Generations with the Inseq Toolkit {#sec-inseq-appendix}

### Additional Details on Turkish Gender Bias Case Study {#sec-inseq-appendix-turkish-gender-bias}

[@tbl-turkish-wordlist] shows the list of occupation terms used in the gender bias case study ([@sec-chap3-gender-bias]).
We correlate the ranking of occupations based on the selected attribution metrics and probabilities with U.S. labor statistics^[[`https://github.com/rudinger/winogender-schemas`](https://github.com/rudinger/winogender-schemas/blob/master/data/occupations-stats.tsv) (`bls_pct_female` column)]. Table [@tbl-m2m-gender-example] example was taken from the BUG dataset [@levy-etal-2021-collecting-large].

{{< include ../tables/chap-3-inseq/_turkish-wordlist.qmd >}}

### Example of Pair Aggregation for Contrastive MT Comparison {#sec-inseq-appendix-pair-agg-gender-swap}

An example of gender translation pair using the synthetic template of [@sec-chap3-gender-bias] is show in [@fig-ex-gender-pair-agg], highlighting a large drop in probability when switching the gendered pronoun for highly gender-stereotypical professions, similar to [@tbl-turkish-gender-bias] results.

::: {#fig-ex-gender-pair-agg}
```python
import inseq
from inseq.data.aggregator import *

# Load the TR-EN translation model and attach the IG method
model = inseq.load_model(
    "Helsinki-NLP/opus-mt-tr-en", "integrated_gradients"
)

# Forced decoding. Return probabilities, no target attr.
out = model.attribute(
    ["O bir teknisyen", "O bir teknisyen"],
    ["She is a technician.","He is a technician."],
    step_scores=["probability"],
)

# Aggregation pipeline composed by two steps:
# 1. Aggregate subword tokens across all dimensions:
# 2. Aggregate hidden size to produce token-level attributions
subw_aggregator = AggregatorPipeline(
    [SubwordAggregator, SequenceAttributionAggregator]
)
masculine = out[0].aggregate(aggregator=subw_aggregator)
feminine = out[1].aggregate(aggregator=subw_aggregator)

# Take the diff of the scores of the two attributions
masculine.show(aggregator=PairAggregator, paired_attr=feminine)
```

![](../figures/chap-3-inseq/tr_en_pair_aggr_example.png){width=60% fig-align="center"}

Comparing attributions for a synthetic Turkish-to-English translation example with underspecified source pronoun gender using a MarianMT Turkish-to-English translation model [@tiedemann-2020-tatoeba]. Values in the visualized attribution matrix show a 46% higher probability of producing the masculine pronoun in the translation and a relative decrease of 18.4% in the importance of the Turkish occupation term compared to the feminine pronoun case.
:::

### Example of Quantized Contrastive Attribution of Factual Knowledge {#sec-inseq-appendix-factual-knowledge}

[@fig-ex-cat-counterfact-code] presents code used in [@sec-chap3-rome-repro] case study, with visualized attribution scores for contrastive examples presented in [@fig-ex-cat-counterfact-plots].

:::{#fig-ex-cat-counterfact-code}
```python
import inseq
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# The model is loaded in 8-bit on available GPUs
model = AutoModelForCausalLM.from_pretrained(
    "gpt2-xl", load_in_8bit=True, device_map="auto"
)
# Counterfact datasets used by Meng et al. (2022)
data = load_dataset("NeelNanda/counterfact-tracing")["train"]

# GPT-2 XL is a Transformer model with 48 layers
for layer in range(48):
    attrib_model = inseq.load_model(
        model,
        "layer_gradient_x_activation",
        tokenizer="gpt2-xl",
        target_layer=model.transformer.h[layer].mlp,
    )
    for i, ex in data:
        # e.g. "The capital of Second Spanish Republic is"
        # -> Madrid (true) / Paris (false)
        prompt = ex["relation"].format(ex["subject"])
        true_answer = prompt + ex["target_true"]
        false_answer = prompt + ex["target_false"] 
        # Contrastive attribution of true vs false answer
        out = attrib_model.attribute(
            prompt,
            true_answer,
            attributed_fn="contrast_prob_diff",
            contrast_targets=false_answer,
            show_progress=False,
        )
```

Example code to contrastively attribute factual statements from the Counterfact Tracing dataset, using Layer Gradient $\times$ Activation to compute importance scores until intermediate layers of the GPT2-XL model.
:::

::: {#fig-ex-cat-counterfact-plots layout-ncol="2"}

![](../figures/chap-3-inseq/cat_example_1.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_2.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_3.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_4.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_5.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_6.png){width=50% fig-align="center"}

Visualization of contrastive attribution scores on a subset of layers (23 to 48) for some selected dataset examples. Plot labels show the contrastive pairs of false $\rightarrow$ true answer used as attribution targets.
:::

{{< pagebreak >}}
\FloatBarrier

## Quantifying Context Usage in Neural Machine Translation {#sec-pecore-appendix}

```pseudocode
#| label: alg-pecore
#| pdf-placement: "ht!"

\begin{algorithm}
\caption{PECoRe cue-target extraction process}
\begin{algorithmic}
\Require $C, x$ (Input context and current sequences), $\theta$ (Model parameters), $s_{\text{cti}}, s_{\text{cci}}$ (Selector functions), $\mathcal{M}$ (Contrastive metrics), $f_\text{att}$ (Contrastive attribution method), $f_\text{tgt}$ (Contrastive attribution target function)
\Procedure{PECoRe}{$C, x, \theta, s_\text{cti}, s_\text{cci}, \mathcal{M}, f_\text{att}, f_\text{tgt}$}
  \State $\hat y = \textnormal{generate(}C, x, \theta$) using any decoding strategy and parameters
  \State $\mathcal{T} = \textnormal{CTI(}C, x, \hat y, \theta, \mathcal{M}, s_\text{cti}\textnormal{)}$
  \ForAll{$t \in \mathcal{T}$}
    \State $\mathcal{C}_t = \textnormal{CCI(}t, C, x, \hat y, \theta, f_\text{att}, f_\text{tgt}, s_\text{cci}\textnormal{)}$
    \ForAll{$c \in \mathcal{C}_t$}
      \State Store $(C_c, \hat y_t)$ in $S_\text{ct}$
    \EndFor
  \EndFor
  \State \textbf{return} $S_\text{ct}$ // Set of cue-target pairs
\EndProcedure
\Procedure{CTI}{$C, x, \hat y, \theta, \mathcal{M}, s_\text{cti}$}
    \State $\mathcal{T} = \emptyset$ // Empty set for context-sensitive indices of $\hat y$ tokens
    \ForAll{$\hat{y}_i \in \hat{y}$}
        \ForAll{$m \in \mathcal{M}$}
            \State $m^i = m_j \big(P_{\text{ctx}}(\hat{y}_i), P_{\text{no-ctx}}(\hat{y}_i) \big)$
        \EndFor
        \If{$(s_{\text{cti}}(m_1^i, \dots, m_M^i) = 1$)}
            \State Store $i$ in set $\mathcal{T}$
        \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{T}$
\EndProcedure
\Procedure{CCI}{$t, C, x, \hat y, \theta, f_\text{att}, f_\text{tgt}, s_\text{cci}$}
    \State $\mathcal{C}_t = \emptyset$ // Empty set for contextual cues for target token $t$
    \State Generate constrained non-contextual target current sequence $\tilde y^*$ from $\hat y_{<t}$
    \State Use attribution method $f_\text{att}$ with target $f_\text{tgt}$ to get importance scores $A_t$
    \State Identify the subset $A_{t\,\text{ctx}}$ corresponding to tokens of context $C = \{ C_1, \dots, C_K\}$
    \ForAll{$a_i \in A_{t\,\text{ctx}} = \{a_1, \dots, a_K\}$}
        \If{$s_\text{cci}(a_i) = 1$}
            \State Store $C_i$ in $\mathcal{C}_t$
        \EndIf
    \EndFor
    \State \textbf{return} $\mathcal{C}_t$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

### Details on Translation Evaluation {#sec-pecore-appendix-eval-details}

We compute BLEU using the SACREBLEU library [@post-2018-call] with default parameters `nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1`.
The models fine-tuned with source and target context clearly outperform the ones trained with source only, both in terms of generic translation quality and context-sensitive disambiguation accuracy.
This motivates our choice to focus primarily on those models for our main analysis. All models are available in the following Huggingface organization: [`https://hf.co/context-mt`](https://hf.co/context-mt). The $S_{\text{ctx}}$ models correspond to those matching `context-mt/scat-<MODEL\_TYPE>-ctx4-cwd1-en-fr`, while $S+T_{\text{ctx}}$ models have the `context-mt/scat-<MODEL\_TYPE>-target-ctx4-cwd0-en-fr` identifier.

### Full CTI and CCI Results {#sec-pecore-appendix-cti-cci-results}

@fig-cti-f1-app and @fig-cti-auprc-app present the CTI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to @fig-marian-big-f1-cti in the main analysis.

:::{#fig-cti-f1-app}

![](../figures/chap-4-pecore/macro_f1_cti_marian-small-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_mbart50-1toM-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-small-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_mbart50-1toM-scat-target_box.pdf){width=85% fig-align="center"}

Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on [ok-cs]{.smallcaps} context-sensitive subsets (right). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ ⓸ OpusMT Small S+T$_\text{ctx}$ ⓹ OpusMT Large S+T$_\text{ctx}$ ⓺ mBART-50 S+T$_\text{ctx}$.
:::

:::{#fig-cti-auprc-app}
![](../figures/chap-4-pecore/auprc_cti_marian-small-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-big-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_mbart50-1toM-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-small-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-big-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_mbart50-1toM-scat-target_box.pdf){width=85% fig-align="center"}

Area Under Precision-Recall Curve (AUPRC) of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on [ok-cs]{.smallcaps} context-sensitive subsets (right). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ ⓸ OpusMT Small S+T$_\text{ctx}$ ⓹ OpusMT Large S+T$_\text{ctx}$ ⓺ mBART-50 S+T$_\text{ctx}$.
:::

@fig-cci-f1-app @fig-cci-auprc-app present the CCI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to @fig-marian-big-f1-cci in the main analysis.

:::{#fig-cci-f1-app}

![](../figures/chap-4-pecore/macro_f1_cci_marian-small-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cci_marian-big-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cci_mbart50-1toM-scat+tgt_box.pdf){width=100% fig-align="center"}

Macro F1 of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show  median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ and S+T$_\text{ctx}$.
:::

:::{#fig-cci-auprc-app}
![](../figures/chap-4-pecore/auprc_cci_marian-small-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cci_marian-big-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cci_mbart50-1toM-scat+tgt_box.pdf){width=100% fig-align="center"}

Area Under Precision-Recall Curve (AUPRC) of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show  median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ and S+T$_\text{ctx}$.
:::

{{< pagebreak >}}
\FloatBarrier

## Context Attribution for Trustworthy Retrieval-Augmented Generation {#sec-mirage-appendix}

### Answer Attribution on the Full XOR-AttriQA {#sec-mirage-appendix-full-attribution}

Differently from the concatenation setup in @sec-chap-5-mirage, we also test [Mirage]{.smallcaps} on the full XOR-AttriQA dataset by constraining CORA generation to match the annotated answer $\mathbf{y}$. We adopt a procedure similar to @muller-etal-2023-evaluating by considering a single document-answer pair $(\text{doc}_i, \mathbf{y})$ at a time, and using [Mirage]{.smallcaps}'s CTI step to detect whether $\mathbf{y}$ is sensitive to the context $\text{doc}_i$. Results in [@tbl-cti-agreement] show that [Mirage]{.smallcaps} achieves performances in line with other AA methods despite these approaches employing ad-hoc validators trained with as many as 540B parameters.

{{< include ../tables/chap-5-mirage/_cti-agreement.qmd >}}

```pseudocode
#| label: alg-mirage-restore
#| pdf-placement: "ht!"

\begin{algorithm}
\caption{Restore original document sequence}
\begin{algorithmic}
\Require $\{Doc_1, ..., Doc_n\}$, $query$, $answer$, $\mathbb{M}$
\Procedure{RestoreSequence}{$\{Doc_1, ..., Doc_n\}, query, answer, \mathbb{M}$}
    \State $iter = 0, \, found=False$
    \While {$iter < 200$}
        \State $pred \gets \mathbb{M}(\{Doc_1, ..., Doc_n\}, query)$
        \If{$pred == answer$}
            \State $found = True$, \textbf{break}
        \Else
            \State ${\rm Shuffle}(\{Doc_1, ..., Doc_n\})$
        \EndIf
        \State iter += 1
    \EndWhile
    \If{$found$}
        \State \textbf{return} $\{Doc_1, ..., Doc_n\}$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
```

### ELI5 Evaluation with Standard Prompt {#sec-mirage-appendix-standard-prompt}

{{< include ../tables/chap-5-mirage/_prompts.qmd >}}

In the main experiments, we use self-citation prompts by @gao-etal-2023-enabling for [Mirage]{.smallcaps} answer attribution to control for the effect of different prompts on model responses, enabling a direct comparison with self-citation. In [@tbl-eli5-full-results], we provide additional results where a standard prompt without citation instructions is used ("Standard" prompt in [@tbl-prompts]). We observe the overall citation quality of [Mirage]{.smallcaps} drops when a standard prompt is used instead of self-citation instructions. We conjecture this might be due to answers that are, in general, less attributable to the provided context due to a lack of explicit instructions to do so. We also observe higher correctness and fluency in the standard prompt setting, suggesting a trade-off between answer and citation quality.

{{< include ../tables/chap-5-mirage/_eli5-full-results.qmd >}}

### More Examples of Disagreement {#sec-mirage-appendix-more-cases}

[@tbl-case-study-3;@tbl-case-study-5] and [@tbl-case-study-6] show three cases where [Mirage]{.smallcaps} answer attributions disagree with self-citation attributions of the same generation.^[Note that we remove citation tags in self-citation generated answers and use MIRAGE to attribute the resulting answers, as introduced in [@].] We adopt the Top-5% threshold for CCI Filtering. In [@tbl-case-study-3], the generated answer becomes the consistent description `cancel the alarm' as mentioned in Document [3]. In this case, [Mirage]{.smallcaps} attributes this sentence to the corresponding Document [3] while NLI maintains its attribution of Document [3] due to lexical overlap, as suggested in @sec-chap-5-mirage.

{{< include ../tables/chap-5-mirage/_case-study-3.qmd >}}

On several occasions, we observe that [Mirage]{.smallcaps} attributes all occurrences of lexically similar tokens in the context when the LLM is generating the same word. For example, in [@tbl-case-study-5] the named entity "Science ABC" is mentioned in both Document [1] and [4], and [Mirage]{.smallcaps} finds both occurrences as salient towards the prediction of the same entity in the output. Similarly, in [@tbl-case-study-6], the generated word `Document' is attributed to the previous mentions of the same word in the context. In both cases, when moving from token-level to sentence-level AA, this dependence would result in wrong AA according to NLI, since the documents are not entailing the answer, but rather making a specific token more likely. These cases reflect the possible discrepancy between AA intended as logical entailment and actual context usage during generation. Future work could explore more elaborate ways to aggregate granular information at sentence level while preserving faithfulness to context usage.

{{< include ../tables/chap-5-mirage/_case-study-5.qmd >}}

{{< include ../tables/chap-5-mirage/_case-study-6.qmd >}}