# Attributing Context Usage in Multilingual NLP {#sec-appendix-a}

## Attributing Language Model Generations with the Inseq Toolkit {#sec-inseq-appendix}

### Additional Details on Turkish Gender Bias Case Study {#sec-inseq-appendix-turkish-gender-bias}

[@tbl-turkish-wordlist] shows the list of occupation terms used in the gender bias case study ([@sec-chap3-gender-bias]).
We correlate the ranking of occupations based on the selected attribution metrics and probabilities with U.S. labor statistics^[[https://github.com/rudinger/winogender-schemas](https://github.com/rudinger/winogender-schemas/blob/master/data/occupations-stats.tsv) (`bls_pct_female` column)]. Table [@tbl-chap3-m2m-gender-example] example was taken from the BUG dataset [@levy-etal-2021-collecting-large].

{{< include ../tables/chap-3-inseq/_turkish-wordlist.qmd >}}

### Example of Pair Aggregation for Contrastive MT Comparison {#sec-inseq-appendix-pair-agg-gender-swap}

An example of gender translation pair using the synthetic template of [@sec-chap3-gender-bias] is show in [@fig-ex-gender-pair-agg], highlighting a large drop in probability when switching the gendered pronoun for highly gender-stereotypical professions, similar to [@tbl-chap3-turkish-gender-bias] results.

::: {#fig-ex-gender-pair-agg}
```python
import inseq
from inseq.data.aggregator import *

# Load the TR-EN translation model and attach the IG method
model = inseq.load_model(
    "Helsinki-NLP/opus-mt-tr-en", "integrated_gradients"
)

# Forced decoding. Return probabilities, no target attr.
out = model.attribute(
    ["O bir teknisyen", "O bir teknisyen"],
    ["She is a technician.","He is a technician."],
    step_scores=["probability"],
)

# Aggregation pipeline composed by two steps:
# 1. Aggregate subword tokens across all dimensions:
# 2. Aggregate hidden size to produce token-level attributions
subw_aggregator = AggregatorPipeline(
    [SubwordAggregator, SequenceAttributionAggregator]
)
masculine = out[0].aggregate(aggregator=subw_aggregator)
feminine = out[1].aggregate(aggregator=subw_aggregator)

# Take the diff of the scores of the two attributions
masculine.show(aggregator=PairAggregator, paired_attr=feminine)
```

![](../figures/chap-3-inseq/tr_en_pair_aggr_example.png){width=60% fig-align="center"}

Comparing attributions for a synthetic Turkish-to-English translation example with underspecified source pronoun gender using a MarianMT Turkish-to-English translation model [@tiedemann-2020-tatoeba]. Values in the visualized attribution matrix show a 46% higher probability of producing the masculine pronoun in the translation and a relative decrease of 18.4% in the importance of the Turkish occupation term compared to the feminine pronoun case.
:::

### Example of Quantized Contrastive Attribution of Factual Knowledge {#sec-inseq-appendix-factual-knowledge}

[@fig-ex-cat-counterfact-code] presents code used in [@sec-chap3-rome-repro] case study, with visualized attribution scores for contrastive examples presented in [@fig-chap3-ex-cat-counterfact-plots].

:::{#fig-ex-cat-counterfact-code}
```python
import inseq
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer

# The model is loaded in 8-bit on available GPUs
model = AutoModelForCausalLM.from_pretrained(
    "gpt2-xl", load_in_8bit=True, device_map="auto"
)
# Counterfact datasets used by Meng et al. (2022)
data = load_dataset("NeelNanda/counterfact-tracing")["train"]

# GPT-2 XL is a Transformer model with 48 layers
for layer in range(48):
    attrib_model = inseq.load_model(
        model,
        "layer_gradient_x_activation",
        tokenizer="gpt2-xl",
        target_layer=model.transformer.h[layer].mlp,
    )
    for i, ex in data:
        # e.g. "The capital of Second Spanish Republic is"
        # -> Madrid (true) / Paris (false)
        prompt = ex["relation"].format(ex["subject"])
        true_answer = prompt + ex["target_true"]
        false_answer = prompt + ex["target_false"] 
        # Contrastive attribution of true vs false answer
        out = attrib_model.attribute(
            prompt,
            true_answer,
            attributed_fn="contrast_prob_diff",
            contrast_targets=false_answer,
            show_progress=False,
        )
```

Example code to contrastively attribute factual statements from the Counterfact Tracing dataset, using Layer Gradient $\times$ Activation to compute importance scores until intermediate layers of the GPT2-XL model.
:::

::: {#fig-chap3-ex-cat-counterfact-plots layout-ncol="2"}

![](../figures/chap-3-inseq/cat_example_1.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_2.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_3.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_4.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_5.png){width=50% fig-align="center"}

![](../figures/chap-3-inseq/cat_example_6.png){width=50% fig-align="center"}

Visualization of contrastive attribution scores on a subset of layers (23 to 48) for some selected dataset examples. Plot labels show the contrastive pairs of false $\rightarrow$ true answer used as attribution targets.
:::

{{< pagebreak >}}
\FloatBarrier

## Quantifying Context Usage in Neural Machine Translation {#sec-pecore-appendix}

### Details on Translation Evaluation {#sec-pecore-appendix-eval-details}

We compute BLEU using the SACREBLEU library [@post-2018-call] with default parameters `nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.3.1`.
The models fine-tuned with source and target context clearly outperform the ones trained with source only, both in terms of generic translation quality and context-sensitive disambiguation accuracy.
This motivates our choice to focus primarily on those models for our main analysis. All models are available in the following Huggingface organization: [`https://hf.co/context-mt`](https://hf.co/context-mt). The $S_{\text{ctx}}$ models correspond to those matching `context-mt/scat-<MODEL\_TYPE>-ctx4-cwd1-en-fr`, while $S+T_{\text{ctx}}$ models have the `context-mt/scat-<MODEL\_TYPE>-target-ctx4-cwd0-en-fr` identifier.

### Full CTI and CCI Results {#sec-pecore-appendix-cti-cci-results}

@fig-cti-f1-app and @fig-cti-auprc-app present the CTI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to @fig-marian-big-f1-cti in the main analysis.

:::{#fig-cti-f1-app}

![](../figures/chap-4-pecore/macro_f1_cti_marian-small-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_mbart50-1toM-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-small-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_marian-big-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cti_mbart50-1toM-scat-target_box.pdf){width=85% fig-align="center"}

Macro F1 of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on [ok-cs]{.smallcaps} context-sensitive subsets (right). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ ⓸ OpusMT Small S+T$_\text{ctx}$ ⓹ OpusMT Large S+T$_\text{ctx}$ ⓺ mBART-50 S+T$_\text{ctx}$.
:::

:::{#fig-cti-auprc-app}
![](../figures/chap-4-pecore/auprc_cti_marian-small-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-big-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_mbart50-1toM-scat_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-small-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_marian-big-scat-target_box.pdf){width=85% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cti_mbart50-1toM-scat-target_box.pdf){width=85% fig-align="center"}

Area Under Precision-Recall Curve (AUPRC) of contrastive metrics for context-sensitive target token identification (CTI) on the full datasets (left) or on [ok-cs]{.smallcaps} context-sensitive subsets (right). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ ⓸ OpusMT Small S+T$_\text{ctx}$ ⓹ OpusMT Large S+T$_\text{ctx}$ ⓺ mBART-50 S+T$_\text{ctx}$.
:::

@fig-cci-f1-app @fig-cci-auprc-app present the CCI plausibility of all tested models for the Macro F1 and AUPRC metrics, similarly to @fig-marian-big-f1-cci in the main analysis.

:::{#fig-cci-f1-app}

![](../figures/chap-4-pecore/macro_f1_cci_marian-small-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cci_marian-big-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/macro_f1_cci_mbart50-1toM-scat+tgt_box.pdf){width=100% fig-align="center"}

Macro F1 of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show  median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ and S+T$_\text{ctx}$.
:::

:::{#fig-cci-auprc-app}
![](../figures/chap-4-pecore/auprc_cci_marian-small-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cci_marian-big-scat+tgt_box.pdf){width=100% fig-align="center"}

![](../figures/chap-4-pecore/auprc_cci_mbart50-1toM-scat+tgt_box.pdf){width=100% fig-align="center"}

Area Under Precision-Recall Curve (AUPRC) of CCI methods over full datasets using models trained with only source context (left) or with source+target context (right). Boxes and red median lines show CCI results based on gold context-sensitive tokens. Dotted bars show  median CCI scores obtained from context-sensitive tokens identified by KL-Divergence during CTI (E2E settings). **Top to bottom:** ⓵ OpusMT Small S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓶ OpusMT Large S$_\text{ctx}$ and S+T$_\text{ctx}$ ⓷ mBART-50 S$_\text{ctx}$ and S+T$_\text{ctx}$.
:::

### [PECoRe]{.smallcaps} for Other Language Generation Tasks {#sec-pecore-appendix-other-tasks}

This section complements our MT analysis and by demonstrating the applicability of [PECoRe]{.smallcaps} to other model architectures and different language generation tasks. @tbl-zephyr-pecore presents some examples. To generate the outputs, we use Zephyr Beta [@tunstall-etal-2023-zephyr], a state-of-the-art conversational decoder-only language model with 7B parameters fine-tuned from the Mistral 7B v0.1 pre-trained model [@jiang-etal-2023-mistral]. We follow the same setup of @sec-chap4-analysis, using KL-Divergence as CTI metric, $\nabla_{\text{diff}}$ as CCI method and setting both $s_\text{CTI}$ and $s_\text{CCI}$ to two standard deviations above the per-example mean.

[Constrained Story Generation]{.paragraph} In the first example, the model is asked to generate a story about \textit{Florbz}, which is defined as a planet with an alien race only in context $C_x$. We observe a plausible influence of several context components throughout the generation process, leading to a short story respecting the constraint specified in the system prompt provided as context.

[Factual Question Answering]{.paragraph} In the second example, the model is asked to retrieve date information from the context and perform a calculation to derive the age of a fictional building. While the non-contextual generation $\tilde y$ hallucinates an age and a construction date associated to a real historical landmark, contextual generation $\hat y$ produces a wrong age, but plausibly relies on the date provided in $C_x$ during generation. Interestingly, we can also identify when the system instruction of "keeping answers concise" intervenes during generation.

[Information Extraction]{.paragraph} The last example simulates a retrieval-augmented generation scenario in which a fictional refund policy is used as context to answer user queries. In this scenario, contextual generation $\hat y$ correctly identifies the user query as leading to a no-refund situation due to the limited refund timeline stated in the policy, and the corresponding timeline (\textit{within 30 days}) is identified as a contextual cue leading to the model's negative response.

{{< include ../tables/chap-4-pecore/_zephyr-pecore.qmd >}}

{{< pagebreak >}}
\FloatBarrier

## Answer Attribution for Trustworthy Retrieval-Augmented Generation {#sec-mirage-appendix}

### Answer Attribution on the Full XOR-AttriQA {#sec-mirage-appendix-full-attribution}

Differently from the concatenation setup in @sec-chap-5-mirage, we also test [Mirage]{.smallcaps} on the full XOR-AttriQA dataset by constraining CORA generation to match the annotated answer $\mathbf{y}$. We adopt a procedure similar to @muller-etal-2023-evaluating by considering a single document-answer pair $(\text{doc}_i, \mathbf{y})$ at a time, and using [Mirage]{.smallcaps}'s CTI step to detect whether $\mathbf{y}$ is sensitive to the context $\text{doc}_i$. Results in [@tbl-cti-agreement] show that [Mirage]{.smallcaps} achieves performances in line with other AA methods despite these approaches employing ad-hoc validators trained with as many as 540B parameters.

{{< include ../tables/chap-5-mirage/_cti-agreement.qmd >}}

### ELI5 Evaluation with Standard Prompt {#sec-mirage-appendix-standard-prompt}

{{< include ../tables/chap-5-mirage/_prompts.qmd >}}

In the main experiments, we use self-citation prompts by @gao-etal-2023-enabling for [Mirage]{.smallcaps} answer attribution to control for the effect of different prompts on model responses, enabling a direct comparison with self-citation. In [@tbl-eli5-full-results], we provide additional results where a standard prompt without citation instructions is used ("Standard" prompt in [@tbl-prompts]). We observe the overall citation quality of [Mirage]{.smallcaps} drops when a standard prompt is used instead of self-citation instructions. We conjecture this might be due to answers that are, in general, less attributable to the provided context due to a lack of explicit instructions to do so. We also observe higher correctness and fluency in the standard prompt setting, suggesting a trade-off between answer and citation quality.

{{< include ../tables/chap-5-mirage/_eli5-full-results.qmd >}}

### More Examples of Disagreement {#sec-mirage-appendix-more-cases}

[@tbl-case-study-3;@tbl-case-study-5] and [@tbl-case-study-6] show three cases where [Mirage]{.smallcaps} answer attributions disagree with self-citation attributions of the same generation.^[Note that we remove citation tags in self-citation generated answers and use MIRAGE to attribute the resulting answers, as introduced in [@sec-chap5-eli5-evaluation].] We adopt the Top-5% threshold for CCI Filtering. In [@tbl-case-study-3], the generated answer becomes the consistent description `cancel the alarm' as mentioned in Document [3]. In this case, [Mirage]{.smallcaps} attributes this sentence to the corresponding Document [3] while NLI maintains its attribution of Document [3] due to lexical overlap, as suggested in @sec-chap-5-mirage.

{{< include ../tables/chap-5-mirage/_case-study-3.qmd >}}

On several occasions, we observe that [Mirage]{.smallcaps} attributes all occurrences of lexically similar tokens in the context when the LLM is generating the same word. For example, in [@tbl-case-study-5] the named entity "Science ABC" is mentioned in both Document [1] and [4], and [Mirage]{.smallcaps} finds both occurrences as salient towards the prediction of the same entity in the output. Similarly, in [@tbl-case-study-6], the generated word `Document' is attributed to the previous mentions of the same word in the context. In both cases, when moving from token-level to sentence-level AA, this dependence would result in wrong AA according to NLI, since the documents are not entailing the answer, but rather making a specific token more likely. These cases reflect the possible discrepancy between AA intended as logical entailment and actual context usage during generation. Future work could explore more elaborate ways to aggregate granular information at sentence level while preserving faithfulness to context usage.

{{< include ../tables/chap-5-mirage/_case-study-5.qmd >}}

{{< include ../tables/chap-5-mirage/_case-study-6.qmd >}}