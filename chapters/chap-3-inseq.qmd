# Attributing Language Model Generations with the Inseq Toolkit {#sec-chap-3-inseq}

::: {.callout-note icon="false"}

## Chapter Summary

This first experimental chapter presents the Inseq interpretability toolkit employed for multiple analyses throughout this thesis. Inseq is a Python library that democratizes access to interpretability analyses of language models by enabling intuitive extraction of models' internal information and saliency scores throughout the generation process. After introducing Inseq design and features, we demonstrate its capabilities through applications highlighting gender biases in machine translation models and factual knowledge location inside the GPT-2 language model. Thanks to its extensible interface supporting cutting-edge techniques, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling reproducible model evaluations.

\vspace{7pt}\noindent

This chapter is adapted from the papers *Inseq: An Interpretability Toolkit for Sequence Generation Models* [@sarti-etal-2023-inseq-fixed] and *Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit* [@sarti-etal-2024-democratizing].

:::

> *As in manufacture so in science, retooling is an extravagance to be reserved for the occasion that demands it. The significance of crises is the indication they provide that an occasion for retooling has arrived.*
>
> *-- Thomas S. Kuhn, The Structure of Scientific Revolutions (1970)*

Recent years saw an increase in studies and tools aimed at improving our behavioral or mechanistic understanding of neural language models [@belinkov-glass-2019-analysis].

Many studies applied such techniques to modern deep learning architectures, including transformers [@vaswani-etal-2017-attention], leveraging gradients [@baherens-etal-2010-explain; @sundararajan-etal-2017-ig], attention patterns [@xu-etal-2015-show; @clark-etal-2019-bert] and input perturbations [@zeiler-fergus-2014-visualizing; @feng-etal-2018-pathologies] to quantify input importance, often leading to controversial outcomes in terms of faithfulness, plausibility and overall usefulness of such explanations [@adebayo-etal-2018-sanity; @jain-wallace-2019-attention; @jacovi-goldberg-2020-towards; @zafar-etal-2021-lack].

However, input attribution techniques have mainly been applied to classification settings [@atanasova-etal-2020-diagnostic; @wallace-etal-2020-interpreting; @madsen-etal-2022-evaluating; @chrysostomou-aletras-2022-empirical], with relatively little interest in the more convoluted mechanisms underlying generation. Classification attribution is a single-step process resulting in one importance score per input token, often allowing for intuitive interpretations in relation to the predicted class. Sequential attribution^[We use *sequence generation* to refer to all iterative tasks including (but not limited to) natural language generation.] instead involves a computationally expensive multi-step iteration producing a matrix $A_{ij}$ representing the importance of every input $i$ in the prediction of every generation outcome $j$ ([@fig-inseq-teaser]).

![Example of Inseq usage with a ðŸ¤— `transformers` causal language model. Given a prompt, attribution scores and next-step probabilities are extracted from the model at every generation step, with a final visualization aggregating values at the token level. Output attribution scores show that the model relies on the keyword "innovate" to begin the idiomatic expression "think outside the box" at relatively low confidence ($p = 0.5$). However, importance shifts to previous tokens in the idiom and confidence progressively grows throughout the generation.](../figures/chap-3-inseq/teaser_horizontal.pdf){#fig-inseq-teaser fig-pos="t"}

Moreover, since previous generation steps causally influence following predictions, they must be dynamically incorporated into the set of attributed inputs throughout the process. Lastly, while classification usually involves a limited set of classes and simple output selection (e.g., argmax after softmax), generation routinely works with large vocabularies and non-trivial decoding strategies [@eikema-aziz-2020-map]. These differences limited the use of input attribution methods for generation settings, with relatively few works improving attribution efficiency [@vafa-etal-2021-rationales; @ferrando-etal-2022-towards] and explanations' informativeness [@yin-neubig-2022-interpreting].

Having established a theoretical background on input attribution methods in @sec-chap2-attrib, here we introduce **Inseq**, a Python library to democratize access to interpretability analyses of generative language models. Inseq centralizes access to a broad set of input attribution methods, sourced in part from the Captum [@kokhlikyan-etal-2020-captum] framework, enabling a fair comparison of different techniques for all sequence-to-sequence and decoder-only models in the popular ðŸ¤— `transformers` library [@wolf-etal-2020-transformers]. Thanks to its intuitive interface, users can easily integrate interpretability analyses into sequence generation experiments with just 3 lines of code ([@fig-chap3-code-short]). Nevertheless, Inseq is also highly flexible, including cutting-edge attribution methods with built-in post-processing features ([@sec-chap3-feat-attr]), supporting customizable attribution targets and enabling constrained decoding of arbitrary sequences ([@sec-chap3-customize]).

::: {#fig-chap3-code-short layout="[0.5,-0.05,0.45]" layout-valign="center" fig-pos="t"}
```python
import inseq

# Load model and attrib. method
model = inseq.load_model(
    "google/flan-t5-base",
    "integrated_gradients"
)
# Answer and attribute generation
attr_out = model.attribute(
    "Does 3 + 3 equal 6?",
    attribute_target=True
)
# Visualize the attribution,
# apply token-level aggregation
attr_out.show()
```

![](../figures/chap-3-inseq/flant5_math_attribution_example.png){width=50% fig-align="center"}

Computing and visualizing attributions for Flan-T5 [@chung-etal-2022-scaling].
:::

In terms of usability, Inseq greatly simplifies access to local and global explanations with built-in support for a command line interface (CLI), optimized batching enabling dataset-wide attribution, and various methods to visualize, serialize and reload attribution outcomes and generated sequences ([@sec-chap3-usability]). Ultimately, Inseq aims to make sequence models first-class citizens in interpretability research and drive future advances in interpretability for generative applications.

## Related Work

[Tools for NLP Interpretability]{.paragraph} Although many post-hoc interpretability libraries were released recently, only a few support sequential input attribution. Notably, LIT [@tenney-etal-2020-language], a structured framework for analyzing models across modalities, and Ecco [@alammar-2021-ecco], a library specialized in interactive visualizations of model internals. LIT is an all-in-one GUI-based tool to analyze model behaviors on entire datasets. However, the library does not provide out-of-the-box support for ðŸ¤— `transformers` models, requiring the definition of custom wrappers to ensure compatibility. Moreover, it has a steep learning curve due to its advanced UI, which might be inconvenient when working on a small amount of examples. All these factors limit LIT usability for researchers working with custom models, needing access to extracted scores, or being less familiar with interpretability research. On the other hand, Ecco is closer to our work, being based on ðŸ¤— `transformers` and having started to support encoder-decoder models concurrently with Inseq development. Despite a marginal overlap in their functionalities, the two libraries provide orthogonal benefits: Inseq's flexible interface makes it especially suitable for methodical quantitative analyses involving repeated evaluations, while Ecco excels in qualitative analyses aimed at visualizing model internals. Other popular tools such as ERASER [@deyoung-etal-2020-eraser], Thermostat [@feldhus-etal-2021-thermostat], transformers-interpret [@pierse-2021-transformers] and ferret [@attanasio-etal-2023-ferret] do not support sequence models.

## Design

Inseq combines sequence models sourced from ðŸ¤— `transformers` [@wolf-etal-2020-transformers] and attribution methods mainly sourced from Captum [@kokhlikyan-etal-2020-captum]. While only text-based tasks are currently supported, the library's modular design would enable the inclusion of other modeling frameworks, e.g. `fairseq` [@ott-etal-2019-fairseq], and modalities (e.g. speech) without requiring substantial redesign. Optional dependencies include ðŸ¤— `datasets` [@lhoest-etal-2021-datasets] and Rich.^[<https://github.com/Textualize/rich>] [@fig-chap3-class-structure] presents the Inseq hierarchy of models and attribution methods. The model-method connection enables out-of-the-box attribution using the selected method. Framework-specific and architecture-specific classes enable extending Inseq to new modeling architectures and frameworks.

![Inseq models and attribution methods. [Concrete]{color="brand-color.white" bg-color="brand-color.lightreddim"} [classes]{color="brand-color.white" bg-color="brand-color.lightbluedim"} combine abstract [framework]{color="brand-color.white" bg-color="brand-color.lightredclear"} and [architecture]{color="brand-color.white" bg-color="brand-color.lightredclear"} attribution models classes, and are derived from abstract attribution methods' [categories]{color="brand-color.white" bg-color="brand-color.lightblueclear"}.](../figures/chap-3-inseq/class_structure.pdf){#fig-chap3-class-structure width=100% fig-pos="t"}

### Guiding Principles

- **Research and Generation-oriented:** Inseq should support interpretability analyses of a broad set of sequence generation models without focusing narrowly on specific architectures or tasks. Moreover, the inclusion of new, cutting-edge methods should be prioritized to enable fair comparisons with well-established ones.

- **Scalable:** The library should provide an optimized interface to a wide range of use cases, models and setups, ranging from interactive attributions of individual examples using toy models to compiling statistics of large language models' predictions for entire datasets.

- **Beginner-friendly:** Inseq should provide built-in access to popular frameworks for sequence generation modeling and be fully usable by non-experts at a high level of abstraction, providing sensible defaults for supported attribution methods.

- **Extensible:** Inseq should support a high degree of customization for experienced users, with out-of-the-box support for user-defined solutions to enable future investigations into models' behaviors.

### Input Attribution and Post-processing {#sec-chap3-feat-attr}

{{< include ../tables/chap-3-inseq/_methods.qmd >}}

At its core, Inseq provides a simple interface to apply input attribution techniques for sequence generation tasks. We categorize methods in three groups, *gradient-based*, *internals-based* and *perturbation-based*, depending on their underlying approach to importance quantification.^[We distinguish between gradient- and internals-based methods to account for their difference in scores' granularity.] [@tbl-methods] presents the full list of supported methods. Aside from popular model-agnostic methods, Inseq notably provides built-in support for attention weight attribution and a range of cutting-edge methods not supported in any other toolkit, such as Discretized Integrated Gradients [@sanyal-ren-2021-discretized], Sequential Integrated Gradients [@enguehard-2023-sequential], Value Zeroing [@mohebbi-etal-2023-quantifying], and ReAGent [@zhao-shan-2024-reagent]. Moreover, multiple methods allow for the importance attribution of custom intermediate model layers, simplifying studies on representational structures and information mixing in sequential models, such as our case study of [@sec-chap3-rome-repro].

[Source and target-side attribution]{.paragraph} When using encoder-decoder architectures, users can set the `attribute_target` parameter to include or exclude the generated prefix in the attributed inputs. In most cases, this should be desirable to account for recently generated tokens when explaining model behaviors, such as when to terminate the generation (e.g. relying on the presence of `_yes` in the target prefix to predict `</s>` in [@fig-chap3-code-short], right matrix). However, attributing the source side separately could prove useful, for example, to derive word alignments from importance scores.

[Post-processing of attribution outputs]{.paragraph} Aggregation is a fundamental but often overlooked step in attribution-based analyses since most methods produce neuron-level or subword-level importance scores that would otherwise be difficult to interpret. Inseq includes several `Aggregator` classes to perform attribution aggregation across various dimensions. For example, the input word `Explanation` could be tokenized in two subword tokens `Expl` and `anation`, and each token would receive $N$ importance scores, with $N$ being the model embedding dimension. In this case, aggregators could first merge subword-level scores into word-level scores, and then merge granular embedding-level scores to obtain a single token-level score that is easier to interpret. Moreover, aggregation could prove especially helpful for long-form generation tasks such as summarization, where word-level importance scores could be aggregated to obtain a measure of sentence-level relevance. Notably, Inseq allows chaining multiple aggregators like in the example above using the `AggregatorPipeline` class, and provides a `PairAggregator` to aggregate different attribution maps, simplifying the conduction of contrastive analyses as in [@sec-chap3-gender-bias].^[See [@sec-inseq-appendix-pair-agg-gender-swap] for an example.]

### Customizing generation and attribution {#sec-chap3-customize}

During attribution, Inseq first generates target tokens using ðŸ¤— `transformers` and then attributes them step by step. If a custom target string is specified alongside model inputs, the generation step is instead skipped, and the provided text is attributed by constraining the decoding of its tokens.^[Users employing constrained decoding should be aware of its limitations in the presence of a high distributional discrepancy with natural model outputs [@vamvas-sennrich-2021-limits].] Constrained attribution can be used, among other things, for contrastive comparisons of minimal pairs and to obtain model justifications for desired outputs.

[Custom step functions]{.paragraph} At every attribution step, Inseq can extract scores of interest (e.g. probabilities, entropy) that can be useful, among other things, to quantify model uncertainty (e.g. how likely the generated `_yes` token was given the context in [@fig-chap3-code-short]). We collectively refer to functions computing these scores as **step functions**. Inseq provides access to multiple built-in step functions ([@tbl-methods], *S*) enabling the computation of these scores, and allows users to create and register new custom ones. Step scores are computed together with the attribution, returned as separate sequences in the output, and visualized alongside importance scores (e.g. the $p(y_t|y_{<t})$ row in [@fig-inseq-teaser]).

[Step functions as attribution targets]{.paragraph} For methods relying on model outputs to predict input importance (gradient and perturbation-based), input attributions are commonly obtained from the model's output logits or class probabilities [@bastings-etal-2022-will]. However, recent work showed the effectiveness of using targets such as the probability difference of a contrastive output pair to answer interesting questions like "What inputs drive the prediction of $y$ rather than $\hat{y}$?" [@yin-neubig-2022-interpreting]. For example, the gradient $\nabla(p(\text{barking}) - p(\text{crying}))$ given the prompt *"Can you stop the dog from ___"* will highlight the role of the entity *dog* in selecting *barking*, disentangling the semantic component from grammatical correctness by providing a *crying* as grammatically valid choice. [@fig-chap3-contrastive-example] provides an example of such approach for gender bias detection in machine translation.
Inseq users can leverage any built-in or custom-defined step function as an attribution target, enabling advanced use cases like contrastive comparisons.

::: {#fig-chap3-contrastive-example layout="[0.50,-0.03,0.47]" fig-align="center" fig-pos="t"}
```python
import inseq

model = inseq.load_model(
    "Helsinki-NLP/opus-mt-en-it",
    "saliency"
)
attr_out = model.attribute(
    "I said hi to the manager",
    "Ho salutato il manager",
    contrast_targets=\
        "Ho salutato la manager",
    attributed_fn=\
        "contrast_prob_diff",
    step_scores=[
        "probability",
        "contrast_prob_diff"
    ]
)
```

![](../figures/chap-3-inseq/contrastive_attribution_gender_example.pdf){fig-align="center"}

Source-to-target attributions aggregated at token-level, indicating the importance of the stereotypical noun "manager" to generate the Italian masculine pronoun "il" (original) over the feminine "la" (contrastive case).
:::

### Usability Features {#sec-chap3-usability}

[Batched and span-focused attributions]{.paragraph} The library provides built-in batching capabilities, enabling users to go beyond single sentences and attribute even entire datasets in a single function call. When the attribution of a specific span of interest is needed, Inseq also allows specifying a start and end position for the attribution process. This functionality greatly accelerates the attribution process for studies on localized phenomena (e.g. pronoun coreference in MT models).

[Alignment of contrastive options]{.paragraph} Inseq supports customizable *word alignments*, i.e. indices aligning tokens in the original and contrastive generated texts, to support contrastive comparisons between texts of different lengths, including automatic alignments using the multilingual LaBSE encoder [@feng-etal-2022-language] to streamline their application.

[CLI, serialization and visualization]{.paragraph} The Inseq library offers an API to attribute single examples or entire ðŸ¤— Datasets from the command line and save resulting outputs and visualizations to a file. Attribution outputs can be saved and loaded in JSON format with their respective metadata to easily identify the provenance of contents. Attributions can be visualized in the console or IPython notebooks and exported as HTML files.

[Quantized and distributed attribution]{.paragraph} Supporting the attribution of large models is critical given recent scaling tendencies [@kaplan-etal-2020-scaling]. All models allowing for quantization using `bitsandbytes` [@dettmers-etal-2022-gpt3] can be loaded in 4-bit and 8-bit directly from ðŸ¤— `transformers`, and their attributions can be computed normally using Inseq at a fraction of the original computational cost.^[`bitsandbytes 0.37.0` required for backward method, see [@sec-inseq-appendix-factual-knowledge] for an example.] Relatedly, Inseq is also compatible with the **Petals** framework [@borzunov-etal-2023-petals], supporting gradient-based attribution across language models whose computation is distributed across several machines. This can alleviate the need for high-end GPUs to run LLMs, enabling the distributed computation of attribution scores.^[Tutorial: <https://inseq.org/en/latest/examples/petals.html>]

## Case Studies {#sec-chap3-case-studies}

### Gender Bias in Machine Translation {#sec-chap3-gender-bias}

In the first case study, we use Inseq to investigate gender bias in MT models. Studying social biases embedded in these models is crucial to understand and mitigate the representational and allocative harms they might engender [@blodgett-etal-2020-language].
@savoldi-etal-2021-gender note that the study of bias in MT could benefit from explainability techniques to identify spurious cues exploited by the model and the interaction of different features that can lead to intersectional bias.

[Synthetic Setup: Turkish to English]{#chap3-par-gender-bias .paragraph} The Turkish language uses the gender-neutral pronoun *o*, which can be translated into English as either `he`, `she`, or `it`, making it interesting to study gender bias in MT when associated with a language such as English for which models will tend to choose a gendered pronoun form. Previous works leveraged translations from gender-neutral languages to show gender bias present in translation systems [@cho-etal-2019-measuring; @prates-etal-2020-assessing; @farkas-nemeth-2022-measure].
We repeat this simple setup using a Turkish-to-English MarianMT model [@tiedemann-2020-tatoeba] and compute different metrics to quantify gender bias using Inseq.

{{< include ../tables/chap-3-inseq/_turkish-gender-bias.qmd >}}

We select 49 Turkish occupation terms verified by a native speaker (see [@sec-inseq-appendix-turkish-gender-bias]) and use them to infill the template sentence *O bir* ____ (*He/She is a(n) ____*). For each translation, we compute attribution scores for source Turkish pronoun ($x_\text{pron}$) and occupation ($x_\text{occ}$) tokens^[For multi-token occupation terms, e.g., *bilim insanÄ±* (scientist), the first token score was used.] when generating the target English pronoun ($y_\text{pron}$) using Integrated Gradients (IG), Gradients ($\nabla$), and Input $\times$ Gradient (I$\times$G).^[We set $\Delta < 0.05$ for IG to ensure convergence. Token-level aggregation is performed using the L2 norm.] We also collect target pronoun probabilities ($p(y_\text{pron})$), rank the 49 occupation terms using these metrics, and finally compute Kendall's $\tau$ correlation with the percentage of women working in the respective fields, using U.S. labor statistics as in previous works [e.g., @caliskan-etal-2017-semantics; @rudinger-etal-2018-gender]. [@tbl-chap3-turkish-gender-bias] presents our results.

In the **base case**, we correlate the different metrics with how much the gender distribution deviates from an equal distribution ($50-50\%$) for each occupation (i.e., the gender bias irrespective of the direction). We observe a strong gender bias, with *she* being chosen only for 5 out of 49 translations and gender-neutral variants never being produced by the MT model. We find a low correlation between pronoun probability and the degree of gender stereotype associated with the occupation. Moreover, we note a weaker correlation for IG compared to the other two methods. For those, attribution scores for $x_\text{occ}$ show significant correlations with labor statistics, supporting the intuition that the MT model will accord higher importance to source occupation terms associated to gender-stereotypical occupations when predicting the gendered target pronoun.

In the **gender-swap case** (â™€ï¸ $\rightarrow$ â™‚ï¸), we use the `PairAggregator` class to contrastively compare attribution scores and probabilities when translating the pronoun as *She* or *He*.^[An example is provided in [@sec-inseq-appendix-pair-agg-gender-swap].]
We correlate resulting scores with the percentage of women working in the respective occupation and find strong correlations for $p(y_\text{pron})$, supporting the validity of contrastive approaches in uncovering gender bias.

[Qualitative Example: English to Dutch]{#chap3-par-gender-bias-bug .paragraph} We also qualitatively analyze biased MT outputs, showing how attributions can help develop hypotheses about models' behavior. [@tbl-chap3-m2m-gender-example] (top) shows the I $\times$ G attributions for English-to-Dutch translation using M2M-100 [@fan-etal-2021-beyond].

{{< include ../tables/chap-3-inseq/_m2m-gender-example.qmd >}}

The model mistranslates the pronoun *her* into the masculine form *zijn* (his). We find that the wrongly translated pronoun exhibits high probability but does not associate substantial importance to the source occupation term *teacher*. Instead, we find good relative importance for the preceding word and *leraar* (male teacher). This suggests a strong prior bias for masculine variants, shown by the pronoun *zijn* and the noun *leraar*, as a possible cause for this mistranslation. When considering the contrastive example obtained by swapping *leraar* with its gender-neutral variant *leerkracht* ([@tbl-chap3-m2m-gender-example], bottom), we find increased importance of the target occupation in determining the correctly-gendered target pronoun *haar* (her). Our results highlight the tendency of MT models to attend inputs sequentially rather than relying on context, hinting at the known benefits of context-aware models for pronoun translation [@voita-etal-2018-context].

### Locating Factual Knowledge inside GPT-2 {#sec-chap3-rome-repro}

For our second case study, we experiment with a novel attribution-based technique to locate factual knowledge encoded in the layers of GPT-2 1.5B [@radford-etal-2019-language]. Specifically, we aim to reproduce the results of @meng-etal-2022-rome, showing the influence of intermediate layers in mediating the recall of factual statements such as *The Eiffel Tower is located in the city of* $\rightarrow$ `Paris`. @meng-etal-2022-rome estimated the effect of network components in the prediction of factual statements as the difference in probability of a correct target (e.g. *Paris*), given a corrupted subject embedding (e.g. for *Eiffel Tower*), before and after restoring clean activations for some input tokens at different layers of the network. Apart from the obvious importance of final token states in terminal layers, their results highlight the presence of an early site associated with the last subject token playing an important role in recalling the network's factual knowledge ([@fig-chap3-rome-repro], top).

![**Top:** Estimated causal importance of GPT-2 XL layers for predicting factual associations, as reported by @meng-etal-2022-rome. **Bottom:** Average GPT-2 XL Gradient $\times$ Layer Activation scores obtained with Inseq using contrastive factual pairs as attribution targets.](../figures/chap-3-inseq/rome-repro.pdf){#fig-chap3-rome-repro width=65% fig-pos="t"}

To verify such results, we propose a novel knowledge location method, which we name **Contrastive Attribution Tracing** (CAT), adopting the contrastive attribution paradigm of @yin-neubig-2022-interpreting to locate relevant network components by attributing minimal pairs of correct and wrong factual targets (e.g. *Paris* vs. *Rome* for the example above). To perform the contrastive attribution, we use the Layer Gradient $\times$ Activation method, a layer-specific variant of Input $\times$ Gradient, to propagate gradients up to intermediate network activations instead of reaching input tokens. The resulting attribution scores hence answer the question *"How important are layer $L$ activations for prefix token $t$ in predicting the correct factual target over a wrong one?"*. We compute attribution scores for 1000 statements taken from the Counterfact Statement dataset [@meng-etal-2022-rome] and present averaged results in [@fig-chap3-rome-repro] (bottom).^[@fig-chap3-ex-cat-counterfact-plots of @sec-inseq-appendix-factual-knowledge presents some examples.] Our results closely match those of the original authors, providing further evidence of how attribution methods can be used to identify salient network components and guide model editing, as shown by @dai-etal-2022-knowledge.

We introduced the proposed CAT method shortly before the attribution patching technique by @nanda-2023-attribution. Together, these two methods represent the most efficient knowledge location techniques based on gradient propagation, with our approach requiring only a single forward and backward pass of the attributed model. Patching-based approaches such as causal mediation [@meng-etal-2022-rome], on the other hand, provide causal guarantees of feature importance at the price of being more computationally intensive. Despite lacking the causal guarantees of such methods, CAT can provide an approximation of feature importance and greatly simplify the study of knowledge encoded in large language model representations thanks to its efficiency.

## Conclusion {#sec-conclusion}

We introduced Inseq, an easy-to-use but versatile toolkit for interpreting sequence generation models. With many libraries focused on the study of classification models, Inseq is the first tool explicitly aimed at analyzing systems for tasks such as machine translation, code generation, and conversational applications. Researchers can easily add interpretability evaluations to their studies using our library to identify unwanted biases and interesting phenomena in their models' predictions.

With the Inseq toolkit providing the foundational infrastructure for interpretability analysis, the following chapters will leverage the supported input attribution techniques to investigate context usage in context-aware machine translation systems [@sec-chap-4-pecore] and multilingual language models for retrieval-augmented generation [@sec-chap-5-mirage].
