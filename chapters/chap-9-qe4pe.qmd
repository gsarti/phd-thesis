# Word-level Quality Estimation for Machine Translation Post-editing {#sec-chap-9-qe4pe}

::: {.callout-note appearance="simple" icon="false"}

This chapter is adapted from the paper *QE4PE: Word-level Quality Estimation for Human Post-Editing* [@sarti-etal-2025-qe4pe].

:::

::: {.callout-note icon="false"}

## Chapter Summary

Word-level quality estimation (QE) methods aim to detect erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. In this study, we investigate the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated by behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.
:::

## Introduction {#sec-qe4pe-intro}

Recent years saw a steady increase in the quality of machine translation (MT) systems and their widespread adoption in professional translation workflows [@kocmi-etal-2024-findings].
Still, human post-editing of MT outputs remains a fundamental step to ensure high-quality translations, particularly for challenging textual domains requiring native fluency and specialized terminology [@liu-etal-2024-beyond-human].
Quality estimation (QE) techniques were introduced to reduce post-editing effort by automatically identifying problematic MT outputs without the need for human-written reference translations and were quickly integrated into industry platforms [@tamchyna-2021-deploying].

*Segment-level* QE models correlate well with human perception of quality [@freitag-etal-2024-llms] and exceed the performance of reference-based metrics in specific settings [@rei-etal-2021-references; @amrhein-etal-2022-aces; @amrhein-etal-2023-aces].
On the other hand, *word-level* QE methods for identifying error spans requiring revision have received less attention in the past due to their modest agreement with human annotations, despite their promise for more granular and interpretable quality assessment in line with modern MT practices [@zerva-etal-2024-findings].
In particular, while the accuracy of these approaches is regularly assessed in evaluation campaigns, research has rarely focused on assessing the impact of such techniques in realistic post-editing workflows, with notable exceptions suggesting limited benefits [@shenoy-etal-2021-investigating; @sugyeong-etal-2022-word].
This hinders current QE evaluation practices: by foregoing experimental evaluation with human editors, it is implicitly assumed that word-level QE will become helpful once sufficient accuracy is reached, without accounting for the additional challenges towards a successful integration of these methods in post-editing workflows.

![A summary of the QE4PE study. Documents are translated by a neural MT model and reviewed by professional editors across two translation directions and four highlight modalities. Editing effort, productivity and usability across modalities are estimated from editing logs and questionnaires. Finally, the quality of MT and edited outputs is assessed with MQM/ESA human annotations and automatic metrics.](../figures/chap-9-qe4pe/highlevel_qe4pe.pdf){#fig-qe4pe-highlevel width=65% fig-pos="t"}

In this study, which we dub QE4PE (**Q**uality **E**stimation for **P**ost **E**diting), we address this gap by conducting a large-scale study with 42 professional translators for the English$\rightarrow$Italian and English$\rightarrow$Dutch directions to measure the impact of word-level QE on editing quality, productivity and usability.
We aim for a realistic and reproducible setup, employing the high-quality open-source NLLB 3.3B MT model [@nllb-2024-scaling] to translate challenging documents from biomedical and social media domains. We then conduct a controlled evaluation of post-editing with error spans in four *highlight modalities*, i.e. using highlights derived from four word-level QE methods:
a [Oracle]{color="brand-color.supervised"} state-of-the-art QE model trained on human error annotations (XCOMET, [@guerreiro-etal-2024-xcomet]), an [Unsupervised]{color="brand-color.unsupervised"} method leveraging the uncertainty of the MT model during generation, [Oracle]{color="brand-color.oracle"} error spans obtained from the consensus of previous human post-editors, and a [No Highlight]{color="brand-color.nohighlight"} baseline.
The human post-editing is performed using [GroTE]{.smallcaps}, a simple online interface we built to support the real-time logging of granular editing data, enabling a quantitative assessment of editing effort and productivity across highlight modalities.
We also survey professionals using an online questionnaire to collect qualitative feedback about the usability and quality of the MT model, as well as the interface and error span highlights.
Finally, a subset of the original MT outputs and their post-edited variants is annotated following the MQM and ESA protocols [@lommel2014multidimensional; @kocmi-etal-2024-error] to verify quality improvements after post-editing.
See @fig-qe4pe-highlevel for an overview of the study.
Our work represents a step towards an evaluation of translation technologies that is centered on users' experience [@guerberof-arenas-etal-2023-towards; @savoldi-etal-2025-translation].

We release all data, code and the [GroTE]{.smallcaps} editing interface to foster future studies on the usability of error span highlighting techniques for other word-level QE methods and translation directions.^[Public link anonymized.]

## Related Work {#sec-qe4pe-related-work}

[MT Post-editing]{.paragraph} Human post-editing of MT outputs is increasingly common in professional translator workflows, as it was shown to increase the productivity of translators while preserving translation quality across multiple domains [@liu-etal-2024-beyond-human].
However, many factors were found to influence the variability of post-editing productivity across setups, including MT quality [@zouhar-etal-2021-neural], interface familiarity [@laubli-etal-2021-impact], individual variability and source-target languages typological similarity [@sarti-etal-2022-divemt].
Studies evaluating the post-editing process generally focus on *productivity*, i.e. number of processed words/characters per minute, and the temporal, technical and cognitive dimensions of post-editing *effort*, operationalized through behavioral metrics such as editing time, keystrokes and pauses [@krings-2001-repairing; @sarti-etal-2022-divemt].
We adopt these metrics for the QE4PE study and relate them to different highlight modalities.

[Quality estimation for MT]{.paragraph} The field of quality estimation was initially concerned with MT model uncertainty [@blatz-etal-2004-confidence; @specia-etal-2009-estimating], but in time began focusing on predicting translation quality even without using references ([@turchi-etal-2013-coping; @turchi-etal-2014-adaptive; @kepler-etal-2019-openkiwi; @thompson-post-2020-automatic] *inter alia*). Advances in segment- and word-level QE research are regularly assessed in annual WMT campaigns [@fomicheva-etal-2021-eval4nlp; @zerva-etal-2022-findings; @zerva-etal-2024-findings; @blain-etal-2023-findings], where the best-performing QE systems are usually Transformer-based language models trained on human quality judgments, such as the popular [comet]{.smallcaps} model suite [@rei-etal-2020-comet; @rei-etal-2021-references; @rei-etal-2022-comet]. The widespread adoption of the fine-grained Multidimensional Quality Metrics scale (MQM, [@lommel2014multidimensional]) prompted a paradigm shift in MT evaluation, leading to new QE metrics predicting quality at various granularity levels [@kocmi-federmann-2023-gemba; @fernandes-etal-2023-devil; @guerreiro-etal-2024-xcomet]. Aside from supervised models, *unsupervised* methods exploiting model uncertainty and its internal mechanisms were proposed as efficient alternatives to identify potential error spans in MT outputs ([@fomicheva-etal-2020-unsupervised; @dale-etal-2023-detecting; @xu-etal-2023-understanding; @himmi-etal-2024-enhanced], surveyed by [@leiter-etal-2024-explainable]).
In this work, we compare the downstream effectiveness of state-of-the-art supervised and unsupervised word-level QE metrics for post-editing settings.

[QE for Human Post-Editing Workflows]{.paragraph} Automatic QE methods are widely used in the translation industry for triaging automatic translations [@tamchyna-2021-deploying].
While QE usage has been found helpful to increase the confidence and speed of human assessment [@mehandru-etal-2023-physician; @zouhar-etal-2025-ai], an incautious usage of these techniques can lead to a misplaced over-reliance on model predictions [@zouhar-etal-2021-backtranslation].
Interfaces supporting word-level error highlights were developed for studying MT post-editing [@coppers-etal-2018-intellingo; @herbig-etal-2020-mmpe] and code reviewing [@sun-etal-2022-investigating; @vasconcelos-etal-2024-generation], with results suggesting that striking the right balance of user-provided information is fundamental to improve the editing experience and prevent cognitive overload. Most similar to our study, @shenoy-etal-2021-investigating investigated the effect of synthetic word-level QE highlights for English$\rightarrow$German post-editing on Wikipedia data, concluding that word-level QE accuracy was at the time still insufficient to produce tangible productivity benefits in human editing workflows. In this work, we expand the scope of such evaluation by including two translation directions, two challenging real-world text domains and state-of-the-art MT and QE systems and methods.

## Experimental Setup {#sec-qe4pe-experiment-setup}

### Structure of the Study {#sec-qe4pe-structure}

Our study is organized in five stages:

[1) Oracle Post-editing]{.paragraph} As a preliminary step, segments later used in the main assessment are post-edited by three professionals per direction using their preferred interface without logging. This allows us to obtain post-edits and produce [Oracle]{color="brand-color.oracle"} word-level spans based on the editing consensus of multiple human professionals. Translators involved in this stage are not involved further in the study.

[2) Pretask ([Pre]{.smallcaps})]{.paragraph} The pretask allows the *core translators* (12 per language direction, see @sec-qe4pe-participants) to familiarize themselves with the [GroTE]{.smallcaps} interface and text highlights. Before starting, all translators complete a questionnaire to provide demographic and professional information about their profile (@tbl-qe4pe-questionnaire). In the pretask, all translators work in an identical setup, post-editing a small set of documents similar to those of the main task with [Oracle]{color="brand-color.supervised"} highlights.
We assign core translators into three groups based on their speed from editing logs (4 translators per group for *faster*, *average* and *slower* groups in each direction).
Individuals from each group are then assigned randomly to each highlight modality to ensure an equal representation of editing speeds, resulting in 1 *faster*, 1 *average* and *slower* translator for each highlight modality. This procedure is repeated independently for both translation directions.

**3) Main Task ([Main]{.smallcaps})** This task, conducted in the two weeks following the pretask, covers the majority of the collected data and is the main object of study for the analyses of @sec-qe4pe-analysis. In the main task, 24 core translators work on the same texts using the [GroTE]{.smallcaps} interface, with three translators per modality in each translation direction, as shown in @fig-qe4pe-highlevel. After the main task, translators complete a questionnaire on the quality and usability of the MT outputs, the interface and, where applicable, word highlights.^[We do not disclose the highlight modality to translators to avoid biasing their judgment in the evaluation.]

**4) Post-Task ([Post]{.smallcaps})** After [Main]{.smallcaps}, the 12 core translators per direction are asked to post-edit an additional small set of related documents with [GroTE]{.smallcaps}, but this time working all with the [No Highlight]{color="brand-color.nohighlight"} modality. This step lets us obtain baseline editing patterns for each translator to estimate individual speed and editing differences across highlight modalities without the confounder of interface proficiency accounted for in the [Pre]{.smallcaps} stage.

**5) Quality Assessment (QA)** Finally, a subset consisting of 148 main task segments is randomly selected for manual annotation by six new translators per direction (see @sec-qe4pe-participants). For each segment, the original MT output and all its post-edited versions are annotated with MQM error spans, including minor/major error severity and a subset of MQM error categories including e.g. mistranslations, omissions and stylistic errors @lommel2014multidimensional.^[See @tbl-qe4pe-qa-example and @tbl-qe4pe-qa-guidelines for an overview of setup and guidelines.] Moreover, the annotator proposes corrections for each error span, ultimately providing a 0-100 quality score matching the common DA scoring adopted in multiple WMT campaigns. We adopt this scoring system, which closely adheres to the ESA evaluation protocol @kocmi-etal-2024-error, following recent results showing its effectiveness and efficiency for ranking MT system.

In summary, for each translation direction, we collect 3 full sets of oracle post-edits, 12 full sets of edits with behavioral logs for [pre]{.smallcaps}, [main]{.smallcaps} and [post]{.smallcaps} task data, and 13 subsets of main task data (12 post-edits, plus the original MT output) annotated with MQM error spans, corrections and segment-level ESA ratings. Moreover, we also collect 12 pre- and post-task questionnaire responses from *core set* translators to obtain a qualitative view of the editing process.

### Highlight Modalities {#sec-qe4pe-modalities}

We conduct our study on four highlight modalities across two severity levels (*minor* and *major* errors).
Using multiple severity levels follows the current MT evaluation practices [@freitag-etal-2021-experts; @freitag-etal-2024-llms], and previous results showing that users tend to prefer more granular and informative word-level highlights [@shenoy-etal-2021-investigating; @vasconcelos-etal-2024-generation]. The highlight modalities we employ are:

**[No Highlight]{color="brand-color.nohighlight"}** The text is presented as-is, without any highlighted spans. This setting serves as a baseline to estimate the default post-editing quality and productivity using our interface.

**[Oracle]{color="brand-color.oracle"}** Following the Oracle Post-editing phase, we produce oracle error spans from the editing consensus of human post-editors. We label text spans that were edited by two out of three translators as *minor*, and those edited by all three translators as *major*, following the intuition that more critical errors are more likely to be identified by several annotators, while minor changes will show more variance across subjects. This modality serves as a best-case scenario, providing an upper bound for future improvements in word-level QE quality.

**[Oracle]{color="brand-color.supervised"}** In this setting, word-level error spans are obtained using XCOMET-XXL [@guerreiro-etal-2024-xcomet], which is a multilingual Transformer encoder [@xlmr] further trained for joint word- and sentence-level QE prediction.
We select XCOMET-XXL in light of its broad adoption, open accessibility and state-of-the-art performance in QE across several translation directions [@zerva-etal-2024-findings].
For the severity levels, we use the labels predicted by the model, mapping *critical* labels to the *major* level.

{{< include ../tables/chap-9-qe4pe/_unsup-perf.qmd >}}

**[Unsupervised]{color="brand-color.unsupervised"}** In this modality, we exploit the access to the MT model producing the original translations to obtain *uncertainty-based highlights*. As a preliminary evaluation to select a capable unsupervised word-level QE method, we evaluate two unsupervised QE methods employing token log-probabilities assigned by MT model to predict human post-edits: raw negative log-probabilities (Logprobs), corresponding to the surprisal assigned by the MT model to every generated token, and their variance for 10 steps of Monte Carlo Dropout (MCD Var., @gal-ghahramani-2016-dropout). We employ surprisal-based metrics following previous work showing their effectiveness in predicting translation errors [@fomicheva-specia-2019-taking] and human editing time [@lim-etal-2024-predicting]. We collect scores for the English$\rightarrow$Italian and English$\rightarrow$Dutch directions of QE4PE Oracle post-edits and [DivEMT]{.smallcaps} [@sarti-etal-2022-divemt] to identify the best-performing method, using metric scores extracted from the original models used for translation to predict human post-edits. We use average precision (AP) as a threshold-agnostic performance metric for the tested continuous methods. [Oracle]{color="brand-color.oracle"} highlights obtained from the consensus of three annotator in the first stage of the study are used as reference for QE4PE, while a single set of post-edits is available for [DivEMT]{.smallcaps}. The XCOMET-XXL model used for [Supervised]{color="brand-color.supervised"} highlights, and the average agreement of individual [Oracle]{color="brand-color.oracle"} editors with the consensus label are also included for comparison. @tbl-qe4pe-unsup-perf^[Full results in @tbl-qe4pe-unsup-selection. Highlights are extended from tokens to words to match the granularity of other modalities.] show a strong performance for the MCD Var. method, even surpassing the accuracy of the supervised XCOMET model across both datasets. Hence, we select MCD Var. for the [Unsupervised]{color="brand-color.unsupervised"} highlight modality, setting value thresholds for minor/major errors to match the respective highlighted word proportions in the [Supervised]{color="brand-color.supervised"} modality to ensure a fair comparison.

### Data and MT model {#sec-qe4pe-data}

**MT Model**
On the one hand, the MT model must achieve *high translation quality* in the selected languages to ensure our experimental setup applies to state-of-the-art proprietary systems.
Still, the MT model should be *open-source* and have a *manageable size* to ensure reproducible findings and enable the computation of uncertainty for the unsupervised setting.
All considered, we use NLLB 3.3B [@nllb-2024-scaling], a widely-used MT model achieving industry-level performances across 200 languages [@moslem-etal-2023-adaptive].

{{< include ../tables/chap-9-qe4pe/_data-stats.qmd >}}

**Data selection**
We begin by selecting two translation directions, English$\rightarrow$Italian and English$\rightarrow$Dutch, according to the availability of professional translators from our industrial partners. We intentionally focus on out-of-English translations as they are generally more challenging for modern MT models [@kocmi-etal-2023-findings]. We aim to identify documents that are manageable for professional translators without domain-specific expertise but still prove challenging for our MT model to ensure a sufficient amount of error spans across modalities. Since original references for our selected translation direction were not available, we do not have a direct mean to compare MT quality in the two languages. However, according to our human MQM assessment in @sec-qe4pe-quality (@tbl-qe4pe-mqm-errors-main-min-maj), NLLB produces a comparable amount of errors across Dutch and Italian MT, suggesting similar quality.

We begin by translating 3,672 multi-segment English documents from the WMT23 General and Biomedical MT shared tasks [@kocmi-etal-2023-findings; @neves-etal-2023-findings] and MT test suites to Dutch and Italian. Our choice for these specialized domains, as opposed to e.g. generic news articles, is driven by the real-world needs of the translation industry for domain-specific post-editing support [@eschbach-dymanus-etal-2024-exploring; @li-etal-2025-transbench]. Moreover, focusing on domains that are considerably more challenging for MT systems than news, as shown by recent WMT campaigns [@neves-etal-2024-findings], ensures a sufficient amount of MT errors to support a sound comparison of word-level QE methods.
Then, XCOMET-XXL is used to produce a first set of segment-level QE scores and word-level error spans for all segments.
To make the study tractable, we further narrow down the selection of documents according to several heuristics to ensure a realistic editing experience and a balanced occurrence of error spans (details in @sec-qe4pe-data-stats). This procedure yields 351 documents, from which we manually select a subset of 64 documents (413 segments, 8,744 source words per post-editor) across two domains:

{{< include ../tables/chap-9-qe4pe/_example-highlights-edits.qmd >}}

- **Social media posts**, including Mastodon posts from the WMT23 General Task [@kocmi-etal-2023-findings] English$\leftrightarrow$German evaluation and Reddit comments from the Robustness Challenge Set for Machine Translation (RoCS-MT; [@bawden-sagot-2023-rocs]), displaying atypical language use, such as slang or acronymization.
- **Biomedical abstracts** extracted from PubMed from the WMT23 Biomedical Translation Task [@neves-etal-2023-findings], including domain-specific terminology.

@tbl-qe4pe-data-stats present statistics for the [Pre]{.smallcaps}, [Main]{.smallcaps} and [Post]{.smallcaps} editing stages, and @tbl-qe4pe-example-highlights-edits shows an example of highlights and edits. While the presence of multiple domains in the same task can render our post-editing setup less realistic, we deem it essential to test the cross-domain validity of our findings.

**Critical Errors** Before producing highlights, we manually introduce 13 critical errors in main task segments to assess post-editing thoroughness. Errors are produced, for example, by negating statements, inverting the polarity of adjectives, inverting numbers, and corrupting acronyms. We replicate the errors in both translation directions to enable direct comparison. Most of these errors were correctly identified across all three highlight modalities (examples in @tbl-qe4pe-critical-errors-examples).

### Participants {#sec-qe4pe-participants}

For both directions, professional translation companies (ANONYMIZED) recruited three translators for the Oracle post-editing stage, the core set of 12 translators working on [Pre]{.smallcaps}, [Main]{.smallcaps} and [Post]{.smallcaps} tasks, and six more translators for the QA stage, for a total of 21 translators per direction.
All translators were freelancers with native proficiency in their target language and self-assessed proficiency of at least C1 in English. Almost all translators had more than two years of professional translation experience and regularly post-edited MT outputs (details in @tbl-qe4pe-questionnaire).

### Editing Interface {#sec-qe4pe-interface}

![An example of the QE4PE [GroTE]{.smallcaps} setup for two segments in an English$\rightarrow$Italian document.](../figures/chap-9-qe4pe/grote_teaser_small_anonymized.pdf){#fig-qe4pe-grote-interface width=80% fig-pos="t"}

We develop a custom interface, which we name [GroTE]{.smallcaps}^[Acronym anonymized.] (@fig-qe4pe-grote-interface), to support editing over texts with word-level highlights. While the MMPE tool used by @shenoy-etal-2021-investigating provide extensive multimodal functionalities [@herbig-etal-2020-mmpe], we aim for a bare-bones setup to avoid confounders in the evaluation. [GroTE]{.smallcaps} is a web interface based on Gradio [@abid-etal-2019-gradio] and hosted on the [Hugging Face Spaces](https://hf.co/spaces) to enable multi-user data collection online.
Upon loading a document, source texts and MT outputs for all segments are presented in two columns following standard industry practices.
For modalities with highlights, the interface provides an informative message and supports the removal of all highlights on a segment via a button, with highlights on words disappearing automatically upon editing, as in [@shenoy-etal-2021-investigating]. The interface supports real-time logging of user actions to analyze the editing process. In particular, we log the start and end times for each edited document, the accessing and exiting of segment textboxes, highlight removals, and individual keystrokes during editing.

[GroTE]{.smallcaps} intentionally lacks standard features such as translation memories, glossaries, and spellchecking to ensure equal familiarity among translators, ultimately controlling for editor proficiency with these tools, as done in previous studies [@shenoy-etal-2021-investigating; @sarti-etal-2022-divemt]. While most translators noted the lack of features in our usability assessment, the majority also found the interface easy to set up, access, and use (@tbl-qe4pe-questionnaire).

## Analysis {#sec-qe4pe-analysis}

### Productivity {#sec-qe4pe-productivity}

![Productivity of post-editors across QE4PE stages ([Pre]{.smallcaps}, [Main]{.smallcaps}, [Post]{.smallcaps}). The → marks outliers and ❌ marks missing data. Each row corresponds to the same three translators across all stages.](../figures/chap-9-qe4pe/productivity_edited.pdf){#fig-qe4pe-productivity width=70% fig-pos="t"}

We obtain segment- and document-level edit times and compute editing *productivity* as the number of processed source characters over the sum of all document-level edit times, measured in characters per minute.
To account for potential breaks taken by post-editors during editing, we filter out pauses between logged actions longer than 5 minutes. We note that this procedure does not significantly impact the overall ranking of translators, while ensuring a more robust evaluation of editing time.

**Do Highlights Make Post-editors Faster?** @fig-qe4pe-productivity shows translators' productivity across stages, with every dot corresponding to the productivity of a single individual. We observe that no highlight modality leads to systematically faster editing across all speed groups and that the ordering of [Pre]{.smallcaps}-task speed groups is maintained in the following stages despite the different highlight modalities. These results suggest that individual variability in editing speed is more critical than highlight modality in predicting editing speed.
However, faster English$\rightarrow$Dutch translators achieve outstanding productivity, i.e. $>2$ standard deviations above the overall mean (outliers with $>300$ char/min, → in @fig-qe4pe-productivity) almost exclusively in [No Highlight]{color="brand-color.nohighlight"}, and, [Oracle]{color="brand-color.oracle"} modalities, suggesting that lower-quality highlights hinder editing speed.

We validate these observations by fitting a negative binomial mixed-effect model on segment-level editing times (model details in @tbl-qe4pe-edit-time-model).
Excluding random factors such as translator and segment identity from the model produces a significant drop in explained variance, confirming the inherent variability of editing times ($R^2 = 0.93 \rightarrow 0.41$).
Model coefficients show that MT output length and the proportion of highlighted characters are the main factors driving an increase in editing times, possibly reflecting an increase in cognitive effort to process additional information.
We find highlights to have a significant impact on increasing the editing speed of English$\rightarrow$Italian translators $(p < 0.001)$, but a minimal impact for English$\rightarrow$Dutch.
Comparing the productivity of the same translator editing with and without highlights ([Main]{.smallcaps} vs [Post]{.smallcaps}), two-thirds of the translators editing with highlights were up to two times slower on biomedical texts. However, the same proportion of translators was up to three times faster on social media texts across both directions.

In summary, we find that **highlight modalities are not predictive of edit times on their own**, but translation direction and domain play an important role in determining the effect of highlights on editing productivity.
We attribute these results to two main factors, which will remain central in the analysis of the following sections: (1) the different *propensity of translators to act upon highlighted issues* in the two tested directions, and (2) the different *nature of errors highlighted across domains*.

### Highlights and Edits {#sec-qe4pe-highlights-edits}

We then examine how highlights are distributed across modalities and how they influence the editing choices of human post-editors.

{{< include ../tables/chap-9-qe4pe/_highlights-edits.qmd >}}

**Agreement Across Modalities** First, we quantify how different modalities agree in terms of highlights' distribution and editing.
We find that highlight overlaps across modalities range between 15% and 39% when comparing highlight modalities in a pairwise fashion, with the highest overlap for English$\rightarrow$Italian social media and English$\rightarrow$Dutch biomedical texts.^[Scores are normalized to account for highlight frequencies across modalities. Agreement is shown in @tbl-qe4pe-highlight-agreement.] Despite the relatively low highlight agreement, we find an average agreement of 73% for post-edited characters across modalities.
This suggests that edits are generally uniform regardless of highlight modalities and are not necessarily restricted to highlighted spans.^[Editing agreement is shown in @fig-qe4pe-edit-agreement.]

**Do Highlights Accurately Identify Potential Issues?** @tbl-qe4pe-highlights-edits (Base Freq.) shows raw highlight and edit frequencies across modalities. We observe different trends across the two language pairs: for English$\rightarrow$Italian, post-editors working with highlights edit more than twice as much as translators with [No Highlight]{color="brand-color.nohighlight"}, regardless of the highlight modality. On the contrary, for English$\rightarrow$Dutch they edit 33% less in the same setting.
These results suggest a different attitude towards acting upon highlighted potential issues across the two translation directions, with English$\rightarrow$Italian translators appearing to be conditioned to edit more when highlights are present.
We introduce four metrics to quantify highlights-edits overlap:

- $P(E|H)$ and $P(H|E)$, reflecting highlights' *precision* and *recall* in predicting edits, respectively.
- $\Lambda_E\,{\stackrel{\text{\tiny def}}{=}}\,P(E|H)/P(E|\neg H)$ shows how much more likely an edit is to fall within rather than outside highlighted characters.
- $\Lambda_H\,{\stackrel{\text{\tiny def}}{=}}\,P(H|E)/P(H|\neg E)$ shows how much more likely it is for a highlight to mark edited rather than unmodified spans.

Intuitively, character-level recall $P(H|E)$ should be more indicative of highlight quality compared to precision $P(E|H)$, provided that word-level highlights can be useful even when not minimal.^[For example, if the fully-highlighted word *tradutt~~ore~~* is changed to its feminine version *tradutt~~rice~~*, $P(H|E) = 1$ (edit correctly and fully predicted) but $P(E|H) = 0.3$ since word stem characters are left unchanged.]
@tbl-qe4pe-highlights-edits (Measured) shows metric values across the three highlight modalities (breakdowns by domain and speed shown in @tbl-qe4pe-edit-highlights-stats-domain-modality and @tbl-qe4pe-edit-highlights-stats-domain-speed).
As expected, [Oracle]{color="brand-color.oracle"} highlights obtain the best performance in terms of precision and recall, with $P(H|E)$, in particular, being significantly higher than the other two modalities across both directions.

Surprisingly, **we find no significant precision and recall differences between [Oracle]{color="brand-color.supervised"} and [Unsupervised]{color="brand-color.unsupervised"} highlights**, despite the word-level QE training of XCOMET used in the former modality. Moreover, they support the potential of unsupervised, model internals-based techniques to complement or substitute more expensive supervised approaches. Still, likelihood ratios $\Lambda_E, \Lambda_H \gg  1$ for all modalities and directions indicate that highlights are 2-4 times more likely to precisely and comprehensively encompass edits than non-highlighted texts. This suggests that even imperfect highlights that do not reach [Oracle]{color="brand-color.oracle"}-level quality might effectively direct editing efforts toward potential issues. We validate these observations by fitting a zero-inflated negative binomial mixed-effects model to predict segment-level edit rates.
Results confirm a significantly higher edit rate for English$\rightarrow$Italian highlighted modalities and the social media domain with $p<0.001$ (features and significances shown in Appendix @tbl-qe4pe-edit-rate-model). We find a significant zero inflation associated with translator identity, suggesting the choice of leaving MT outputs unedited is highly subjective.

[Do Highlights Influence Editing Choices?]{.paragraph}
Since in @sec-qe4pe-productivity we found the proportion of highlighted characters to impact the editing rate of translators, we question whether the relatively high $P(E|H)$ and $P(H|E)$ values might be artificially inflated by the eagerness of translators to intervene on highlighted spans. In other words, *do highlights identify actual issues, or do they condition translators to edit when they otherwise would not?* To answer this, we propose to *project* highlights from a selected modality---in which highlights were shown during editing---onto the edits performed by the [No Highlight]{color="brand-color.nohighlight"} translators on the same segments.
The resulting difference between measured and projected metrics can then be taken as an estimate for the impact of highlight presentation on their resulting accuracy.

To further ensure the soundness of our analysis, we use a set of projected [Random]{color="brand-color.random"} highlights as a lower bound for highlight performance. To make the comparison fair, [Random]{color="brand-color.random"} highlights are created by randomly highlighting words in MT outputs matching the average word-level highlight frequency across all highlighted modalities given the current domain and translation direction.
@tbl-qe4pe-highlights-edits (Projected) shows results for the three highlighted modalities. First, all projected metrics remain consistently above the [Random]{color="brand-color.random"} baseline, suggesting a higher-than-chance ability to identify errors even for worst-performing highlight modalities. Projected precision scores $P(E|H)$ depend on edit frequency, and hence see a major decrease for English$\rightarrow$Italian, where the [No Highlight]{color="brand-color.nohighlight"} edit rate $P(E)$ is much lower. However, the increase in $\Lambda_E$ across all English$\rightarrow$Italian modalities confirms that, despite the lower edit proportion, highlighted texts remain notably more likely to be edited than non-highlighted ones.
Conversely, the lower $\Lambda_E$, $P(H|E)$ and $\Lambda_H$ for English$\rightarrow$Dutch show that edits become much less skewed towards highlighted spans in this direction when accounting for presentation bias.

Overall, while the presence of highlights makes English$\rightarrow$Italian translators more likely to intervene in MT outputs, their location in the MT output often pinpoints issues that would be edited regardless of highlighting. English$\rightarrow$Dutch translators, on the contrary, intervene at roughly the same rate regardless of highlights presence, but their edits are focused mainly on highlighted spans when they are present. This difference is consistent across all subjects in the two directions despite the identical setup and comparable MT and QE quality across languages. This suggests that cultural factors might play a non-trivial role in determining the usability and influence of QE methods regardless of span accuracy, a phenomenon previously observed in human-AI interaction studies @ge-etal-2024-culture.

### Quality Assessment {#sec-qe4pe-quality}

We continue our assessment by inspecting the quality of MT and post-edited outputs along three dimensions.
First, we use XCOMET segment-level QE ratings as an automatic approximation of quality and compare them to human-annotated quality scores collected in the last phase of our study. For efficiency, these are obtained for the 0-100 Direct Assessment scale commonly used in QE evaluation [@specia-etal-2020-findings-wmt], but following an initial step of MQM error annotation to condition scoring on found errors, as prescribed by the ESA protocol [@kocmi-etal-2024-error]. Then, MQM error span annotations are used to analyze the distribution of error categories. Finally, we manually assess critical errors, which were inserted to quantify highlight modalities effect on unambiguous issues.

**Do Highlights Influence Post-Editing Quality?**
In this stage, we focus particularly on *edited quality improvements*, i.e. how post-editing the same MT outputs under different highlight conditions influences the resulting quality of translations. We operationalize this assessment using human ratings and automatic metrics to score MT and post-edited translations, using their difference as the effective quality gain after the post-editing stage. Scores for this metric are generally positive, i.e. human post-editing improves quality, and bounded by the maximal achievable quality gain given the initial MT quality.
@fig-qe4pe-quality shows median improvement values across quality bins defined from the distribution of initial MT quality scores (shown in histograms), in which all post-edited versions of each MT output appear as separate observations. Positive median scores confirm that post-edits generally lead to quality improvements across all tested settings. However, we observe different trends across the two metrics: across both domains, XCOMET greatly underestimates the human-assessed ESA quality improvement, especially for biomedical texts where it shows negligible improvement regardless of the initial MT quality.
These results echo recent findings cautioning users against the poor performance of trained MT metrics for unseen domains and high-quality translations [@agrawal-etal-2024-automatic-metrics; @zouhar-etal-2024-fine].
Focusing on the more reliable ESA scores, we observe large quality improvements from post-editing, as shown by near-maximal quality gains across most bins and highlight modalities.
While [No Highlight]{color="brand-color.nohighlight"} seems to underperform other modalities in the social media domain, the lack of more notable differences in gains across highlight modalities suggests that **highlights' quality impact might not be evident in terms of segment-level quality**, motivating our next steps in the quality analysis.

![Median quality improvement for post-edited segments at various initial MT quality levels across domains and highlight modalities. Quality scores are estimated using XCOMET segment-level QE (top) and professional ESA annotations (bottom). Histograms show example counts across quality bins for the two metrics. Dotted lines show upper bounds for quality improvements given starting MT quality.](../figures/chap-9-qe4pe/quality_edited_horizontal.pdf){#fig-qe4pe-quality width=100% fig-pos="t"}

We also find no clear relationship between translator speed and edited quality improvements, suggesting that higher productivity does not come at a cost for faster translators (@fig-qe4pe-quality-across-speed). This finding confirms that neglecting errors is not the cause of different editing patterns from previous sections.

**Which Error Types Do Highlights Identify?** @tbl-qe4pe-mqm-errors-main-min-maj shows a breakdown of MQM annotations for MT and all highlight modalities using the *Accuracy*, *Style* and *Linguistic* macro-categories of MQM errors.^[Full micro-category breakdown in @tbl-qe4pe-critical-errors, per-domain breakdown in @fig-qe4pe-quality-mqm. Category descriptions in @tbl-qe4pe-qa-guidelines.] At this granularity, differences across modalities become visible, with overall error counts showing a clear relation to $\Lambda_E$ from @tbl-qe4pe-highlights-edits ([Oracle]{color="brand-color.oracle"} being remarkably better for English$\rightarrow$Italian, with milder and more uniform trends in English$\rightarrow$Dutch). At least for English$\rightarrow$Italian, these results confirm that an observable quality improvement from editing with highlights is present in the best-case [Oracle]{color="brand-color.oracle"} scenario. By contrast, for English$\rightarrow$Dutch the [Unsupervised]{color="brand-color.unsupervised"} method is found to outperform even the [Oracle]{color="brand-color.oracle"} setting in reducing the amount of errors, while it fares relatively poorly for English$\rightarrow$Italian. We also note a different distribution of Accuracy and Style errors, with the former being more common in biomedical texts while the latter appearing more often for translated social media posts (@fig-qe4pe-quality-mqm). We posit that differences in error types across domains might explain the opposite productivity trends observed in @sec-qe4pe-productivity: while highlighted accuracy errors might lead to time-consuming terminology verification in biomedical texts, style errors might be corrected more quickly and naturally in the social media domain.

{{< include ../tables/chap-9-qe4pe/_mqm-errors-main-min-maj.qmd >}}

**Do Highlights Detect Critical Errors?** We examine whether the critical errors we inserted were detected by different modalities, finding that while most modalities fare decently with more than 62% of critical errors highlighted, [Unsupervised]{color="brand-color.unsupervised"} is the only setting for which all errors are correctly highlighted across both directions. Then, critical errors are manually verified in all outputs, finding that 16-20% more critical errors are edited in highlighted modalities compared to [No Highlight]{color="brand-color.nohighlight"} (full results in @tbl-qe4pe-critical-errors).
Hence, **highlights might lead to narrow but tangible quality improvements that can go undetected in coarse quality assessments**, and finer-grained evaluations might be needed to quantify future improvements in word-level QE quality.

### Usability {#sec-qe4pe-usability}

In post-task questionnaire answers (@tbl-qe4pe-questionnaire), most translators stated that MT outputs had average-to-high quality and that provided texts were challenging to translate. Highlights were generally found decently accurate, but they were generally not found useful to improve either productivity or quality (including [Oracle]{color="brand-color.oracle"} ones). Interestingly, despite the convincing gains for critical errors measured in the last section, most translators stated that highlights did not influence their editing and did not help them identify errors that would have otherwise been missed. Concretely, this suggests that the potential quality improvements might not be easily perceived by translators and might have secondary importance compared to the extra cognitive load elicited by highlighted spans. When asked to comment about highlights, several translators called them *"more of an eye distraction, as they often weren't actual mistakes"* and *"not quite accurate enough to rely on them as a suggestion"*. Some translators also stated that missed errors led them to *"disregarding the highlights to focus on checking each sentence"*. Despite their high quality, only one editor working with [Oracle]{color="brand-color.oracle"} highlights found highlights helpful in *"making the editing process faster and somehow easier"*. Taken together, these comments convincingly point to a negative perception of the quality and usefulness of highlights, suggesting that **improvement in QE accuracy may not be sufficient to improve QE usefulness** in editors' eyes.

{{< include ../tables/chap-9-qe4pe/_questionnaire.qmd >}}

## Conclusion {#sec-qe4pe-conclusion}

This study evaluated the impact of various error-span highlighting modalities, including automatic and human-made ones, on the productivity and quality of human post-editing in a realistic professional setting. Our findings highlight the importance of domain, language and editors' speed in determining highlights' effect on productivity and quality, underscoring the need for broad evaluations encompassing diverse settings. The limited gains of human-made highlights over automatic QE and their indistinguishable perception from editors' assessment indicate that further gains in the accuracy of these techniques might not be the determining factor in improving their integration into post-editing workflows. In particular, future work might explore other directions to further assess and improve the usability of word-level QE highlights, for example, studying their impact on non-professional translators and language learners or combining them with edit suggestions to justify the presence of error spans.

## Limitations {#sec-qe4pe-limitations}

Our study presents certain limitations that warrant consideration when interpreting its findings and for guiding future research.

Firstly, while we included two domains and translation directions to improve the generalizability of our findings, our results suggest that language and domain play an important role in defining the effectiveness of word-level QE for human post-editing. While we observed mild gains from word-level QE on our tested mid-resourced translation directions (English$\rightarrow$Italian and English$\rightarrow$Dutch), we expect limited, if any, benefit of such approaches in low-resource languages and domains for which MT systems and QE methods are likely to underperform [@sarti-etal-2022-divemt; @zouhar-etal-2024-fine]. Furthermore, the domains tested in our study (biomedical and social media posts) provided concrete challenges in the form of specialized terminology and idiomatic expressions, respectively, which are known to hinder the quality of MT outputs [@neves-etal-2024-findings; @bawden-sagot-2023-rocs]. While future work should ensure our findings can be extended to other domains and languages, the limited benefits brought by the tested word-level QE methods in challenging settings suggest a limited usefulness for higher-resource languages and more standard domains such as news or Wiki texts.

Secondly, we acknowledge that several design choices in our evaluation setup, rather than pertaining to the QE methods themselves, may have influenced our results. These include, for instance, the specific procedure for discretizing continuous scores from the [Unsupervised]{color="brand-color.unsupervised"} method into error spans, and the method of obtaining [Oracle]{color="brand-color.oracle"} highlights via majority voting among post-editors. While we believe these choices are justified within the context of our study, their impact on the outcomes cannot be entirely discounted. Future studies might benefit from a more fine-grained assessment of how such low-level decisions influence the perceived accuracy and usability of word-level QE.

Finally, subjective factors such as the translators' inherent propensity to edit, their prior opinions on the role of MT in post-editing, and their individual editing styles inevitably influenced both quantitative and qualitative assessments in this study. Although we attempted to mitigate these effects by ensuring a uniform and controlled evaluation setup for all 42 professional translators and by employing averaged judgments for translators working on the same highlight modality, we acknowledge that post-editor subjectivity might limit the reproducibility of our findings.

## Broader Impact and Ethical Considerations {#sec-qe4pe-broader-impact}

Our study explicitly centers the experience of professional translators, responding to recent calls for user-centered evaluation of translation technologies. By prioritizing translators' perspectives and productivity, we aim to contribute to methods that complement rather than replace human expertise.
Our findings highlight a gap between user perception and measured quality improvements, suggesting that future efforts should focus primarily on improving the usability of these methods in editing interfaces. In particular, new assistive approaches for post-editing should not only strive to increase productivity but rather reduce the cognitive burden associated with post-editing work. This insight is crucial for designing more user-centered quality estimation tools that genuinely support human work. Ultimately, our results suggest that subjective norms across different domains and cultures play an important role in determining the effectiveness of proposed methodologies, underscoring the importance of accounting for human factors when designing such evaluations. All participants in this study were professional translators who provided informed consent. The research protocol ensured anonymity and voluntary participation, with translators recruited and remunerated through professional translation providers. The study's open data release further promotes transparency, enabling other researchers to reproduce and build upon our findings.
