::: {.content-visible when-format="html"}
::: {#thesis-abstract}
<h1 style="font-size: 18pt">Abstract</h1>

While neural network-based language models have proven remarkably effective for natural language processing tasks, their internal mechanisms remain largely opaque to researchers and practitioners alike. Recent advances in interpretability research began to shed light on how these systems work, particularly in identifying model subnetworks responsible for specific tasks and understanding how knowledge is stored and employed by models. However, interpretability research has faced criticism for rarely producing actionable outputs that would meaningfully improve the design or usage of language models in practice. This thesis investigates the inner workings of neural machine translation (MT) systems through the lens of interpretability, developing novel methods to understand, control, and integrate these systems into human translation workflows. Our research addresses fundamental questions about how language models leverage contextual information, how their generation processes can be steered for personalization, and how interpretability insights can enhance professional translation practices.

The first part establishes a comprehensive infrastructure and methodological framework for analyzing context usage in language models and machine translation systems. We begin by introducing Inseq, an open-source toolkit designed to democratize access to interactive analyses of language models' behaviors, providing a simple unified interface to state-of-the-art input attribution interpretability methods. Building on this foundation, we develop [PECoRe]{.smallcaps}, an end-to-end framework employing contrastive input attribution to faithfully quantify how language models exploit contextual information throughout generation, and apply it to study context influence in context-aware machine translation. We further extend our investigation to retrieval-augmented generation with large language models, demonstrating that model internals can produce faithful high-quality citations to context paragraphs for open-book question answering across multiple languages.

The second part shifts the focus from analysis to intervention, exploring two complementary paradigms for conditioning machine translation outputs to meet user-specific requirements. We first present [Ramp]{.smallcaps}, a retrieval-augmented prompting technique using semantic similarity and explicit attribute marking to achieve attribute-controlled translation. We then advance to more sophisticated personalization through direct intervention on model representations, employing sparse autoencoders to steer translations toward individual translator styles in the challenging domain of literary translation. Our comparative analysis reveals that successful prompting and interpretability-based steering approaches converge to similar mechanistic solutions, uncovering fundamental principles of generation conditioning in neural language models.

The final part explores how insights derived from model inner workings can inform human editors in real-world translation workflows, through comprehensive cross-lingual user studies involving professional translators. We first present [DivEMT]{.smallcaps}, the first publicly available cross-lingual post-editing dataset spanning six typologically diverse languages, showing that traditional MT quality metrics fail to correlate with actual post-editing productivity and that language relatedness significantly influences editing efficiency. Building on these insights, our second study QE4PE investigates how word-level error highlights---including those generated by unsupervised uncertainty-based methods---impact translator productivity and output quality in realistic editing scenarios. Our evaluation of unsupervised quality estimation methods demonstrates that approaches based on model internals can outperform supervised baselines in downstream usability, highlighting the importance of method calibration and multiple quality annotations to account for human label variation.

Overall, this dissertation advances the field of machine translation interpretability across three dimensions: it develops accessible tools and frameworks for understanding context usage in generative models, demonstrates how interpretability methods can enable fine-grained control over translation outputs, and establishes empirical evidence for the usage of language model internals professional translation workflows. These contributions lay the groundwork for more transparent, controllable, and human-centered translation systems, with the potential to enhance the productivity and effectiveness of human translators in their interactions with AI systems.
:::
:::

# Introduction {#sec-chap-1-introduction}

In recent years, language models have undergone a significant transformation, going from simple research prototypes producing barely coherent text to becoming a cornerstone of modern technological infrastructure. This success stems in large part from the remarkable ability of large neural networks such as the transformer [@vaswani-etal-2017-attention] to learn rich representations of language---and by extension, our world and society---from staggering amounts of text. Yet, the complex and deeply intertwined structure that renders these systems so powerful is also the main culprit behind their opacity. The inner workings of neural networks remain notoriously difficult to interpret, and the lack of transparency in their decision-making processes has raised serious concerns about their reliability and fairness in high-stakes applications [@rudin-2019-stop].

These circumstances have led to a growing interest in *interpretability*--- a field closely aligned with the broader area of explainable artificial intelligence (XAI), which seeks to develop methods and tools to understand how neural networks work and provide insights into their decision-making processes [@doshivelez-kim-2017-towards;@li-etal-2022-interpretable]. In natural language processing (NLP), interpretability research has made significant strides by uncovering how language models encode and process factual knowledge and linguistic information [@tenney-etal-2019-bert;@belinkov-2022-probing;@meng-etal-2022-rome], revealing their use of context during generation [@clark-etal-2019-bert;@ferrando-etal-2022-measuring] and identifying the learned mechanisms underlying their capabilities [@elhage-etal-2021-mathematical;@saphra-wiegreffe-2024-mechanistic]. 

While interpretability insights have earned broad recognition and influence within the NLP research community [@mosbach-etal-2024-insights], critics have often pointed out that these findings rarely translate into actionable improvements for real-world systems [@rauker-etal-2023-toward;@rai-etal-2024-practical;@hendrycks-hiscott-2025-misguided]. Most interpretability work today focuses on identifying subnetworks and mechanisms responsible for specific tasks inside language models [@ferrando-etal-2024-primer;@sharkey-etal-2025-open], yet few studies have put interpretability insights in relation to end-users' needs and desires [@ehsan-etal-2021-expanding], despite their crucial role in determining the practical usefulness of interpretability findings [@ehsan-etal-2024-who]. This disconnect stems from a fundamental divide between research communities: most AI interpretability researchers pursue theoretical understanding of complex systems, while human-computer interaction (HCI) researchers prioritize actionable insights and practical applications.

A prime example of this disconnect can be found in the field of machine translation (MT), a long-standing area of research within NLP. MT researchers pioneered the use of neural language models for sequence generation tasks [@sutskever-etal-2014-sequence;@bahdanau-etal-2015-neural], and were among the first to analyze their inner workings [@belinkov-etal-2017-neural;@voita-etal-2019-analyzing;@rogers-etal-2020-primer]. Yet, despite the significant progress in the performance of MT systems across hundreds of languages over the past decade, the field has been remarkably slow to bring interpretability insights to the users of these systems, especially in the case of professional translators who work with these systems on a daily basis. Users of "classic" translation tools such as Google Translate are, to this day, simply presented with translations, without the possibility to personalize their tone or properties, quantify the model uncertainty in its response, or identify potential errors or alternative formulations. At the other extreme, when large language models like GPT-4 [@openai-2023-gpt4] eagerly offer eloquent justifications alongside their translations, these explanations may sound plausible but often fail to reflect the model's actual processing and context usage, resulting in plausible yet unfaithful rationalizations [@turpin-etal-2023-language].

This dissertation aims to bridge the gap between method-centric interpretability research and outcome-centric real-world machine translation applications. We develop novel methods to understand and control language model generation, then study how to integrate these advances effectively into human translation workflows. Our research spans three interconnected macro-themes: (1) understanding how language models exploit contextual information during generation, (2) controlling model generation for personalized translation outputs, and (3) integrating interpretability insights into human translation workflows. Our methodological contributions, empirical evaluations, and user studies demonstrate how insights from interpretability research can lead to meaningful impact in the way machine translation systems are used in real-world translation workflows.

## Outline and Contributions

The experimental chapters of this dissertation are organized into three parts, each addressing one of the research directions outlined above. Each part is composed of multiple chapters, each presenting a self-contained contribution or study related to the overarching theme. @fig-chap1-chapter-guide provides a visual overview of parts and chapters, highlighting for each chapter the topics introduced in detail in @sec-chap-2-background. Below, we summarize the contents, research questions and contributions of each part.

![Chapter guide for the three parts of this dissertation.](figures/index/chapter_guide.pdf){#fig-chap1-chapter-guide fig-pos="t"}

\vspace{7pt}
**Part I: Attributing Context Usage in Multilingual NLP**

Part I establishes the foundational infrastructure and methodological frameworks for understanding how neural language models and machine translation systems process contextual information during generation. We begin with Inseq (@sec-chap-3-inseq), a toolkit that democratizes access to interpretability analyses of generative language models, providing the foundation for our investigations into context usage. Then, @sec-chap-4-pecore introduces [PECoRe]{.smallcaps}, a data-driven framework for quantifying the plausibility of context usage in language models through the contrastive identification of context-sensitive tokens and contextual cues that influence their prediction. [PECoRe]{.smallcaps} is used to study context usage in context-aware machine translation systems, identifying failure cases stemming from an incorrect usage of context. @sec-chap-5-mirage extends this analysis to modern large language models and retrieval-augmented generation settings with [Mirage]{.smallcaps}, adapting the PECoRe framework to demonstrate how model internals enable faithful answer attribution in question answering. This part addresses two fundamental research questions:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 1 (RQ1)

What are the conceptual and technical requirements for interpretability software tools enabling scalable and reproducible analyses into the inner workings of generative language models?
:::

::: {.callout-tip icon="false"}

## ‚ùì Research Question 2 (RQ2)

How do language models and machine translation systems exploit contextual information during generation, and how can we quantify this usage in a faithful manner?
:::

Part I's primary contributions include: (1) two open-source releases of the Inseq interpretability library; (2) the contrastive attribution tracing (CAT) method, a gradient-based alternative to causal intervention for efficiently identifying salient model components; (3) the PECoRe framework for context reliance attribution in language models, enabling data-driven exploration of context usage patterns in context-aware MT systems; and (4) an extended evaluation of context attribution for retrieval-augmented generation using [Mirage]{.smallcaps}, producing high quality citations of retrieved documents while ensuring greater faithfulness to the model's reasoning process.

\vspace{7pt}
**Part II: Conditioning Generation for Personalized Machine Translation**

Part II moves from understanding context usage to actively controlling model generation for customized translation outputs. Across two chapters, we explore two paradigms to condition machine translation outputs---prompting-based methods and direct interventions in model processing---addressing the question:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 3 (RQ3)

Are interpretability-based steering methods viable approaches for controllable machine translation? How do they compare with prompting-based methods in terms of their performance and their impact on models' internal mechanisms?
:::

@sec-chap-6-ramp pioneers the usage of prompting-based strategies for attribute-controlled translation, while @sec-chap-7-sae-litmt connects generation conditioning to interpretability techniques, expanding the scope of our analysis from simple attributes in common domains to sophisticated personal styles in the challenging literary translation domain.

The core contributions of Part II include: (1) [Ramp]{.smallcaps}, a novel prompting methodology achieving strong performance in attribute-controlled translation across multiple languages and attributes without model fine-tuning; (2) the first comprehensive comparison of prompting versus interpretability-based steering for machine translation personalization; (3) a novel contrastive steering method using sparse autoencoder latents to achieve personalization accuracy comparable to prompting while preserving quality in literary translation; and (4) evidence that prompting and steering methods converge to similar mechanistic solutions, revealing fundamental principles of generation conditioning.

\vspace{7pt}
**Part III: Interpretability in Human Translation Workflows**

Part III evaluates how interpretability insights can benefit human professionals who edit machine-translated content in a practical sense. We begin with [DivEMT]{.smallcaps} (@sec-chap-8-divemt), a study investigating the effectiveness of professional MT post-editing across a diverse set of mid-resourced languages, going beyond the one-size-fits-all analysis of high-resourced translation directions. This allows us to establish our human evaluation setup, providing valuable insights into the question:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 4 (RQ4)

Does MT contribute positively to the productivity of professional translators across different languages? Which factors influence its effectiveness?
:::

Building upon these insights, our second large-scale study QE4PE (@sec-chap-9-qe4pe) investigates how word-level error span highlights---including those derived from MT systems' uncertainty during generation---impact the productivity of professional translators and the quality of post-edited contents:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 5 (RQ5)

How do word-level error highlights impact the productivity and editing choices of professional translators and the quality of resulting translations?
:::

@sec-chap-10-unsup-wqe concludes our human-centered investigation with a deeper analysis of multiple uncertainty and interpretability-based word-level quality estimation methods. Such analysis allows us to assess how the performance of such techniques varies across different models, languages and human annotators:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 6 (RQ6)

Can unsupervised error span detection methods reliably identify problems in machine-translated outputs? How does human label variation affect their performance, compared to traditional supervised approaches?
:::

Part III contributions include (1) [DivEMT]{.smallcaps}, a cross-lingual post-editing dataset enabling controlled comparison of translator productivity across editing modalities and typologically diverse languages; (2) evidence that MT quality metrics fail to correlate with human post-editing productivity across languages, with productivity being heavily influenced by source-target language relatedness; (3) QE4PE, a comprehensive post-editing dataset containing error spans, behavioral editing metrics, and quality annotations from 42 professional post-editors for two translation directions;  (4) evidence that error span highlights may reduce productivity but improve critical error detection; and (5) evidence that unsupervised quality estimation methods based on model internals can match state-of-the-art supervised approaches in both accuracy and downstream usability, revealing how subjective editing choice impact the evaluation of error span detection methods.

## Scientific Output

This dissertation is the product of several research articles and open-source projects, which are categorized in the following sections.

### Main Publications

The following articles represent the main contributions reflected in this thesis' experimental chapters, organized in their respective parts:[^1]

[^1]: Shared first co-authorship is indicated by $^\dagger$.

\vspace{5pt}
**Introduction and Background**

- Ferrando, J., **Sarti, G.**, Bisazza, A. and Costa-juss√†, M. R. [-@ferrando-etal-2024-primer]. A Primer on the Inner Workings of Transformer-based Language Models. *Arxiv Preprint* (**@sec-chap-2-background**)

\vspace{5pt}
**Part I: Attributing Context Usage in Multilingual NLP**

- **Sarti, G.**, Feldhus, N., Sickert, L., van der Wal, O., Nissim, M. and Bisazza, A. [-@sarti-etal-2023-inseq-fixed]. Inseq: An Interpretability Toolkit for Sequence Generation Models. In *Proc. of the 61st Annual Meeting of the Association for Computational Linguistics (ACL Demo)* (**@sec-chap-3-inseq**)

- **Sarti, G.**, Feldhus, N., Qi, J., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-democratizing]. Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit. In *Proc. of the 2nd World Conference on eXplainable Artificial Intelligence: Late-breaking works and demos (xAI Demo)* (**@sec-chap-3-inseq** and **@sec-chap-4-pecore**)

- **Sarti, G.**, Chrupa≈Ça G., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-quantifying]. Quantifying the Plausibility of Context Reliance in Neural Machine Translation. In *Proc. of the 12th International Conference on Learning Representations (ICLR)* (**@sec-chap-4-pecore**)

- Qi, J.$^\dagger$, **Sarti, G.**$^\dagger$, Fern√°ndez, R. and Bisazza, A. [-@qi-sarti-etal-2024-model]. Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation. In *Proc. of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (**@sec-chap-5-mirage**)

**Part II: Conditioning Generation for Personalized Machine Translation**

- **Sarti, G.**, Htut, P. M., Niu, X., Hsu, B., Currey, A., Dinu, G. and Nadejde, M. [-@sarti-etal-2023-ramp]. RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation. In *Proc. of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)* (**@sec-chap-6-ramp**)

- Scalena, D.$^\dagger$, **Sarti, G.**$^\dagger$, Bisazza, A., Fersini, E. and Nissim, M. [-@scalena-sarti-etal-2025-steering]. Steering Large Language Models for Machine Translation Personalization. *Arxiv Preprint* (**@sec-chap-7-sae-litmt**)

**Part III: Interpretability in Human Translation Workflows**

- **Sarti, G.**, Bisazza, A., Guerberof-Arenas, A. and Toral, A. [-@sarti-etal-2022-divemt]. DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. In *Proc. of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (**@sec-chap-8-divemt**)

- **Sarti, G.**, Zouhar, V., Chrupa≈Ça, G., Guerberof-Arenas, A., Nissim, M. and Bisazza, A. [-@sarti-etal-2025-qe4pe]. QE4PE: Word-level Quality Estimation for Human Post-Editing. *Transactions of the Association for Computational Linguistics (TACL)* (**@sec-chap-9-qe4pe**)

- **Sarti, G.**, Zouhar, V., Nissim, M. and Bisazza, A. [-@sarti-etal-2025-unsupervised]. Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement. In *Proc. of the 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (**@sec-chap-10-unsup-wqe**)

I led the conceptualization, implementation, experimental evaluation, and manuscript writing for each article for which I am the sole first author. For articles with shared first authorship, I co-led the conceptualization, experimental design, and manuscript writing. In @qi-sarti-etal-2024-model, I implemented the API for experimental evaluation. The background in @sec-chap-2-background adapts parts of our primer on transformer interpretability [@ferrando-etal-2024-primer], for which I contributed by surveying the literature and writing content regarding transformer architecture, input attribution methods, steering approaches, and interpretability tools.

### Open-source Contributions

Open-source software proved fundamental to this thesis, providing a solid foundation for conducting reproducible experimental work. Notably, all investigations we conducted employed solely open-source tools, models and datasets, despite the current popularity of proprietary language models. Each chapter provides links to all datasets, models, code, and demos to encourage scrutiny and foster further research.

My most notable contribution to the open-source research ecosystem is the **Inseq** toolkit, presented in @sec-chap-3-inseq, for which I serve as development lead. The library now counts 430+ Github stars and 80+ citations across international venues.

I also contributed to the development of the following open-source projects:

- The **Groningen Translation Environment** ([GroTE]{.smallcaps}), a Gradio-based UI for machine translation post-editing supporting the live recording of behavioral logs using the Hugging Face `datasets` hub and `spaces` ecosystem, developed with the help of Vil√©m Zouhar for the QE4PE study (@sec-chap-9-qe4pe). Available at <https://github.com/gsarti/grote> or via `pip install grote`.

- `gradio-highlightedtextbox`, a Svelte component for Gradio supporting text editing with highlighted spans, developed for collecting behavioral edit data in [GroTE]{.smallcaps}. Available at <https://huggingface.co/spaces/gsarti/gradio_highlightedtextbox> or via `pip install gradio-highlightedtextbox`.

- `labl`, a toolkit to facilitate token-level analyses of annotated texts with multiple edits and tokenization schemes, developed with the help of Vil√©m Zouhar for @sec-chap-10-unsup-wqe analyses. Available at <https://github.com/gsarti/labl> or via `pip install labl`.

- **Interpreto**, a Python toolbox for concept-based interpretability analyses of language models maintained by the [FOR](https://www.irt-saintexupery.com/for-program/)/[DEEL](https://www.deel.ai/) teams, which I helped design and develop as part of my visit to the IRT Saint Exup√©ry research institute in Toulouse, France. Interpreto is available at <https://github.com/FOR-sight-ai/interpreto> or via `pip install interpreto`.

The full set of open-source contributions, including demos, models, and datasets, are available on [GitHub](https://github.com/gsarti) and [ü§ó Hugging Face](https://huggingface.co/gsarti).

### Other Research Contributions

Beyond this dissertation's scope, my research output included projects organized around two main themes:

\vspace{5pt}
**Advancing Italian natural language processing:**

- Miaschi, A., **Sarti, G.**, Brunato, D., Dell'Orletta, F. and Venturi, G. [-@miaschi-etal-2022-probing]. Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties. *Italian Journal of Computational Linguistics (IJCoL)*

- Bianchi, F., Attanasio, G., Pisoni, R., Terragni, S., **Sarti, G.** and Balestri, D. [-@bianchi-etal-2023-contrastive]. Contrastive Language-Image Pre-training for the Italian Language. In *Proc. of the 9th Italian Conference on Computational Linguistics (CLiC-it)*

- **Sarti, G.** and Nissim, M. [-@sarti-nissim-2024-it5]. IT5: Text-to-text Pretraining for Italian Language Understanding and Generation. In *Proc. of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)*

- **Sarti, G.**, Caselli, T., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-non]. Non Verbis, Sed Rebus: Large Language Models Are Weak Solvers of Italian Rebuses. In *Proc. of the 10th Italian Conference on Computational Linguistics (CLiC-it)*

- **Sarti, G.**, Caselli, T., Bisazza, A. and Nissim, M. [-@sarti-etal-2024-eurekarebus]. EurekaRebus - Verbalized Rebus Solving with LLMs: A CALAMITA Challenge. In *Proc. of the 10th Italian Conference on Computational Linguistics (CLiC-it)*

- Ciaccio, C., **Sarti, G.**, Miaschi, A. and Dell'Orletta, F. [-@ciaccio-etal-2025-crossword]. Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond. In *Proc. of the 11th Italian Conference on Computational Linguistics (CLiC-it)*

\noindent
**Interpreting the inner workings of generative language models:**

- Langedijk, A., Mohebbi, H., **Sarti, G.**, Zuidema, W. and Jumelet, J. [-@langedijk-etal-2024-decoderlens]. DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. In *Findings of the North American Chapter of the Association for Computational Linguistics (NAACL Findings)*

- Edman, L., **Sarti, G.**, Toral, A., van Noord, G. and Bisazza, A. [-@edman-etal-2024-character]. Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation. *Trans. of the Association for Computational Linguistics (TACL)*

- Scalena, D., **Sarti, G.** and Nissim, M. [-@scalena-etal-2024-multi]. Multi-property Steering of Large Language Models with Dynamic Activation Composition. In *Proc. of the 7th Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP)*

- Ghasemi Madani, M. R., Gema, A. P., **Sarti, G.**, Zhao, Y., Minervini, P. and Passerini, A. [-@ghasemi-madani-etal-2025-noiser]. Noiser: Bounded Input Perturbations for Attributing Large Language Models. In *Proc. of the Second Conference on Language Modeling (CoLM)*

- Candussio, S., Saveri, G., **Sarti, G.** and Bortolussi, L. [-@candussio-etal-2025-bridging]. Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers. In *Proc. of the European Conference on Machine Learning and Principles of Knowledge Discovery in Databases (ECML-PKDD)*

- Islam, K. I. and **Sarti, G.** [-@islam-sarti-2025-reveal]. Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation. *Arxiv Preprint*.

I also had the privilege of organizing the BlackboxNLP 2025 workshop^[<https://blackboxnlp.github.io/2025>]---the leading venue for NLP interpretability work---and its shared task on benchmarking mechanistic interpretability methods for circuit localization and causal variable identification in large language models.