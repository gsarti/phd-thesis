::: {.content-visible when-format="html"}
::: {#thesis-abstract}
<h1 style="font-size: 18pt">Abstract</h1>

While neural network-based language models have proven remarkably effective for natural language processing tasks, their internal mechanisms remain largely opaque to researchers and practitioners alike. Recent advances in interpretability research began to shed light on how these systems work, particularly in identifying model subnetworks responsible for specific tasks and understanding how knowledge is stored and employed by models. However, interpretability research has faced criticism for rarely producing actionable outputs that would meaningfully improve the design or usage of language models in practice. This thesis investigates the inner workings of neural machine translation (MT) systems through the lens of interpretability, developing novel methods to understand, control, and integrate these systems into human translation workflows. Our research addresses fundamental questions about how language models leverage contextual information, how their generation processes can be steered for personalization, and how interpretability insights can enhance professional translation practices.

The first part establishes a comprehensive infrastructure and methodological framework for analyzing context usage in language models and machine translation systems. We begin by introducing Inseq, an open-source toolkit designed to democratize access to interactive analyses of language models' behaviors, providing a simple unified interface to state-of-the-art input attribution interpretability methods. Building on this foundation, we develop [PECoRe]{.smallcaps}, an end-to-end framework employing contrastive input attribution to faithfully quantify how language models exploit contextual information throughout generation, and apply it to study context influence in context-aware machine translation. We further extend our investigation to retrieval-augmented generation with large language models, demonstrating that model internals can produce faithful high-quality citations to context paragraphs for open-book question answering across multiple languages.

The second part shifts the focus from analysis to intervention, exploring two complementary paradigms for conditioning machine translation outputs to meet user-specific requirements. We first present [Ramp]{.smallcaps}, a retrieval-augmented prompting technique using semantic similarity and explicit attribute marking to achieve attribute-controlled translation. We then advance to more sophisticated personalization through direct intervention on model representations, employing sparse autoencoders to steer translations toward individual translator styles in the challenging domain of literary translation. Our comparative analysis reveals that successful prompting and interpretability-based steering approaches converge to similar mechanistic solutions, uncovering fundamental principles of generation conditioning in neural language models.

The final part explores how insights derived from model inner workings can inform human editors in real-world translation workflows, through comprehensive cross-lingual user studies involving professional translators. We first present [DivEMT]{.smallcaps}, the first publicly available cross-lingual post-editing dataset spanning six typologically diverse languages, showing that traditional MT quality metrics fail to correlate with actual post-editing productivity and that language relatedness significantly influences editing efficiency. Building on these insights, our second study QE4PE investigates how word-level error highlights---including those generated by unsupervised uncertainty-based methods---impact translator productivity and output quality in realistic editing scenarios. Our evaluation of unsupervised quality estimation methods demonstrates that approaches based on model internals can outperform supervised baselines in downstream usability, highlighting the importance of method calibration and multiple quality annotations to account for human label variation.

Overall, this dissertation advances the field of machine translation interpretability across three dimensions: it develops accessible tools and frameworks for understanding context usage in generative models, demonstrates how interpretability methods can enable fine-grained control over translation outputs, and establishes empirical evidence for the usage of language model internals professional translation workflows. These contributions lay the groundwork for more transparent, controllable, and human-centered translation systems, with the potential to enhance the productivity and effectiveness of human translators in their interactions with AI systems.
:::
:::

# Introduction {#sec-chap-1-introduction}

Recent years have seen astounding advances in the development of language models, which have quickly progressed from being simple research prototypes producing barely-sensical outputs, to becoming cornerstones of modern technological infrastructure. The success of these systems can be largely attributed to the outstanding ability of large deep neural networks such as the Transformer [@vaswani-etal-2017-attention], to learn rich representations of language---and hence of our world and society---from vertiginous amounts of textual data. However, the complex and deeply intertwined structure that renders these models so powerful is also the main culprit behind their opacity: the inner workings of neural networks are notoriously difficult to interpret, and the lack of transparency in their decision-making processes has raised significant concerns about their reliability and fairness in high-stakes applications [@rudin-2019-stop].

These circumstances have led to a growing interest in the field of *interpretability*, closely related to the broader area of explainable artificial intelligence (XAI), which aims to develop methods and tools to understand how neural networks work, and to provide insights into their decision-making processes [@doshivelez-kim-2017-towards;@li-etal-2022-interpretable]. In the context of natural language processing (NLP), interpretability research has largely focused on understanding how language models encode and process factual knowledge and linguistic information [@tenney-etal-2019-bert;@belinkov-2022-probing;@meng-etal-2022-rome], how they leverage contextual information during generation [@clark-etal-2019-bert;@ferrando-etal-2022-measuring] and studying the learned mechanisms underlying their capabilities [@elhage-etal-2021-mathematical;@saphra-wiegreffe-2024-mechanistic]. 

Despite major advances in interpretability research, the field has faced criticism for rarely producing actionable outputs that would meaningfully improve the design or usage of language models in practice [@rauker-etal-2023-toward;@rai-etal-2024-practical;@hendrycks-hiscott-2025-misguided], despite interpretability insights being recognized as highly influential among the research community [@mosbach-etal-2024-insights]. While a majority of interpretability works nowadays focus on identifying subnetworks and mechanisms responsible for specific tasks inside language models [@ferrando-etal-2024-primer;@sharkey-etal-2025-open], few works have put interpretability insights in relation to the needs and desires of end-users of such systems [@ehsan-etal-2021-expanding], whose perception plays a crucial role in determining the practical effectiveness of interpretability findings in applied settings [@ehsan-etal-2024-who]. This is largely due to a disconnect between the NLP and AI research communities, focusing mostly on theoretical understanding, and human-computer interaction (HCI) researchers, who are more concerned with actionable insights and practical applications.

A prime example of the disconnect between insights and impact of interpretability research comes from the field of machine translation (MT), a long-standing area within NLP research. MT researchers pioneered the use of neural language models for sequence generation tasks [@sutskever-etal-2014-sequence;@bahdanau-etal-2015-neural], and were among the first to analyze the inner workings of neural language models [@belinkov-etal-2017-neural;@voita-etal-2019-analyzing;@rogers-etal-2020-primer]. Despite the significant progress in MT systems' performance across hundreds of languages over the past decade, the field has been slow to convert new approaches shedding light on models' usage of contextual information and predictive uncertainty into improvements for users and professional translators. As of today, most users of "classic" translation tools such as Google Translate are simply presented with translations, without the possibility to personalize their tone or properties, or to identify potential errors or alternative formulations. On the other hand, the eloquent justifications that large language models (LLMs) such as GPT-4 [@openai-2023-gpt4] eagerly produce alongside their translations are not guaranteed to actually reflect the models' inner processing and context usage, often resulting in plausible but misleading explanations [@turpin-etal-2023-language].

This dissertation aims to bridge the gap between method-centric interpretability research and outcome-centric real-world machine translation applications by developing novel actionable methods to understand and control neural language models' generation process, and how these can be integrated effectively in human translation workflows. The research presented here explores three main directions, namely: 1) understanding how language models exploit contextual information during generation, 2) controlling model generation for personalized translation outputs, and 3) integrating interpretability insights into human translation workflows. We investigate these themes through a series of methodological contributions, empirical evaluations and user studies, with the goal of providing a comprehensive understanding of how insights from interpretability can be used to improve the interaction between human users and machine translation systems.

## Outline and Contributions

The experimental chapters of this dissertation are organized into three parts, corresponding each to one of the main research directions outlined above. Each part is composed of multiple chapters, each presenting a self-contained contribution or study related to the overarching theme. @fig-chap1-chapter-guide provides a visual overview of parts and chapters, with topics introduced in @sec-chap-2-background highlighted within each chapter. Below, we summarize the contents, research questions and contributions of each part.

![Chapter guide for the three parts of this dissertation.](figures/index/chapter_guide.pdf){#fig-chap1-chapter-guide fig-pos="t"}

\vspace{7pt}
**Part I: Attributing Context Usage in Multilingual NLP**

Part I establishes the foundational infrastructure and methodological frameworks for understanding how neural language models and machine translation systems process and rely on contextual information during generation. This part begins by introducing Inseq (@sec-chap-3-inseq), a toolkit to simplify the access to interpretability analyses of sequence generation models, supporting various cutting-edge input attribution methods and utility functions. Building upon this, @sec-chap-4-pecore introduces [PECoRe]{.smallcaps}, a data-driven framework for quantifying context usage plausibility in language models through contrastive identification of context-sensitive tokens and contextual cues influencing their prediction. Finally, @sec-chap-5-mirage extends our analysis to modern large language models and retrieval-augmented generation settings with MIRAGE, an adaptation of the [PECoRe]{.smallcaps} framework demonstrating how model internals can be employed for faithful answer attribution in question answering settings. In this part, we aim to answer two main research questions:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 1 (RQ1)

What design principles and abstractions are necessary to democratize access to cutting-edge input attribution methods for generative language models?
:::

::: {.callout-tip icon="false"}

## ‚ùì Research Question 2 (RQ2)

How do language models and machine translation systems exploit contextual information during generation, and how can we quantify this usage faithfully?
:::

The primary contributions of Part I include: (1) the development of two open-source releases of the Inseq interpretability library; (2) the contrastive attribution tracing (CAT) method, a gradient-based alternative to causal intervention for efficiently identifying salient model components; (3) the [PECoRe]{.smallcaps} framework for context reliance attribution in language models, allowing data-driven explorations of context usage patterns in context-aware MT systems; and (4) an extended evaluation of context attribution for retrieval-augmented generation using the MIRAGE framework, achieving good citation quality while ensuring greater faithfulness to the model's reasoning process.

\vspace{7pt}
**Part II: Conditioning Generation for Personalized Machine Translation**

Part II shifts the focus from understanding context usage to actively controlling model generation for customizing translation outputs. In two chapters, we explore complementary methods to conditioning machine translation outputs---prompting-based methods and direct interventions in model processing---asking:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 3 (RQ3)

Are interpretability-based steering methods viable approaches for controllable machine translation? How do they compare with prompting-based methods?
:::

Specifically, @sec-chap-6-ramp pioneers the usage of prompting-based strategies for attribute-controlled translation, while @sec-chap-7-sae-litmt connects generation conditioning to interpretability techniques, expanding the scope from simple attributes in common domains to sophisticated translator personal styles in the challenging setting of literary translation.

The core contributions of Part II include: (1) [Ramp]{.smallcaps}, a novel prompting methodology that achieves strong performance in attribute-controlled translation across multiple languages and attributes without requiring model fine-tuning; (2) the first comprehensive comparison of prompting versus interpretability-based steering approaches for machine translation personalization; (3) a contrastive steering technique employing sparse autoencoder latents to achieve personalization accuracy comparable to prompting while preserving quality in the literary translation domain; and (4) evidence that successful prompting and steering approaches converge to similar mechanistic solutions, revealing fundamental principles of generation conditioning in language models.

\vspace{7pt}
**Part III: Towards Interpretability in Human Translation Workflows**

Part III is dedicated to evaluating how interpretability insights can practically benefit the work of human professionals editing machine translated contents. This part begins with [DivEMT]{.smallcaps} (@sec-chap-8-divemt), a comprehensive cross-lingual study to establish the effectiveness of MT post-editing compared to translation from scratch across six typologically diverse languages, addressing the question:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 4 (RQ4)

How does post-editing effectiveness vary across typologically diverse languages, and what factors influence translator productivity?
:::

Building upon these insights, the second user study QE4PE (@sec-chap-9-qe4pe) investigates how word-level error span highlights, including those derived by the uncertainty of the MT system during generation, impact the productivity of professional translators and the quality of post-edited contents, seeking to answer: 

::: {.callout-tip icon="false"}

## ‚ùì Research Question 5 (RQ5)

How do word-level error highlights impact professional translator productivity and output quality in realistic editing settings?
:::

Finally, @sec-chap-10-unsup-wqe concludes our human-centered investigation with an assessment spanning multiple uncertainty and interpretability-based word-level quality estimation methods, identifying how their performance varies across models, languages and labels produced by different human annotators:

::: {.callout-tip icon="false"}

## ‚ùì Research Question 6 (RQ6)

Can unsupervised error span detection reliably detect problems in machine translated outputs? How does human label variation affect such comparisons?
:::

Part III contributions include (1) [DivEMT]{.smallcaps}, a cross-lingual post-editing dataset enabling controlled comparison of translator productivity across editing modality and typologically diverse languages; (2) Evidence that MT quality metric do not correlate with human post-editing productivity across languages, and that the latter is heavily influenced by the relatedness between source and target languages; (3) QE4PE, a comprehensive bilingual post-editing dataset containing error spans, behavioral editing metrics and quality annotations from 42 professional post-editors; (4) Evidence that error span highlights might result in lower productivity, but improve the detection of critical errors in translations; and (5) Evidence that model internals-based unsupervised quality estimation methods can perform on par with state-of-the-art supervised approaches both in terms of accuracy and downstream usability, showing the impact of human editing propensity on the effectiveness of error span detection methods.

## Scientific Output

This dissertation was the product of several research articles and open-source projects, which are categorized in the following sections.

### Main Publications

The following articles represent the main contributions reflected in the experimental chapters of this thesis, organized in their respective parts and chapters:[^1]

[^1]: $^\dagger$ indicates shared first co-authorship.

- Ferrando, J., **Sarti, G.**, Bisazza, A. and Costa-juss√†, M. R. [-@ferrando-etal-2024-primer]. A Primer on the Inner Workings of Transformer-based Language Models. *Arxiv Preprint* (**@sec-chap-2-background**)

\vspace{5pt}
**Part I: Attributing Context Usage in Multilingual NLP**

- **Sarti, G.**, Feldhus, N., Sickert, L., van der Wal, O., Nissim, M. and Bisazza, A. [-@sarti-etal-2023-inseq]. Inseq: An Interpretability Toolkit for Sequence Generation Models. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL Demo)* (**@sec-chap-3-inseq**)

- **Sarti, G.**, Feldhus, N., Qi, J., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-democratizing]. Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit. In *Proceedings of the 2nd World Conference on eXplainable Artificial Intelligence: Late-breaking works and demos (xAI Demo)* (**@sec-chap-3-inseq** and **@sec-chap-4-pecore**)

- **Sarti, G.**, Chrupa≈Ça G., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-quantifying]. Quantifying the Plausibility of Context Reliance in Neural Machine Translation. In *Proceedings of the 12th International Conference on Learning Representations (ICLR)* (**@sec-chap-4-pecore**)

- Qi, J.$^\dagger$, **Sarti, G.**$^\dagger$, Fern√°ndez, R. and Bisazza, A. [-@qi-sarti-etal-2024-model]. Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (**@sec-chap-5-mirage**)

**Part II: Conditioning Generation for Personalized Machine Translation**

- **Sarti, G.**, Htut, P. M., Niu, X., Hsu, B., Currey, A., Dinu, G. and Nadejde, M. [-@sarti-etal-2023-ramp]. RAMP: Retrieval and Attribute-Marking Enhanced Prompting for Attribute-Controlled Translation. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL)* (**@sec-chap-6-ramp**)

- Scalena, D.$^\dagger$, **Sarti, G.**$^\dagger$, Bisazza, A., Fersini, E. and Nissim, M. [-@scalena-sarti-etal-2025-steering]. Steering Large Language Models for Machine Translation Personalization. *Arxiv Preprint* (**@sec-chap-7-sae-litmt**)

**Part III: Towards Interpretability in Human Translation Workflows**

- **Sarti, G.**, Bisazza, A., Guerberof-Arenas, A. and Toral, A. [-@sarti-etal-2022-divemt]. DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (**@sec-chap-8-divemt**)

- **Sarti, G.**, Zouhar, V., Chrupa≈Ça, G., Guerberof-Arenas, A., Nissim, M. and Bisazza, A. [-@sarti-etal-2025-qe4pe]. QE4PE: Word-level Quality Estimation for Human Post-Editing. *Transactions of the Association for Computational Linguistics (TACL)* (**@sec-chap-9-qe4pe**)

- **Sarti, G.**, Zouhar, V., Nissim, M. and Bisazza, A. [-@sarti-etal-2025-unsupervised]. Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement. *Arxiv Preprint* (**@sec-chap-10-unsup-wqe**)

I was responsible for the conceptualization, implementation, experimental evaluation and writing of the manuscript in each article in which I am the sole first author. For articles with shared first authorship, I co-led to the conceptualization, experimental design and manuscript writing and, in the case of @qi-sarti-etal-2024-model, for the implementation of the API used to conduct the experimental evaluation. The background in @sec-chap-2-background is adapted in part from our primer [@ferrando-etal-2024-primer], for which I contributed in surveying literature and writing content regarding the transformer architecture, input attribution methods, steering approaches and interpretability tools.

### Open-source Contributions

Open-source software played a fundamental role in this thesis' development, acting as a solid foundation for conducting experimental work in a reproducible way. Notably, all investigations conducted in this thesis employ solely open-source tools, models and datasets, despite the popularity of proprietary language models in the current research landscape. Links to all datasets, models, code and demos are provided in each of the following chapters to encourage scrutiny and foster further research work.

My most notable contribution to the open-source research ecosystem is the **Inseq** toolkit, presented in @sec-chap-3-inseq, for which I acted as development lead, and which now counts 430+ stars on GitHub and 70+ citations across multiple international venues.

I also contributed to the development of the following open-source projects:

- The **Groningen Translation Environment** ([GroTE]{.smallcaps}), a Gradio-based UI for machine translation post-editing supporting the live recording of behavioral logs using the Hugging Face `datasets` hub and `spaces` ecosystem, which I developed with the help of Vil√©m Zouhar for collecting the data of the QE4PE user study of @sec-chap-9-qe4pe. Available at <https://github.com/gsarti/grote> or through `pip install grote`.

- `gradio-highlightedtextbox`, a Svelte component for Gradio supporting editing of texts containing highlighted spans, which I developed for collecting behavioral edit data in the [GroTE]{.smallcaps} interface. Available at <https://huggingface.co/spaces/gsarti/gradio_highlightedtextbox> or through `pip install gradio-highlightedtextbox`.

- `labl`, a toolkit to facilitate token-level analyses of annotated texts with multiple edits and tokenization schemes, which I developed with the help of Vil√©m Zouhar to support the analyses in @sec-chap-10-unsup-wqe. Available at <https://github.com/gsarti/labl> or through `pip install labl`.

- **Interpreto**, a Python toolbox for concept-based interpretability analyses of language models maintained by the [FOR](https://www.irt-saintexupery.com/for-program/)/[DEEL](https://www.deel.ai/) teams, which I helped design and develop as part of my visit to the IRT Saint Exup√©ry research institute in Toulouse, France. Interpreto is available at <https://github.com/FOR-sight-ai/interpreto> or through `pip install interpreto`.

The full set of my open-source contributions, including various demos, models and datasets, is available on my [GitHub](https://github.com/gsarti) and my [ü§ó Hugging Face](https://huggingface.co/gsarti) profile pages.

### Other Research Contributions

Apart from the main thesis work, an important part of my research output during my PhD includes other projects beyond the scope of this dissertation. These include the following publications, organized around two macro-themes:

\vspace{5pt}
**Advancing Italian natural language processing:**

- Miaschi, A., **Sarti, G.**, Brunato, D., Dell'Orletta, F. and Venturi, G. [-@miaschi-etal-2022-probing]. Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties. *Italian Journal of Computational Linguistics (IJCoL)*

- Bianchi, F., Attanasio, G., Pisoni, R., Terragni, S., **Sarti, G.** and Balestri, D. [-@bianchi-etal-2023-contrastive]. Contrastive Language-Image Pre-training for the Italian Language. In *Proceedings of the 9th Italian Conference on Computational Linguistics (CLiC-it)*

- **Sarti, G.** and Nissim, M. [-@sarti-nissim-2024-it5]. IT5: Text-to-text Pretraining for Italian Language Understanding and Generation. In *Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING)*

- **Sarti, G.**, Caselli, T., Nissim, M. and Bisazza, A. [-@sarti-etal-2024-non]. Non Verbis, Sed Rebus: Large Language Models Are Weak Solvers of Italian Rebuses. In *Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it)*

- **Sarti, G.**, Caselli, T., Bisazza, A. and Nissim, M. [-@sarti-etal-2024-eurekarebus]. EurekaRebus - Verbalized Rebus Solving with LLMs: A CALAMITA Challenge. In *Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it)*

- Ciaccio, C., **Sarti, G.**, Miaschi, A. and Dell'Orletta, F. [-@ciaccio-etal-2025-crossword]. Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond. In *Proceedings of the 11th Italian Conference on Computational Linguistics (CLiC-it)*

\noindent
**Interpreting the inner workings of generative language models:**

- Langedijk, A., Mohebbi, H., **Sarti, G.**, Zuidema, W. and Jumelet, J. [-@langedijk-etal-2024-decoderlens]. DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers. In *Findings of the North American Chapter of the Association for Computational Linguistics (NAACL Findings)*

- Edman, L., **Sarti, G.**, Toral, A., van Noord, G. and Bisazza, A. [-@edman-etal-2024-character]. Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation. *Transactions of the Association for Computational Linguistics (TACL)*

- Scalena, D., **Sarti, G.** and Nissim, M. [-@scalena-etal-2024-multi]. Multi-property Steering of Large Language Models with Dynamic Activation Composition. In *Proceedings of the 7th Workshop on Analyzing and Interpreting Neural Networks for NLP (BlackboxNLP)*

- Ghasemi Madani, M. R., Gema, A. P., **Sarti, G.**, Zhao, Y., Minervini, P. and Passerini, A. [-@ghasemi-madani-etal-2025-noiser]. Noiser: Bounded Input Perturbations for Attributing Large Language Models. In *Proceedings of the Second Conference on Language Modeling (CoLM)*

- Candussio, S., Saveri, G., **Sarti, G.** and Bortolussi, L. [-@candussio-etal-2025-bridging]. Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers. In *Proceedings of the 2025 European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)*

I was also lucky to be involved in the organization of the BlackboxNLP 2025 workshop^[<https://blackboxnlp.github.io/2025>]---the leading venue for interpretability work in the NLP domain---and its shared task focused on benchmarking mechanistic interpretability methods for circuit localization and causal variable identification in large language models.