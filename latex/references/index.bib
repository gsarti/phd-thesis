@article{ferrando-etal-2024-primer,
	title = {A Primer on the Inner Workings of Transformer-based Language Models},
	author = {Javier Ferrando and Gabriele Sarti and Arianna Bisazza and Marta R. Costa-juss\`{a}},
	journal = {Arxiv Preprint},
	url = {https://arxiv.org/abs/2405.00208},
	year = 2024
}

@inproceedings{sarti-etal-2024-democratizing,
	title = "Democratizing Advanced Attribution Analyses of Generative Language Models with the Inseq Toolkit",
	author = "Sarti, Gabriele  and
        Feldhus, Nils  and
        Qi, Jirui  and
        Nissim, Malvina and
        Bisazza, Arianna",
	year = 2024,
	month = jul,
	booktitle = "xAI-2024 Late-breaking Work, Demos and Doctoral Consortium Joint Proceedings",
	publisher = "CEUR.org",
	address = "Valletta, Malta",
	pages = "289--296",
	url = "https://ceur-ws.org/Vol-3793/paper_37.pdf"
}
@inproceedings{ghasemi-madani-etal-2025-noiser,
	title = {Noiser: Bounded Input Perturbations for Attributing Large Language Models},
	author = {Mohammad Reza Ghasemi Madani and Aryo Pradipta Gema and Gabriele Sarti and Yu Zhao and Pasquale Minervini and Andrea Passerini},
	year = 2025,
	booktitle = {Second Conference on Language Modeling},
	series = {CoLM 2025},
	url = {https://openreview.net/forum?id=17yFbHmblo}
}
@article{miaschi-etal-2022-probing,
	title = "Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties",
	author = "Miaschi, Alessio and Sarti, Gabriele and Brunato, Dominique and Dell'Orletta, Felice and Venturi, Giulia",
	year = 2022,
	month = jul,
	journal = "Italian Journal of Computational Linguistics (IJCoL)",
	publisher = "OpenEdition",
	volume = 8,
	number = 1,
	pages = "25--44",
	doi = "10.4000/ijcol.965",
	issn = "2499-4553",
	url = "https://journals.openedition.org/ijcol/965"
}
@article{sarti-etal-2025-qe4pe,
	title = {{QE4PE}: Word-level Quality Estimation for Human Post-Editing},
	author = {Gabriele Sarti and Vil\'{e}m Zouhar and Grzegorz Chrupa\l{}a and Ana Guerberof-Arenas and Malvina Nissim and Arianna Bisazza},
	year = 2025,
	journal = {Arxiv Preprint},
	url = {https://arxiv.org/abs/2503.03044}
}
@inproceedings{sarti-etal-2024-quantifying,
	title = "Quantifying the Plausibility of Context Reliance in Neural Machine Translation",
	author = "Sarti, Gabriele and 
        Chrupa{\l}a, Grzegorz and 
        Nissim, Malvina and
        Bisazza, Arianna",
	year = 2024,
	month = may,
	booktitle = "The Twelfth International Conference on Learning Representations (ICLR 2024)",
	publisher = "OpenReview",
	address = "Vienna, Austria",
	url = "https://openreview.net/forum?id=XTHfNGI3zT"
}
@article{scalena-sarti-etal-2025-steering,
	title = {Steering Large Language Models for Machine Translation Personalization},
	author = {Scalena$^*$, Daniel and Sarti$^*$, Gabriele and Bisazza, Arianna and Fersini, Elisabetta and Nissim, Malvina},
	year = 2025,
	journal = {Arxiv Preprint},
	url = {https://arxiv.org/abs/2505.16612}
}
@article{sarti-etal-2025-unsupervised,
	title = {Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement},
	author = {Gabriele Sarti and Vil\'{e}m Zouhar and Malvina Nissim and Arianna Bisazza},
	year = 2025,
	journal = {Arxiv Preprint},
	url = {https://arxiv.org/abs/2505.23183}
}

@inproceedings{candussio-etal-2025-bridging,
	author="Candussio, Sara and Saveri, Gaia and Sarti, Gabriele and Bortolussi, Luca",
	title="Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers",
	booktitle="Machine Learning and Knowledge Discovery in Databases. Research Track",
	year="2025",
	publisher="Springer Nature Switzerland",
	series="ECML-PKDD",
	url="https://doi.org/10.1007/978-3-032-06096-9_1",
	doi="10.1007/978-3-032-06096-9_1"
}


@inproceedings{ciaccio-etal-2025-crossword,
	title = {Crossword Space: Latent Manifold Learning for Italian Crosswords and Beyond},
	author = {Ciaccio, Cristiano and Sarti, Gabriele and Miaschi, Alessio and Dell'Orletta, Felice},
	year = 2025,
	month = sep,
	booktitle = {Proceedings of the 11th Italian Conference on Computational Linguistics (CLiC-it 2023)},
	publisher = "CEUR Workshop Proceedings",
	address = {Cagliari, Italy},
	editor = {Cristina Bosco and Elisabetta Jezek and Marco Polignano and Manuela Sanguinetti},
	url = {https://clic2025.unica.it/wp-content/uploads/2025/09/25_main_long.pdf}
}

@inproceedings{sarti-etal-2023-inseq-fixed,
    title = "Inseq: An Interpretability Toolkit for Sequence Generation Models",
    author = "Sarti, Gabriele  and
      Feldhus, Nils  and
      Sickert, Ludwig  and
      van der Wal, Oskar and
	  Nissim, Malvina  and
	  Bisazza, Arianna",
    editor = "Bollegala, Danushka  and
      Huang, Ruihong  and
      Ritter, Alan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.40/",
    doi = "10.18653/v1/2023.acl-demo.40",
    pages = "421--435"
}

@inproceedings{qi-sarti-etal-2024-model,
    title = "Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation",
    author = "Qi$^*$, Jirui  and
      Sarti$^*$, Gabriele  and
      Fern{\'a}ndez, Raquel  and
      Bisazza, Arianna",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.347/",
    doi = "10.18653/v1/2024.emnlp-main.347",
    pages = "6037--6053"
}

@article{rudin-2019-stop,
  author = {Rudin, Cynthia},
  title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  journal = {Nature Machine Intelligence},
  volume = {1},
  pages = {206--215},
  year = {2019},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://doi.org/10.1038/s42256-019-0048-x}
}

@misc{doshivelez-kim-2017-towards,
      title={Towards A Rigorous Science of Interpretable Machine Learning}, 
      author={Finale Doshi-Velez and Been Kim},
      year={2017},
      eprint={1702.08608},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1702.08608}, 
}

@article{li-etal-2022-interpretable,
  author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
  title = {Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond},
  journal = {Knowledge and Information Systems},
  volume = {64},
  number = {12},
  pages = {3197--3234},
  year = {2022},
  month = {12},
  doi = {10.1007/s10115-022-01756-8},
  url = {https://doi.org/10.1007/s10115-022-01756-8},
}

@inproceedings{nannini-etal-2023-explainability,
	author = {Nannini, Luca and Balayn, Agathe and Smith, Adam Leon},
	title = {Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK},
	year = {2023},
	isbn = {9798400701924},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3593013.3594074},
	doi = {10.1145/3593013.3594074},
	abstract = {Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.},
	booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
	pages = {1198–1212},
	numpages = {15},
	keywords = {AI policy, Explainable AI, social epistemology},
	location = {Chicago, IL, USA},
	series = {FAccT '23}
}

@inproceedings{rauker-etal-2023-toward,
  author={Räuker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)}, 
  title={Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks}, 
  year={2023},
  pages={464-483},
  doi={10.1109/SaTML54575.2023.00039}
}

@article{rai-etal-2024-practical,
	title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models}, 
	author={Daking Rai and Yilun Zhou and Shi Feng and Abulhair Saparov and Ziyu Yao},
	year={2024},
	journal={Arxiv Preprint},
	url={https://arxiv.org/abs/2407.02646}, 
}

@misc{hendrycks-hiscott-2025-misguided,
  author = {Hendrycks, Dan and Hiscott, Laura},
  title = {The Misguided Quest for Mechanistic AI Interpretability},
  howpublished = {AI Frontiers},
  url = {https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability},
  year = {2025},
  note = {Accessed August 4, 2025}
}

@misc{sharkey-etal-2025-open,
      title={Open Problems in Mechanistic Interpretability}, 
      author={Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath},
      year={2025},
      eprint={2501.16496},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.16496}, 
}

@inproceedings{ehsan-etal-2021-expanding,
	author = {Ehsan, Upol and Liao, Q. Vera and Muller, Michael and Riedl, Mark O. and Weisz, Justin D.},
	title = {Expanding Explainability: Towards Social Transparency in AI systems},
	year = {2021},
	isbn = {9781450380966},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3411764.3445188},
	doi = {10.1145/3411764.3445188},
	booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
	articleno = {82},
	numpages = {19},
	location = {Yokohama, Japan},
	series = {CHI '21}
}

@inproceedings{ehsan-etal-2024-who,
	author = {Ehsan, Upol and Passi, Samir and Liao, Q. Vera and Chan, Larry and Lee, I-Hsiang and Muller, Michael and Riedl, Mark O},
	title = {The Who in XAI: How AI Background Shapes Perceptions of AI Explanations},
	year = {2024},
	isbn = {9798400703300},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3613904.3642474},
	doi = {10.1145/3613904.3642474},
	abstract = {Explainability of AI systems is critical for users to take informed actions. Understanding who opens the black-box of AI is just as important as opening it. We conduct a mixed-methods study of how two different groups—people with and without AI background—perceive different types of AI explanations. Quantitatively, we share user perceptions along five dimensions. Qualitatively, we describe how AI background can influence interpretations, elucidating the differences through lenses of appropriation and cognitive heuristics. We find that (1) both groups showed unwarranted faith in numbers for different reasons and (2) each group found value in different explanations beyond their intended design. Carrying critical implications for the field of XAI, our findings showcase how AI generated explanations can have negative consequences despite best intentions and how that could lead to harmful manipulation of trust. We propose design interventions to mitigate them.},
	booktitle = {Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems},
	articleno = {316},
	numpages = {32},
	keywords = {Explainable AI, Human-Centered Explainable AI, User Characteristics},
	location = {Honolulu, HI, USA},
	series = {CHI '24}
}

@inproceedings{sutskever-etal-2014-sequence,
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	title = {Sequence to sequence learning with neural networks},
	year = {2014},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
	abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
	pages = {3104–3112},
	numpages = {9},
	location = {Montreal, Canada},
	series = {NIPS'14}
}

@inproceedings{turpin-etal-2023-language,
	author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},
	title = {Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting},
	year = {2023},
	publisher = {Curran Associates Inc.},
	address = {Red Hook, NY, USA},
	abstract = {Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs—e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(A)"—which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36\% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.},
	booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
	articleno = {3275},
	numpages = {14},
	location = {New Orleans, LA, USA},
	series = {NIPS '23}
}

@article{islam-sarti-2025-reveal,
	title = {Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation},
	author = {Khondoker Ittehadul Islam and Gabriele Sarti},
	journal = {Arxiv Preprint},
	url = {https://arxiv.org/abs/2508.08933},
	year = 2025
}