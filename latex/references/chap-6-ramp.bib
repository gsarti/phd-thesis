@article{bigscience-2023-bloom,
  title={{BLOOM}: A 176B-Parameter Open-Access Multilingual Language Model},
  author={{BigScience Workshop} and Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra and Yvon, Fran{\c{c}}ois and others},
  journal={Arxiv},
  year={2022},
  url={https://arxiv.org/abs/2211.05100}
}

@article{chowdhery-etal-2023-palm,
  author  = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  title   = {PaLM: Scaling Language Modeling with Pathways},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {240},
  pages   = {1--113},
  url     = {http://jmlr.org/papers/v24/22-1144.html}
}

@article{garcia-etal-2021-multilingual,
  title={Towards universality in multilingual text rewriting},
  author={Garcia, Xavier and Constant, Noah and Guo, Mandy and Firat, Orhan},
  journal={Arxiv},
  year={2021},
  url={https://arxiv.org/abs/2107.14749}
}

@article{garcia-firat-2022-using,
  title={Using natural language prompts for machine translation},
  author={Garcia, Xavier and Firat, Orhan},
  journal={Arxiv},
  year={2022},
  url={https://arxiv.org/abs/2202.11822}
}

@inproceedings{he-etal-2023-debertav3,
  title={{DeBERTaV3}: Improving {DeBERTa} using {ELECTRA}-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  author={He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
  booktitle={Proceedings of the 11th International Conference on Learning Representations},
  series={ICLR},
  year={2023},
  url={https://openreview.net/forum?id=sE7-XhLxHA}
}



@article{gao-etal-2021-pile,
  author       = {Leo Gao and
                  Stella Biderman and
                  Sid Black and
                  Laurence Golding and
                  Travis Hoppe and
                  Charles Foster and
                  Jason Phang and
                  Horace He and
                  Anish Thite and
                  Noa Nabeshima and
                  Shawn Presser and
                  Connor Leahy},
  title        = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  journal      = {Arxiv},
  year         = {2021},
  url          = {https://arxiv.org/abs/2101.00027},
}

@article{leiter-etal-2024-towards,
  author  = {Christoph Leiter and Piyawat Lertvittayakumjorn and Marina Fomicheva and Wei Zhao and Yang Gao and Steffen Eger},
  title   = {Towards Explainable Evaluation Metrics for Machine Translation},
  journal = {Journal of Machine Learning Research},
  year    = {2024},
  volume  = {25},
  number  = {75},
  pages   = {1--49},
  url     = {http://jmlr.org/papers/v25/22-0416.html}
}

@article{niu-carpuat-2020-controlling,
  title={Controlling Neural Machine Translation Formality with Synthetic Supervision},
  author={Niu, Xing and Carpuat, Marine},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8568--8575},
  year={2020},
  publisher={AAAI Press},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/6379}
}

@inproceedings{nye-etal-2022-show,
    title={Show Your Work: Scratchpads for Intermediate Computation with Language Models},
    author={Maxwell Nye and Anders Johan Andreassen and Guy Gur-Ari and Henryk Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and David Luan and Charles Sutton and Augustus Odena},
    booktitle={Deep Learning for Code Workshop},
    year={2022},
    url={https://openreview.net/forum?id=HBlx2idbkbq}
}

@inproceedings{wang-etal-2020-minilm,
    author = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
    title = {MINILM: deep self-attention distillation for task-agnostic compression of pre-trained transformers},
    year = {2020},
    isbn = {9781713829546},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
    booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
    articleno = {485},
    numpages = {13},
    location = {Vancouver, BC, Canada},
    series = {NIPS '20},
    url={https://proceedings.neurips.cc/paper/2020/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
}