While neural language models are remarkably effective, their internal mechanisms remain largely opaque. This thesis investigates the inner workings of neural machine translation (MT) systems through the lens of interpretability, developing novel methods to understand, control, and integrate these systems into human translation workflows. The research addresses fundamental questions about how language models leverage contextual information, how their generation processes can be steered for personalization, and how interpretability insights can enhance professional translation practices.

The first part of the thesis establishes a framework for analyzing context usage in language models. We introduce Inseq, an open-source toolkit for interactive analysis of language model behavior, and develop 	extsc{PECoRe}, a framework that uses contrastive input attribution to quantify how language models exploit contextual information. We extend this to retrieval-augmented generation, demonstrating that model internals can produce faithful, high-quality citations for open-book question answering.

The second part shifts from analysis to intervention, exploring two paradigms for conditioning MT outputs. We first present 	extsc{Ramp}, a retrieval-augmented prompting technique for attribute-controlled translation. We then employ sparse autoencoders to steer translations toward individual translator styles in the challenging domain of literary translation. Our analysis reveals that successful prompting and steering approaches converge on similar mechanistic solutions.

The final part explores how insights from model internals can inform human editors in real-world translation workflows. We present 	extsc{DivEMT}, the first cross-lingual post-editing dataset spanning six typologically diverse languages, which shows that traditional MT quality metrics fail to correlate with post-editing productivity. Our second study, QE4PE, investigates how word-level error highlights impact translator productivity and output quality. Our evaluation of unsupervised quality estimation methods demonstrates that approaches based on model internals can outperform supervised baselines, highlighting the importance of calibration and multiple annotations to account for human label variation.

Overall, this dissertation advances the field of machine translation interpretability by developing accessible tools for understanding context usage, enabling fine-grained control over translation outputs, and establishing empirical evidence for the use of model internals in professional translation workflows. These contributions lay the groundwork for more transparent, controllable, and human-centered translation systems.