While neural network-based language models have proven remarkably effective for natural language processing tasks, their internal mechanisms remain largely opaque to researchers and practitioners alike. Recent advances in interpretability research have begun to shed light on how these systems work, particularly in identifying model subnetworks responsible for specific tasks and understanding how knowledge is stored and employed by models. However, interpretability research has faced criticism for rarely producing actionable outputs that would meaningfully improve the design or usage of language models in practice. This thesis investigates the inner workings of neural machine translation (MT) systems through the lens of interpretability, developing novel methods to understand, control, and integrate these systems into human translation workflows. The research addresses fundamental questions about how language models leverage contextual information, how their generation processes can be steered for personalization, and how interpretability insights can enhance professional translation practices.

The first part establishes a comprehensive infrastructure and methodological framework for analyzing context usage in language models and machine translation systems. We begin by introducing Inseq, an open-source toolkit designed to democratize access to interactive analyses of language models' behaviors, providing a simple unified interface to state-of-the-art input attribution interpretability methods. Building on this foundation, we develop \textsc{PECoRe}, an end-to-end framework employing contrastive input attribution to faithfully quantify how language models exploit contextual information throughout generation, and apply it to study context influence in context-aware machine translation. We further extend our investigation to retrieval-augmented generation with large language models, demonstrating that model internals can produce faithful high-quality citations to context paragraphs for open-book question answering across multiple languages.

The second part shifts the focus from analysis to intervention, exploring two complementary paradigms for conditioning machine translation outputs to meet user-specific requirements. We first present \textsc{Ramp}, a retrieval-augmented prompting technique using semantic similarity and explicit attribute marking to achieve attribute-controlled translation. We then advance to more sophisticated personalization through direct intervention on model representations, employing sparse autoencoders to steer translations toward individual translator styles in the challenging domain of literary translation. Our comparative analysis reveals that successful prompting and interpretability-based steering approaches converge to similar mechanistic solutions, uncovering fundamental principles of generation conditioning in neural language models.

The final part explores how insights derived from model inner workings can inform human editors in real-world translation workflows, through comprehensive cross-lingual user studies involving professional translators. We first present \textsc{DivEMT}, the first publicly available cross-lingual post-editing dataset spanning six typologically diverse languages, showing that traditional MT quality metrics fail to correlate with actual post-editing productivity and that language relatedness significantly influences editing efficiency. Building on these insights, our second study QE4PE investigates how word-level error highlights---including those generated by unsupervised uncertainty-based methods---impact translator productivity and output quality in realistic editing scenarios. Our evaluation of unsupervised quality estimation methods demonstrates that approaches based on model internals can outperform supervised baselines in downstream usability, highlighting the importance of method calibration and multiple quality annotations to account for human label variation.

Overall, this dissertation advances the field of machine translation interpretability across three dimensions: it develops accessible tools and frameworks for understanding context usage in generative models, demonstrates how interpretability methods can enable fine-grained control over translation outputs, and establishes empirical evidence for the usage of language model internals professional translation workflows. These contributions lay the groundwork for more transparent, controllable, and human-centered translation systems, with the potential to enhance the productivity and effectiveness of human translators in their interactions with AI systems.